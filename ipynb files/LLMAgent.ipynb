{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ananyahooda/miniforge3/envs/ml_env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/ananyahooda/miniforge3/envs/ml_env/lib/python3.8/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/Users/ananyahooda/miniforge3/envs/ml_env/lib/python3.8/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "import re\n",
    "import json\n",
    "from transformers import pipeline\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "import flair\n",
    "from flair.data import Sentence\n",
    "\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /Users/ananyahooda/.cache/lm-studio/models/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_S.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 14\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  217 tensors\n",
      "llama_model_loader: - type q5_K:    8 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Small\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 3.86 GiB (4.57 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.22 MiB\n",
      "ggml_backend_metal_buffer_from_ptr: allocated buffer, size =  3877.58 MiB, ( 3877.64 / 10922.67)\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "llm_load_tensors:      Metal buffer size =  3877.57 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M1\n",
      "ggml_metal_init: picking default device: Apple M1\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: GGML_METAL_PATH_RESOURCES = nil\n",
      "ggml_metal_init: loading '/Users/ananyahooda/miniforge3/envs/ml_env/lib/python3.8/site-packages/llama_cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M1\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple7  (1007)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   256.00 MiB, ( 4135.45 / 10922.67)\n",
      "llama_kv_cache_init:      Metal KV buffer size =   256.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =    13.02 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   164.02 MiB, ( 4299.47 / 10922.67)\n",
      "llama_new_context_with_model:      Metal compute buffer size =   164.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =     8.00 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 2\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '8', 'llama.context_length': '32768', 'llama.attention.head_count': '32', 'llama.rope.freq_base': '1000000.000000', 'llama.rope.dimension_count': '128', 'general.file_type': '14', 'llama.feed_forward_length': '14336', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'mistralai_mistral-7b-instruct-v0.2'}\n",
      "Guessed chat format: mistral-instruct\n"
     ]
    }
   ],
   "source": [
    "llm = Llama(model_path=\"/Users/ananyahooda/.cache/lm-studio/models/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_S.gguf\",  \n",
    "n_ctx=2048,\n",
    "n_gpu_layers=-1,\n",
    "n_batch=512,\n",
    "callback_manager=callback_manager,\n",
    "verbose=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ananyahooda/miniforge3/envs/ml_env/lib/python3.8/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "# Initialize the pipeline and tokenizer once\n",
    "triplet_extractor_rebel = pipeline('text2text-generation', model='Babelscape/rebel-large', tokenizer='Babelscape/rebel-large')\n",
    "\n",
    "def extract_text_triplets_rebel(input_text):\n",
    "    \"\"\"\n",
    "    Extracts triplets from the given text.\n",
    "\n",
    "    Parameters:\n",
    "    input_text (str): The text from which to extract triplets.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of dictionaries, each representing a triplet with 'head', 'type', and 'tail'.\n",
    "    \"\"\"\n",
    "    # Use the tokenizer manually since we need special tokens\n",
    "    extracted_text = triplet_extractor_rebel.tokenizer.batch_decode([\n",
    "        triplet_extractor_rebel(input_text, return_tensors=True, return_text=False)[0][\"generated_token_ids\"]\n",
    "    ])\n",
    "\n",
    "    # Function to parse the generated text and extract the triplets\n",
    "    def extract_triplets(text):\n",
    "        triplets = []\n",
    "        relation, subject, object_ = '', '', ''\n",
    "        text = text.strip()\n",
    "        current = 'x'\n",
    "        for token in text.replace(\"<s>\", \"\").replace(\"<pad>\", \"\").replace(\"</s>\", \"\").split():\n",
    "            if token == \"<triplet>\":\n",
    "                current = 't'\n",
    "                if relation != '':\n",
    "                    triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "                    relation = ''\n",
    "                subject = ''\n",
    "            elif token == \"<subj>\":\n",
    "                current = 's'\n",
    "                if relation != '':\n",
    "                    triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "                object_ = ''\n",
    "            elif token == \"<obj>\":\n",
    "                current = 'o'\n",
    "                relation = ''\n",
    "            else:\n",
    "                if current == 't':\n",
    "                    subject += ' ' + token\n",
    "                elif current == 's':\n",
    "                    object_ += ' ' + token\n",
    "                elif current == 'o':\n",
    "                    relation += ' ' + token\n",
    "        if subject != '' and relation != '' and object_ != '':\n",
    "            triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "        return triplets\n",
    "\n",
    "    extracted_triplets = extract_triplets(extracted_text[0])\n",
    "    return extracted_triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplet_extractor_knowgl = pipeline(\"text2text-generation\", model=\"ibm/knowgl-large\")\n",
    "\n",
    "def extract_text_triplets_knowgl(input_text):\n",
    "    \"\"\"\n",
    "    Extracts triplets from the given text.\n",
    "\n",
    "    Parameters:\n",
    "    input_text (str): The text from which to extract triplets.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of dictionaries, each representing a triplet with 'head', 'type', and 'tail'.\n",
    "    \"\"\"\n",
    "    extracted_text = triplet_extractor_knowgl.tokenizer.batch_decode([\n",
    "        triplet_extractor_knowgl(input_text, return_tensors=True, return_text=False)[0][\"generated_token_ids\"]\n",
    "    ])\n",
    "    return extract_triples(extracted_text[0])\n",
    "\n",
    "def split_spo(input_string):\n",
    "    pattern = r\"\\[(.*?)\\|\\s*(.*?)\\s*\\|\\s*(.*?)\\]\"\n",
    "    matches = re.findall(pattern, input_string)\n",
    "    if matches:\n",
    "        return matches[0]\n",
    "\n",
    "def clean_entity(entity_string):\n",
    "    return entity_string.split(\"#\")[0][1:]\n",
    "\n",
    "def convert_format(input_text):\n",
    "    subj, rel, obj = split_spo(input_text)\n",
    "    return {'head': clean_entity(subj).strip(), 'type': rel,'tail': clean_entity(obj).strip()}\n",
    "\n",
    "def extract_triples(gen_text):\n",
    "    gen_text = gen_text.replace(\"<s><s>\", \"\").replace(\"</s>\", \"\")\n",
    "    triples = []\n",
    "    for triple_text in gen_text.split(\"$\"):\n",
    "        triples.append(convert_format(triple_text))\n",
    "    return triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_triplets(input_text):\n",
    "    \"\"\"\n",
    "    Extracts triplets from the given text using both Rebel and KnowGL models,\n",
    "    and ranks the outputs based on some criteria.\n",
    "\n",
    "    Parameters:\n",
    "    input_text (str): The text from which to extract triplets.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of dictionaries, each representing a triplet with 'head', 'type', and 'tail',\n",
    "    sorted based on the ranking criteria.\n",
    "    \"\"\"\n",
    "    # Extract triplets using Rebel model\n",
    "    triplets_rebel = extract_text_triplets_rebel(input_text)\n",
    "    \n",
    "    # Extract triplets using KnowGL model\n",
    "    triplets_knowgl = extract_text_triplets_knowgl(input_text)\n",
    "    \n",
    "    # Combine the triplets from both models\n",
    "    all_triplets = triplets_rebel + triplets_knowgl\n",
    "    \n",
    "    # Rank the triplets based on some criteria (e.g., uniqueness, confidence, etc.)\n",
    "    # Here, let's assume the ranking criteria is based on the length of the triplets\n",
    "    unique_triplets = []\n",
    "    for triplet in all_triplets:\n",
    "        if triplet not in unique_triplets:\n",
    "            unique_triplets.append(triplet)\n",
    "    \n",
    "    # Rank the triplets based on some criteria (e.g., uniqueness, confidence, etc.)\n",
    "    # Here, let's assume the ranking criteria is based on the length of the triplets\n",
    "    ranked_triplets = sorted(unique_triplets, key=lambda x: len(x['type']), reverse=True)\n",
    "    \n",
    "    return ranked_triplets\n",
    "\n",
    "# Example usage:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'head': 'Fado', 'type': 'educated at', 'tail': 'IIT'}]\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Fado does not work at IIT\"\n",
    "extracted_triplets = extract_text_triplets(input_text)\n",
    "print(extracted_triplets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-15 02:25:30,936 SequenceTagger predicts: Dictionary with 20 tags: <unk>, O, S-ORG, S-MISC, B-PER, E-PER, S-LOC, B-ORG, E-ORG, I-PER, S-PER, B-MISC, I-MISC, E-MISC, I-ORG, B-LOC, E-LOC, I-LOC, <START>, <STOP>\n"
     ]
    }
   ],
   "source": [
    "# Load the Flair NER model\n",
    "flair_ner = flair.models.SequenceTagger.load('ner')\n",
    "\n",
    "def extract_entities(text):\n",
    "    # Create a Flair Sentence\n",
    "    sentence = Sentence(text)\n",
    "\n",
    "    # Run NER on the sentence\n",
    "    flair_ner.predict(sentence)\n",
    "\n",
    "    # Initialize lists to store head and tail entities\n",
    "    head_entities = []\n",
    "    tail_entities = []\n",
    "\n",
    "    # Extract entities and their types\n",
    "    for entity in sentence.get_spans('ner'):\n",
    "        if entity.tag == 'PER':\n",
    "            head_entities.append(entity.text)\n",
    "        else:\n",
    "            tail_entities.append(entity.text)\n",
    "\n",
    "    # Create a list of dictionaries containing head and tail entities\n",
    "    result = [{\"head\": head, \"tail\": tail} for head, tail in zip(head_entities, tail_entities)]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = '''<s>[INST] <<SYS>>\n",
    "Assistant is an expert JSON builder designed to assist with a wide range of tasks.\n",
    "\n",
    "Assistant is able to trigger actions for User by responding with JSON strings that contain \"action\" and \"action_input\" parameters.\n",
    "\n",
    "The available actions to Assistant are:\n",
    "\n",
    "- \"extract_text_triplets\": Useful when Assistant is asked to extract triplets from a given text.\n",
    "  - To use the extract_triplets tool, Assistant should respond like so:\n",
    "    {{\"action\": \"extract_text_triplets\", \"action_input\": \"Your text here\"}}\n",
    "\n",
    "- \"extract_entities\": Useful when Assistant is asked to extract entities from a given text.\n",
    "  - To use the extract_triplets tool, Assistant should respond like so:\n",
    "    {{\"action\": \"extract_entities\", \"action_input\": \"Your text here\"}} \n",
    "\n",
    "Here are some previous conversations between the Assistant and User:\n",
    "\n",
    "User: Hey how are you today?\n",
    "Assistant: I'm good thanks, how are you?\n",
    "User: Can you extract all the triplets from this text: \"Gràcia is a district of the city of Barcelona, Spain.\"\n",
    "Assistant: {{\"action\": \"extract_text_triplets\", \"action_input\": \"Gràcia is a district of the city of Barcelona, Spain.\"}}\n",
    "User: Also give triples for \"obama was US president\"\n",
    "Assistant: {{\"action\": \"extract_text_triplets\", \"action_input\": \"obama was US president\"}}\n",
    "User: Can you extract all the entities from this text: \"Gràcia is a district of the city of Barcelona, Spain.\"\n",
    "Assistant: {{\"action\": \"extract_entities\", \"action_input\": \"Gràcia is a district of the city of Barcelona, Spain.\"}}\n",
    "User: Also give entities for \"obama was US president\"\n",
    "Assistant: {{\"action\": \"extract_entities\", \"action_input\": \"obama was US president\"}}\n",
    "\n",
    "\n",
    "<</SYS>>\n",
    "\n",
    "{0}[/INST]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_command(command):\n",
    "    # Put user command into prompt\n",
    "    prompt = prompt_template.format(\"User: \" + command)\n",
    "    # Send command to the model\n",
    "    output = llm(prompt, max_tokens=2000, stop=[\"User:\"])\n",
    "    response = output['choices'][0]['text']\n",
    "\n",
    "    # try to find json in the response\n",
    "    try:\n",
    "        # Extract json from model response by finding first and last brackets {}\n",
    "        firstBracketIndex = response.index(\"{\")\n",
    "        lastBracketIndex = len(response) - response[::-1].index(\"}\")\n",
    "        jsonString = response[firstBracketIndex:lastBracketIndex]\n",
    "        responseJson = json.loads(jsonString)\n",
    "        if responseJson['action'] == 'extract_text_triplets':\n",
    "            extracted_triplets = extract_text_triplets(responseJson['action_input'])\n",
    "            return extracted_triplets   \n",
    "        elif responseJson['action'] == 'extract_entities':\n",
    "            extracted_entities = extract_entities(responseJson['action_input'])\n",
    "            return extracted_entities   \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    # No json match, just return response\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      55.59 ms /   169 runs   (    0.33 ms per token,  3040.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12348.61 ms /   519 tokens (   23.79 ms per token,    42.03 tokens per second)\n",
      "llama_print_timings:        eval time =   13762.48 ms /   168 runs   (   81.92 ms per token,    12.21 tokens per second)\n",
      "llama_print_timings:       total time =   27075.69 ms /   687 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'head': 'Berlin Wall',\n",
       "  'type': 'located in the administrative territorial entity',\n",
       "  'tail': 'East Berlin'},\n",
       " {'head': 'West Berlin', 'type': 'shares border with', 'tail': 'East Berlin'},\n",
       " {'head': 'East Berlin', 'type': 'shares border with', 'tail': 'West Berlin'},\n",
       " {'head': 'Berlin Wall', 'type': 'location', 'tail': 'East Berlin'}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_command(\"Can you please give all the triples for  \\\"While john F Kennedy and ronald reagan delivered their famous speeches from the safeto of West Berlin, Bruce Springsteen's speaking out against the Berlin Wall in the middle of East Berlin added to the euphoria\\\". \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      23.79 ms /   107 runs   (    0.22 ms per token,  4497.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2936.05 ms /   109 tokens (   26.94 ms per token,    37.12 tokens per second)\n",
      "llama_print_timings:        eval time =    9188.82 ms /   106 runs   (   86.69 ms per token,    11.54 tokens per second)\n",
      "llama_print_timings:       total time =   12617.74 ms /   215 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'head': 'john F Kennedy', 'tail': 'West Berlin'},\n",
       " {'head': 'Bruce Springsteen', 'tail': 'Berlin Wall'}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_command(\"Can you please give all the entities for  \\\"While john F Kennedy and ronald reagan delivered their famous speeches from the safeto of West Berlin, Bruce Springsteen's speaking out against the Berlin Wall in the middle of East Berlin added to the euphoria\\\". \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Function to extract triples for each context in eval.json and create pred.json\n",
    "def generate_pred_json(eval_file_path, pred_file_path):\n",
    "    # Load the evaluation data from eval.json\n",
    "    with open(eval_file_path, 'r') as file:\n",
    "        eval_data = json.load(file)\n",
    "    \n",
    "    # Initialize a list to hold the modified data with extracted triples\n",
    "    modified_data = []\n",
    "    \n",
    "    # Iterate over each item in the evaluation data\n",
    "    for item in eval_data:\n",
    "        context = item['context']\n",
    "        # Prepare the command with the context\n",
    "        command = f\"Can you please give triple for \\\"{context}\\\"\"\n",
    "        # Use the process_command function to predict the extracted triples\n",
    "        extracted_triplets = process_command(command)\n",
    "        # Append the extracted triples to the item under the 'triples' key\n",
    "        item['triples'] = extracted_triplets\n",
    "        # Append the modified item to the modified_data list\n",
    "        modified_data.append(item)\n",
    "    \n",
    "    # Write the modified data with extracted triples to pred.json\n",
    "    with open(pred_file_path, 'w') as file:\n",
    "        json.dump(modified_data, file, indent=4)\n",
    "\n",
    "# Example usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      15.62 ms /    51 runs   (    0.31 ms per token,  3265.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   11802.93 ms /    51 runs   (  231.43 ms per token,     4.32 tokens per second)\n",
      "llama_print_timings:       total time =   12046.12 ms /    52 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      23.10 ms /    28 runs   (    0.83 ms per token,  1212.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4942.51 ms /    13 tokens (  380.19 ms per token,     2.63 tokens per second)\n",
      "llama_print_timings:        eval time =    2135.49 ms /    27 runs   (   79.09 ms per token,    12.64 tokens per second)\n",
      "llama_print_timings:       total time =    7414.95 ms /    40 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     155.89 ms /   224 runs   (    0.70 ms per token,  1436.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =     995.89 ms /    67 tokens (   14.86 ms per token,    67.28 tokens per second)\n",
      "llama_print_timings:        eval time =   18493.06 ms /   223 runs   (   82.93 ms per token,    12.06 tokens per second)\n",
      "llama_print_timings:       total time =   22105.84 ms /   290 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     207.66 ms /   277 runs   (    0.75 ms per token,  1333.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =     702.12 ms /    61 tokens (   11.51 ms per token,    86.88 tokens per second)\n",
      "llama_print_timings:        eval time =   22639.19 ms /   276 runs   (   82.03 ms per token,    12.19 tokens per second)\n",
      "llama_print_timings:       total time =   26176.33 ms /   337 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     147.59 ms /   216 runs   (    0.68 ms per token,  1463.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =     973.47 ms /    76 tokens (   12.81 ms per token,    78.07 tokens per second)\n",
      "llama_print_timings:        eval time =   18087.72 ms /   215 runs   (   84.13 ms per token,    11.89 tokens per second)\n",
      "llama_print_timings:       total time =   21072.94 ms /   291 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     113.68 ms /   161 runs   (    0.71 ms per token,  1416.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =     691.43 ms /    53 tokens (   13.05 ms per token,    76.65 tokens per second)\n",
      "llama_print_timings:        eval time =   13302.76 ms /   160 runs   (   83.14 ms per token,    12.03 tokens per second)\n",
      "llama_print_timings:       total time =   15480.14 ms /   213 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     197.47 ms /   246 runs   (    0.80 ms per token,  1245.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =     700.01 ms /    56 tokens (   12.50 ms per token,    80.00 tokens per second)\n",
      "llama_print_timings:        eval time =   19906.24 ms /   245 runs   (   81.25 ms per token,    12.31 tokens per second)\n",
      "llama_print_timings:       total time =   23360.13 ms /   301 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      28.17 ms /    56 runs   (    0.50 ms per token,  1988.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7022.18 ms /    39 tokens (  180.06 ms per token,     5.55 tokens per second)\n",
      "llama_print_timings:        eval time =    5191.06 ms /    55 runs   (   94.38 ms per token,    10.60 tokens per second)\n",
      "llama_print_timings:       total time =   12780.07 ms /    94 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     104.19 ms /   168 runs   (    0.62 ms per token,  1612.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1903.66 ms /    45 tokens (   42.30 ms per token,    23.64 tokens per second)\n",
      "llama_print_timings:        eval time =   14029.13 ms /   167 runs   (   84.01 ms per token,    11.90 tokens per second)\n",
      "llama_print_timings:       total time =   17480.02 ms /   212 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra data: line 3 column 1 (char 202)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     109.61 ms /   165 runs   (    0.66 ms per token,  1505.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =     576.98 ms /    36 tokens (   16.03 ms per token,    62.39 tokens per second)\n",
      "llama_print_timings:        eval time =   14094.08 ms /   164 runs   (   85.94 ms per token,    11.64 tokens per second)\n",
      "llama_print_timings:       total time =   16111.32 ms /   200 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      20.17 ms /    28 runs   (    0.72 ms per token,  1388.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =     409.04 ms /    13 tokens (   31.46 ms per token,    31.78 tokens per second)\n",
      "llama_print_timings:        eval time =    2138.68 ms /    27 runs   (   79.21 ms per token,    12.62 tokens per second)\n",
      "llama_print_timings:       total time =    2838.11 ms /    40 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      23.25 ms /    27 runs   (    0.86 ms per token,  1161.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =     403.38 ms /    13 tokens (   31.03 ms per token,    32.23 tokens per second)\n",
      "llama_print_timings:        eval time =    2094.93 ms /    26 runs   (   80.57 ms per token,    12.41 tokens per second)\n",
      "llama_print_timings:       total time =    2748.82 ms /    39 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      52.62 ms /    65 runs   (    0.81 ms per token,  1235.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =     529.65 ms /    13 tokens (   40.74 ms per token,    24.54 tokens per second)\n",
      "llama_print_timings:        eval time =    5090.54 ms /    64 runs   (   79.54 ms per token,    12.57 tokens per second)\n",
      "llama_print_timings:       total time =    6270.32 ms /    77 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      35.49 ms /    47 runs   (    0.76 ms per token,  1324.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =     977.09 ms /    30 tokens (   32.57 ms per token,    30.70 tokens per second)\n",
      "llama_print_timings:        eval time =    3765.57 ms /    46 runs   (   81.86 ms per token,    12.22 tokens per second)\n",
      "llama_print_timings:       total time =    5213.14 ms /    76 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      18.19 ms /    31 runs   (    0.59 ms per token,  1703.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2194.26 ms /    15 tokens (  146.28 ms per token,     6.84 tokens per second)\n",
      "llama_print_timings:        eval time =    2608.28 ms /    30 runs   (   86.94 ms per token,    11.50 tokens per second)\n",
      "llama_print_timings:       total time =    5081.81 ms /    45 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      72.79 ms /    92 runs   (    0.79 ms per token,  1263.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =     553.72 ms /    11 tokens (   50.34 ms per token,    19.87 tokens per second)\n",
      "llama_print_timings:        eval time =    7367.61 ms /    91 runs   (   80.96 ms per token,    12.35 tokens per second)\n",
      "llama_print_timings:       total time =    9082.08 ms /   102 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      59.65 ms /    82 runs   (    0.73 ms per token,  1374.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2001.16 ms /    66 tokens (   30.32 ms per token,    32.98 tokens per second)\n",
      "llama_print_timings:        eval time =    6563.39 ms /    81 runs   (   81.03 ms per token,    12.34 tokens per second)\n",
      "llama_print_timings:       total time =    9464.94 ms /   147 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      51.94 ms /    65 runs   (    0.80 ms per token,  1251.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =     699.36 ms /    49 tokens (   14.27 ms per token,    70.06 tokens per second)\n",
      "llama_print_timings:        eval time =    5289.99 ms /    64 runs   (   82.66 ms per token,    12.10 tokens per second)\n",
      "llama_print_timings:       total time =    6938.68 ms /   113 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      37.77 ms /    46 runs   (    0.82 ms per token,  1217.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =     431.80 ms /    30 tokens (   14.39 ms per token,    69.48 tokens per second)\n",
      "llama_print_timings:        eval time =    3559.51 ms /    45 runs   (   79.10 ms per token,    12.64 tokens per second)\n",
      "llama_print_timings:       total time =    4454.23 ms /    75 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      23.53 ms /    28 runs   (    0.84 ms per token,  1190.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2814.38 ms /    13 tokens (  216.49 ms per token,     4.62 tokens per second)\n",
      "llama_print_timings:        eval time =    2182.53 ms /    27 runs   (   80.83 ms per token,    12.37 tokens per second)\n",
      "llama_print_timings:       total time =    5289.34 ms /    40 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     116.96 ms /   148 runs   (    0.79 ms per token,  1265.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =     677.69 ms /    39 tokens (   17.38 ms per token,    57.55 tokens per second)\n",
      "llama_print_timings:        eval time =   11910.27 ms /   147 runs   (   81.02 ms per token,    12.34 tokens per second)\n",
      "llama_print_timings:       total time =   14359.11 ms /   186 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     226.75 ms /   305 runs   (    0.74 ms per token,  1345.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =     438.79 ms /    31 tokens (   14.15 ms per token,    70.65 tokens per second)\n",
      "llama_print_timings:        eval time =   25095.38 ms /   304 runs   (   82.55 ms per token,    12.11 tokens per second)\n",
      "llama_print_timings:       total time =   28924.28 ms /   335 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     193.43 ms /   229 runs   (    0.84 ms per token,  1183.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =     691.44 ms /    39 tokens (   17.73 ms per token,    56.40 tokens per second)\n",
      "llama_print_timings:        eval time =   18407.00 ms /   228 runs   (   80.73 ms per token,    12.39 tokens per second)\n",
      "llama_print_timings:       total time =   21596.76 ms /   267 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      88.25 ms /   118 runs   (    0.75 ms per token,  1337.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =     674.66 ms /    42 tokens (   16.06 ms per token,    62.25 tokens per second)\n",
      "llama_print_timings:        eval time =    9658.79 ms /   117 runs   (   82.55 ms per token,    12.11 tokens per second)\n",
      "llama_print_timings:       total time =   11492.85 ms /   159 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      33.99 ms /    55 runs   (    0.62 ms per token,  1618.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1326.01 ms /    39 tokens (   34.00 ms per token,    29.41 tokens per second)\n",
      "llama_print_timings:        eval time =    4687.10 ms /    54 runs   (   86.80 ms per token,    11.52 tokens per second)\n",
      "llama_print_timings:       total time =    6556.29 ms /    93 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     157.68 ms /   190 runs   (    0.83 ms per token,  1205.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =     709.37 ms /    47 tokens (   15.09 ms per token,    66.26 tokens per second)\n",
      "llama_print_timings:        eval time =   15193.92 ms /   189 runs   (   80.39 ms per token,    12.44 tokens per second)\n",
      "llama_print_timings:       total time =   17930.95 ms /   236 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra data: line 3 column 1 (char 189)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      70.29 ms /    84 runs   (    0.84 ms per token,  1195.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =     574.02 ms /    35 tokens (   16.40 ms per token,    60.97 tokens per second)\n",
      "llama_print_timings:        eval time =    6658.20 ms /    83 runs   (   80.22 ms per token,    12.47 tokens per second)\n",
      "llama_print_timings:       total time =    8111.14 ms /   118 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     175.22 ms /   223 runs   (    0.79 ms per token,  1272.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =     694.57 ms /    49 tokens (   14.17 ms per token,    70.55 tokens per second)\n",
      "llama_print_timings:        eval time =   18283.87 ms /   222 runs   (   82.36 ms per token,    12.14 tokens per second)\n",
      "llama_print_timings:       total time =   21353.32 ms /   271 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      42.18 ms /    58 runs   (    0.73 ms per token,  1375.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =     671.25 ms /    42 tokens (   15.98 ms per token,    62.57 tokens per second)\n",
      "llama_print_timings:        eval time =    4846.77 ms /    57 runs   (   85.03 ms per token,    11.76 tokens per second)\n",
      "llama_print_timings:       total time =    6134.36 ms /    99 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      34.74 ms /    53 runs   (    0.66 ms per token,  1525.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =     710.12 ms /    37 tokens (   19.19 ms per token,    52.10 tokens per second)\n",
      "llama_print_timings:        eval time =    4504.01 ms /    52 runs   (   86.62 ms per token,    11.55 tokens per second)\n",
      "llama_print_timings:       total time =    5917.31 ms /    89 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      42.32 ms /    54 runs   (    0.78 ms per token,  1276.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =     838.72 ms /    42 tokens (   19.97 ms per token,    50.08 tokens per second)\n",
      "llama_print_timings:        eval time =    4289.09 ms /    53 runs   (   80.93 ms per token,    12.36 tokens per second)\n",
      "llama_print_timings:       total time =    5695.57 ms /    95 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      39.16 ms /    50 runs   (    0.78 ms per token,  1276.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =     887.55 ms /    34 tokens (   26.10 ms per token,    38.31 tokens per second)\n",
      "llama_print_timings:        eval time =    3902.58 ms /    49 runs   (   79.64 ms per token,    12.56 tokens per second)\n",
      "llama_print_timings:       total time =    5269.72 ms /    83 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     144.47 ms /   175 runs   (    0.83 ms per token,  1211.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1155.01 ms /    46 tokens (   25.11 ms per token,    39.83 tokens per second)\n",
      "llama_print_timings:        eval time =   14000.77 ms /   174 runs   (   80.46 ms per token,    12.43 tokens per second)\n",
      "llama_print_timings:       total time =   17055.87 ms /   220 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     156.65 ms /   174 runs   (    0.90 ms per token,  1110.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =     948.46 ms /    65 tokens (   14.59 ms per token,    68.53 tokens per second)\n",
      "llama_print_timings:        eval time =   13785.92 ms /   173 runs   (   79.69 ms per token,    12.55 tokens per second)\n",
      "llama_print_timings:       total time =   16653.55 ms /   238 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     113.49 ms /   138 runs   (    0.82 ms per token,  1215.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1472.71 ms /    42 tokens (   35.06 ms per token,    28.52 tokens per second)\n",
      "llama_print_timings:        eval time =   10937.51 ms /   137 runs   (   79.84 ms per token,    12.53 tokens per second)\n",
      "llama_print_timings:       total time =   13908.12 ms /   179 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     157.25 ms /   236 runs   (    0.67 ms per token,  1500.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =     965.66 ms /    66 tokens (   14.63 ms per token,    68.35 tokens per second)\n",
      "llama_print_timings:        eval time =   19973.82 ms /   235 runs   (   84.99 ms per token,    11.77 tokens per second)\n",
      "llama_print_timings:       total time =   23400.61 ms /   301 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     122.88 ms /   149 runs   (    0.82 ms per token,  1212.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =     693.76 ms /    45 tokens (   15.42 ms per token,    64.86 tokens per second)\n",
      "llama_print_timings:        eval time =   12027.43 ms /   148 runs   (   81.27 ms per token,    12.31 tokens per second)\n",
      "llama_print_timings:       total time =   14391.98 ms /   193 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     142.00 ms /   167 runs   (    0.85 ms per token,  1176.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =     687.47 ms /    46 tokens (   14.95 ms per token,    66.91 tokens per second)\n",
      "llama_print_timings:        eval time =   13322.91 ms /   166 runs   (   80.26 ms per token,    12.46 tokens per second)\n",
      "llama_print_timings:       total time =   15851.58 ms /   212 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     165.79 ms /   199 runs   (    0.83 ms per token,  1200.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1006.33 ms /    91 tokens (   11.06 ms per token,    90.43 tokens per second)\n",
      "llama_print_timings:        eval time =   16135.85 ms /   198 runs   (   81.49 ms per token,    12.27 tokens per second)\n",
      "llama_print_timings:       total time =   19490.69 ms /   289 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      21.93 ms /    29 runs   (    0.76 ms per token,  1322.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =     400.93 ms /    13 tokens (   30.84 ms per token,    32.42 tokens per second)\n",
      "llama_print_timings:        eval time =    2207.52 ms /    28 runs   (   78.84 ms per token,    12.68 tokens per second)\n",
      "llama_print_timings:       total time =    2949.04 ms /    41 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      24.18 ms /    30 runs   (    0.81 ms per token,  1240.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =     412.30 ms /    15 tokens (   27.49 ms per token,    36.38 tokens per second)\n",
      "llama_print_timings:        eval time =    2305.13 ms /    29 runs   (   79.49 ms per token,    12.58 tokens per second)\n",
      "llama_print_timings:       total time =    3019.07 ms /    44 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     136.35 ms /   171 runs   (    0.80 ms per token,  1254.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =     681.13 ms /    46 tokens (   14.81 ms per token,    67.54 tokens per second)\n",
      "llama_print_timings:        eval time =   13728.64 ms /   170 runs   (   80.76 ms per token,    12.38 tokens per second)\n",
      "llama_print_timings:       total time =   16252.24 ms /   216 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     233.47 ms /   340 runs   (    0.69 ms per token,  1456.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4599.86 ms /    51 tokens (   90.19 ms per token,    11.09 tokens per second)\n",
      "llama_print_timings:        eval time =   30699.77 ms /   339 runs   (   90.56 ms per token,    11.04 tokens per second)\n",
      "llama_print_timings:       total time =   39305.38 ms /   390 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     108.97 ms /   147 runs   (    0.74 ms per token,  1348.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2982.61 ms /    59 tokens (   50.55 ms per token,    19.78 tokens per second)\n",
      "llama_print_timings:        eval time =   12028.06 ms /   146 runs   (   82.38 ms per token,    12.14 tokens per second)\n",
      "llama_print_timings:       total time =   16553.45 ms /   205 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      97.14 ms /   181 runs   (    0.54 ms per token,  1863.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2259.93 ms /    54 tokens (   41.85 ms per token,    23.89 tokens per second)\n",
      "llama_print_timings:        eval time =   15547.59 ms /   180 runs   (   86.38 ms per token,    11.58 tokens per second)\n",
      "llama_print_timings:       total time =   19214.89 ms /   234 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     143.10 ms /   179 runs   (    0.80 ms per token,  1250.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1125.43 ms /    48 tokens (   23.45 ms per token,    42.65 tokens per second)\n",
      "llama_print_timings:        eval time =   14334.75 ms /   178 runs   (   80.53 ms per token,    12.42 tokens per second)\n",
      "llama_print_timings:       total time =   17452.58 ms /   226 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     153.78 ms /   213 runs   (    0.72 ms per token,  1385.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1380.18 ms /    55 tokens (   25.09 ms per token,    39.85 tokens per second)\n",
      "llama_print_timings:        eval time =   17447.11 ms /   212 runs   (   82.30 ms per token,    12.15 tokens per second)\n",
      "llama_print_timings:       total time =   21074.52 ms /   267 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     104.39 ms /   141 runs   (    0.74 ms per token,  1350.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =     670.69 ms /    40 tokens (   16.77 ms per token,    59.64 tokens per second)\n",
      "llama_print_timings:        eval time =   11608.89 ms /   140 runs   (   82.92 ms per token,    12.06 tokens per second)\n",
      "llama_print_timings:       total time =   13927.98 ms /   180 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     118.82 ms /   169 runs   (    0.70 ms per token,  1422.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =     684.13 ms /    45 tokens (   15.20 ms per token,    65.78 tokens per second)\n",
      "llama_print_timings:        eval time =   13746.75 ms /   168 runs   (   81.83 ms per token,    12.22 tokens per second)\n",
      "llama_print_timings:       total time =   16365.43 ms /   213 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     122.51 ms /   160 runs   (    0.77 ms per token,  1306.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =     685.00 ms /    51 tokens (   13.43 ms per token,    74.45 tokens per second)\n",
      "llama_print_timings:        eval time =   12940.50 ms /   159 runs   (   81.39 ms per token,    12.29 tokens per second)\n",
      "llama_print_timings:       total time =   15394.89 ms /   210 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      82.43 ms /   101 runs   (    0.82 ms per token,  1225.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =     988.50 ms /    88 tokens (   11.23 ms per token,    89.02 tokens per second)\n",
      "llama_print_timings:        eval time =    8005.87 ms /   100 runs   (   80.06 ms per token,    12.49 tokens per second)\n",
      "llama_print_timings:       total time =   10116.35 ms /   188 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     100.88 ms /   123 runs   (    0.82 ms per token,  1219.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =     688.00 ms /    36 tokens (   19.11 ms per token,    52.33 tokens per second)\n",
      "llama_print_timings:        eval time =    9669.62 ms /   122 runs   (   79.26 ms per token,    12.62 tokens per second)\n",
      "llama_print_timings:       total time =   11746.13 ms /   158 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     139.98 ms /   180 runs   (    0.78 ms per token,  1285.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =     690.76 ms /    37 tokens (   18.67 ms per token,    53.56 tokens per second)\n",
      "llama_print_timings:        eval time =   14397.75 ms /   179 runs   (   80.43 ms per token,    12.43 tokens per second)\n",
      "llama_print_timings:       total time =   17025.11 ms /   216 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra data: line 3 column 1 (char 178)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     143.68 ms /   181 runs   (    0.79 ms per token,  1259.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =     585.70 ms /    48 tokens (   12.20 ms per token,    81.95 tokens per second)\n",
      "llama_print_timings:        eval time =   14554.10 ms /   180 runs   (   80.86 ms per token,    12.37 tokens per second)\n",
      "llama_print_timings:       total time =   17145.35 ms /   228 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      34.52 ms /    44 runs   (    0.78 ms per token,  1274.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2280.59 ms /    29 tokens (   78.64 ms per token,    12.72 tokens per second)\n",
      "llama_print_timings:        eval time =    3477.99 ms /    43 runs   (   80.88 ms per token,    12.36 tokens per second)\n",
      "llama_print_timings:       total time =    6278.78 ms /    72 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      54.84 ms /    65 runs   (    0.84 ms per token,  1185.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =     954.89 ms /    40 tokens (   23.87 ms per token,    41.89 tokens per second)\n",
      "llama_print_timings:        eval time =    5068.22 ms /    64 runs   (   79.19 ms per token,    12.63 tokens per second)\n",
      "llama_print_timings:       total time =    6736.76 ms /   104 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     128.55 ms /   159 runs   (    0.81 ms per token,  1236.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =     650.49 ms /    35 tokens (   18.59 ms per token,    53.81 tokens per second)\n",
      "llama_print_timings:        eval time =   12607.26 ms /   158 runs   (   79.79 ms per token,    12.53 tokens per second)\n",
      "llama_print_timings:       total time =   15026.12 ms /   193 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      77.24 ms /   104 runs   (    0.74 ms per token,  1346.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1045.04 ms /    35 tokens (   29.86 ms per token,    33.49 tokens per second)\n",
      "llama_print_timings:        eval time =    8266.33 ms /   103 runs   (   80.26 ms per token,    12.46 tokens per second)\n",
      "llama_print_timings:       total time =   10426.25 ms /   138 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     106.21 ms /   134 runs   (    0.79 ms per token,  1261.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =     679.67 ms /    47 tokens (   14.46 ms per token,    69.15 tokens per second)\n",
      "llama_print_timings:        eval time =   10678.94 ms /   133 runs   (   80.29 ms per token,    12.45 tokens per second)\n",
      "llama_print_timings:       total time =   12820.19 ms /   180 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      98.97 ms /   120 runs   (    0.82 ms per token,  1212.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1342.58 ms /    36 tokens (   37.29 ms per token,    26.81 tokens per second)\n",
      "llama_print_timings:        eval time =    9469.86 ms /   119 runs   (   79.58 ms per token,    12.57 tokens per second)\n",
      "llama_print_timings:       total time =   12173.50 ms /   155 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     157.54 ms /   198 runs   (    0.80 ms per token,  1256.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1285.03 ms /    57 tokens (   22.54 ms per token,    44.36 tokens per second)\n",
      "llama_print_timings:        eval time =   15929.18 ms /   197 runs   (   80.86 ms per token,    12.37 tokens per second)\n",
      "llama_print_timings:       total time =   19583.83 ms /   254 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      80.92 ms /   102 runs   (    0.79 ms per token,  1260.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =     698.12 ms /    39 tokens (   17.90 ms per token,    55.86 tokens per second)\n",
      "llama_print_timings:        eval time =    8065.58 ms /   101 runs   (   79.86 ms per token,    12.52 tokens per second)\n",
      "llama_print_timings:       total time =    9911.89 ms /   140 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     113.47 ms /   135 runs   (    0.84 ms per token,  1189.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =     395.95 ms /    29 tokens (   13.65 ms per token,    73.24 tokens per second)\n",
      "llama_print_timings:        eval time =   10649.60 ms /   134 runs   (   79.47 ms per token,    12.58 tokens per second)\n",
      "llama_print_timings:       total time =   12542.62 ms /   163 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      22.78 ms /    30 runs   (    0.76 ms per token,  1317.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =     977.72 ms /    15 tokens (   65.18 ms per token,    15.34 tokens per second)\n",
      "llama_print_timings:        eval time =    2324.49 ms /    29 runs   (   80.15 ms per token,    12.48 tokens per second)\n",
      "llama_print_timings:       total time =    3664.79 ms /    44 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     120.00 ms /   142 runs   (    0.85 ms per token,  1183.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =     671.88 ms /    41 tokens (   16.39 ms per token,    61.02 tokens per second)\n",
      "llama_print_timings:        eval time =   11210.39 ms /   141 runs   (   79.51 ms per token,    12.58 tokens per second)\n",
      "llama_print_timings:       total time =   13481.47 ms /   182 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     160.91 ms /   200 runs   (    0.80 ms per token,  1242.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1479.06 ms /    45 tokens (   32.87 ms per token,    30.42 tokens per second)\n",
      "llama_print_timings:        eval time =   16153.59 ms /   199 runs   (   81.17 ms per token,    12.32 tokens per second)\n",
      "llama_print_timings:       total time =   19917.26 ms /   244 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra data: line 3 column 1 (char 220)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     100.18 ms /   132 runs   (    0.76 ms per token,  1317.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =     582.43 ms /    40 tokens (   14.56 ms per token,    68.68 tokens per second)\n",
      "llama_print_timings:        eval time =   10568.27 ms /   131 runs   (   80.67 ms per token,    12.40 tokens per second)\n",
      "llama_print_timings:       total time =   12611.56 ms /   171 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra data: line 3 column 1 (char 224)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     108.76 ms /   132 runs   (    0.82 ms per token,  1213.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =     577.55 ms /    39 tokens (   14.81 ms per token,    67.53 tokens per second)\n",
      "llama_print_timings:        eval time =   10393.64 ms /   131 runs   (   79.34 ms per token,    12.60 tokens per second)\n",
      "llama_print_timings:       total time =   12468.98 ms /   170 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     158.25 ms /   188 runs   (    0.84 ms per token,  1187.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =     688.10 ms /    52 tokens (   13.23 ms per token,    75.57 tokens per second)\n",
      "llama_print_timings:        eval time =   14981.27 ms /   187 runs   (   80.11 ms per token,    12.48 tokens per second)\n",
      "llama_print_timings:       total time =   17840.46 ms /   239 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      26.44 ms /    33 runs   (    0.80 ms per token,  1247.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =     411.67 ms /    14 tokens (   29.40 ms per token,    34.01 tokens per second)\n",
      "llama_print_timings:        eval time =    2521.98 ms /    32 runs   (   78.81 ms per token,    12.69 tokens per second)\n",
      "llama_print_timings:       total time =    3290.57 ms /    46 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      43.49 ms /    53 runs   (    0.82 ms per token,  1218.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =     657.57 ms /    37 tokens (   17.77 ms per token,    56.27 tokens per second)\n",
      "llama_print_timings:        eval time =    4186.64 ms /    52 runs   (   80.51 ms per token,    12.42 tokens per second)\n",
      "llama_print_timings:       total time =    5722.79 ms /    89 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     633.50 ms /   901 runs   (    0.70 ms per token,  1422.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2777.36 ms /   173 tokens (   16.05 ms per token,    62.29 tokens per second)\n",
      "llama_print_timings:        eval time =   76401.27 ms /   900 runs   (   84.89 ms per token,    11.78 tokens per second)\n",
      "llama_print_timings:       total time =   91924.35 ms /  1073 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     191.12 ms /   249 runs   (    0.77 ms per token,  1302.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1760.25 ms /    41 tokens (   42.93 ms per token,    23.29 tokens per second)\n",
      "llama_print_timings:        eval time =   20139.96 ms /   248 runs   (   81.21 ms per token,    12.31 tokens per second)\n",
      "llama_print_timings:       total time =   24707.63 ms /   289 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     102.75 ms /   139 runs   (    0.74 ms per token,  1352.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =     685.01 ms /    54 tokens (   12.69 ms per token,    78.83 tokens per second)\n",
      "llama_print_timings:        eval time =   11436.78 ms /   138 runs   (   82.88 ms per token,    12.07 tokens per second)\n",
      "llama_print_timings:       total time =   13699.72 ms /   192 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      44.56 ms /    63 runs   (    0.71 ms per token,  1413.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2589.58 ms /    47 tokens (   55.10 ms per token,    18.15 tokens per second)\n",
      "llama_print_timings:        eval time =    5082.30 ms /    62 runs   (   81.97 ms per token,    12.20 tokens per second)\n",
      "llama_print_timings:       total time =    8304.60 ms /   109 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      69.23 ms /    86 runs   (    0.80 ms per token,  1242.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1071.19 ms /    76 tokens (   14.09 ms per token,    70.95 tokens per second)\n",
      "llama_print_timings:        eval time =    6978.27 ms /    85 runs   (   82.10 ms per token,    12.18 tokens per second)\n",
      "llama_print_timings:       total time =    9349.26 ms /   161 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      16.42 ms /    27 runs   (    0.61 ms per token,  1644.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =     482.14 ms /    12 tokens (   40.18 ms per token,    24.89 tokens per second)\n",
      "llama_print_timings:        eval time =    2035.29 ms /    26 runs   (   78.28 ms per token,    12.77 tokens per second)\n",
      "llama_print_timings:       total time =    2812.62 ms /    38 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      29.51 ms /    40 runs   (    0.74 ms per token,  1355.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =     403.75 ms /    25 tokens (   16.15 ms per token,    61.92 tokens per second)\n",
      "llama_print_timings:        eval time =    3170.43 ms /    39 runs   (   81.29 ms per token,    12.30 tokens per second)\n",
      "llama_print_timings:       total time =    4005.95 ms /    64 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      48.25 ms /    58 runs   (    0.83 ms per token,  1202.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =     686.80 ms /    46 tokens (   14.93 ms per token,    66.98 tokens per second)\n",
      "llama_print_timings:        eval time =    4524.32 ms /    57 runs   (   79.37 ms per token,    12.60 tokens per second)\n",
      "llama_print_timings:       total time =    5868.39 ms /   103 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     132.53 ms /   178 runs   (    0.74 ms per token,  1343.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1286.02 ms /    64 tokens (   20.09 ms per token,    49.77 tokens per second)\n",
      "llama_print_timings:        eval time =   14781.91 ms /   177 runs   (   83.51 ms per token,    11.97 tokens per second)\n",
      "llama_print_timings:       total time =   18266.48 ms /   241 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     217.38 ms /   266 runs   (    0.82 ms per token,  1223.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =     703.77 ms /    63 tokens (   11.17 ms per token,    89.52 tokens per second)\n",
      "llama_print_timings:        eval time =   21280.58 ms /   265 runs   (   80.30 ms per token,    12.45 tokens per second)\n",
      "llama_print_timings:       total time =   25229.16 ms /   328 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra data: line 3 column 1 (char 296)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      86.99 ms /   115 runs   (    0.76 ms per token,  1322.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =     626.97 ms /    45 tokens (   13.93 ms per token,    71.77 tokens per second)\n",
      "llama_print_timings:        eval time =    9212.64 ms /   114 runs   (   80.81 ms per token,    12.37 tokens per second)\n",
      "llama_print_timings:       total time =   11165.28 ms /   159 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     173.25 ms /   237 runs   (    0.73 ms per token,  1367.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =     900.25 ms /    46 tokens (   19.57 ms per token,    51.10 tokens per second)\n",
      "llama_print_timings:        eval time =   19611.38 ms /   236 runs   (   83.10 ms per token,    12.03 tokens per second)\n",
      "llama_print_timings:       total time =   23196.63 ms /   282 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     169.02 ms /   225 runs   (    0.75 ms per token,  1331.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =     688.57 ms /    55 tokens (   12.52 ms per token,    79.88 tokens per second)\n",
      "llama_print_timings:        eval time =   18535.28 ms /   224 runs   (   82.75 ms per token,    12.09 tokens per second)\n",
      "llama_print_timings:       total time =   21773.06 ms /   279 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     161.85 ms /   232 runs   (    0.70 ms per token,  1433.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2476.77 ms /    56 tokens (   44.23 ms per token,    22.61 tokens per second)\n",
      "llama_print_timings:        eval time =   19173.93 ms /   231 runs   (   83.00 ms per token,    12.05 tokens per second)\n",
      "llama_print_timings:       total time =   24241.58 ms /   287 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     132.08 ms /   181 runs   (    0.73 ms per token,  1370.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =     708.19 ms /    59 tokens (   12.00 ms per token,    83.31 tokens per second)\n",
      "llama_print_timings:        eval time =   14903.87 ms /   180 runs   (   82.80 ms per token,    12.08 tokens per second)\n",
      "llama_print_timings:       total time =   17637.79 ms /   239 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      43.74 ms /    61 runs   (    0.72 ms per token,  1394.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =     691.68 ms /    45 tokens (   15.37 ms per token,    65.06 tokens per second)\n",
      "llama_print_timings:        eval time =    5044.56 ms /    60 runs   (   84.08 ms per token,    11.89 tokens per second)\n",
      "llama_print_timings:       total time =    6431.10 ms /   105 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      37.56 ms /    53 runs   (    0.71 ms per token,  1411.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =     665.81 ms /    36 tokens (   18.49 ms per token,    54.07 tokens per second)\n",
      "llama_print_timings:        eval time =    4105.72 ms /    52 runs   (   78.96 ms per token,    12.67 tokens per second)\n",
      "llama_print_timings:       total time =    5321.07 ms /    88 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     139.25 ms /   184 runs   (    0.76 ms per token,  1321.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1547.51 ms /    62 tokens (   24.96 ms per token,    40.06 tokens per second)\n",
      "llama_print_timings:        eval time =   14902.32 ms /   183 runs   (   81.43 ms per token,    12.28 tokens per second)\n",
      "llama_print_timings:       total time =   18566.71 ms /   245 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      53.44 ms /    63 runs   (    0.85 ms per token,  1179.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =     710.07 ms /    48 tokens (   14.79 ms per token,    67.60 tokens per second)\n",
      "llama_print_timings:        eval time =    5067.68 ms /    62 runs   (   81.74 ms per token,    12.23 tokens per second)\n",
      "llama_print_timings:       total time =    6528.13 ms /   110 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     121.45 ms /   156 runs   (    0.78 ms per token,  1284.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =     697.33 ms /    42 tokens (   16.60 ms per token,    60.23 tokens per second)\n",
      "llama_print_timings:        eval time =   12464.35 ms /   155 runs   (   80.42 ms per token,    12.44 tokens per second)\n",
      "llama_print_timings:       total time =   14921.85 ms /   197 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      99.82 ms /   123 runs   (    0.81 ms per token,  1232.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =     732.08 ms /    43 tokens (   17.03 ms per token,    58.74 tokens per second)\n",
      "llama_print_timings:        eval time =    9721.82 ms /   122 runs   (   79.69 ms per token,    12.55 tokens per second)\n",
      "llama_print_timings:       total time =   11834.97 ms /   165 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     142.34 ms /   196 runs   (    0.73 ms per token,  1377.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1860.44 ms /    54 tokens (   34.45 ms per token,    29.03 tokens per second)\n",
      "llama_print_timings:        eval time =   16098.19 ms /   195 runs   (   82.55 ms per token,    12.11 tokens per second)\n",
      "llama_print_timings:       total time =   20292.06 ms /   249 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     186.33 ms /   245 runs   (    0.76 ms per token,  1314.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =     718.63 ms /    61 tokens (   11.78 ms per token,    84.88 tokens per second)\n",
      "llama_print_timings:        eval time =   19855.74 ms /   244 runs   (   81.38 ms per token,    12.29 tokens per second)\n",
      "llama_print_timings:       total time =   23416.00 ms /   305 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra data: line 3 column 1 (char 259)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     109.81 ms /   164 runs   (    0.67 ms per token,  1493.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =     318.82 ms /    28 tokens (   11.39 ms per token,    87.82 tokens per second)\n",
      "llama_print_timings:        eval time =   13754.77 ms /   163 runs   (   84.39 ms per token,    11.85 tokens per second)\n",
      "llama_print_timings:       total time =   15700.72 ms /   191 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unterminated string starting at: line 1 column 53 (char 52)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     129.86 ms /   188 runs   (    0.69 ms per token,  1447.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =     606.94 ms /    62 tokens (    9.79 ms per token,   102.15 tokens per second)\n",
      "llama_print_timings:        eval time =   15453.95 ms /   187 runs   (   82.64 ms per token,    12.10 tokens per second)\n",
      "llama_print_timings:       total time =   18147.47 ms /   249 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      50.95 ms /    83 runs   (    0.61 ms per token,  1629.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1678.63 ms /    31 tokens (   54.15 ms per token,    18.47 tokens per second)\n",
      "llama_print_timings:        eval time =    7672.38 ms /    82 runs   (   93.57 ms per token,    10.69 tokens per second)\n",
      "llama_print_timings:       total time =   10223.78 ms /   113 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     122.90 ms /   171 runs   (    0.72 ms per token,  1391.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1445.84 ms /    29 tokens (   49.86 ms per token,    20.06 tokens per second)\n",
      "llama_print_timings:        eval time =   14004.99 ms /   170 runs   (   82.38 ms per token,    12.14 tokens per second)\n",
      "llama_print_timings:       total time =   17373.60 ms /   199 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     130.59 ms /   201 runs   (    0.65 ms per token,  1539.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =     905.61 ms /    52 tokens (   17.42 ms per token,    57.42 tokens per second)\n",
      "llama_print_timings:        eval time =   16680.84 ms /   200 runs   (   83.40 ms per token,    11.99 tokens per second)\n",
      "llama_print_timings:       total time =   19890.65 ms /   252 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     150.98 ms /   190 runs   (    0.79 ms per token,  1258.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =     701.98 ms /    53 tokens (   13.24 ms per token,    75.50 tokens per second)\n",
      "llama_print_timings:        eval time =   15282.62 ms /   189 runs   (   80.86 ms per token,    12.37 tokens per second)\n",
      "llama_print_timings:       total time =   18243.19 ms /   242 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      45.34 ms /    61 runs   (    0.74 ms per token,  1345.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =     704.62 ms /    45 tokens (   15.66 ms per token,    63.86 tokens per second)\n",
      "llama_print_timings:        eval time =    4743.36 ms /    60 runs   (   79.06 ms per token,    12.65 tokens per second)\n",
      "llama_print_timings:       total time =    6113.33 ms /   105 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     133.58 ms /   183 runs   (    0.73 ms per token,  1369.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =     668.42 ms /    33 tokens (   20.26 ms per token,    49.37 tokens per second)\n",
      "llama_print_timings:        eval time =   15084.27 ms /   182 runs   (   82.88 ms per token,    12.07 tokens per second)\n",
      "llama_print_timings:       total time =   17796.35 ms /   215 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     111.34 ms /   147 runs   (    0.76 ms per token,  1320.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =     741.11 ms /    45 tokens (   16.47 ms per token,    60.72 tokens per second)\n",
      "llama_print_timings:        eval time =   11820.41 ms /   146 runs   (   80.96 ms per token,    12.35 tokens per second)\n",
      "llama_print_timings:       total time =   14277.05 ms /   191 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     190.31 ms /   264 runs   (    0.72 ms per token,  1387.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =     727.57 ms /    62 tokens (   11.74 ms per token,    85.21 tokens per second)\n",
      "llama_print_timings:        eval time =   21509.88 ms /   263 runs   (   81.79 ms per token,    12.23 tokens per second)\n",
      "llama_print_timings:       total time =   25303.36 ms /   325 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      80.28 ms /   102 runs   (    0.79 ms per token,  1270.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =     682.18 ms /    52 tokens (   13.12 ms per token,    76.23 tokens per second)\n",
      "llama_print_timings:        eval time =    8066.54 ms /   101 runs   (   79.87 ms per token,    12.52 tokens per second)\n",
      "llama_print_timings:       total time =    9875.65 ms /   153 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      89.91 ms /   119 runs   (    0.76 ms per token,  1323.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1179.15 ms /    37 tokens (   31.87 ms per token,    31.38 tokens per second)\n",
      "llama_print_timings:        eval time =    9540.27 ms /   118 runs   (   80.85 ms per token,    12.37 tokens per second)\n",
      "llama_print_timings:       total time =   12059.38 ms /   155 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      50.10 ms /    64 runs   (    0.78 ms per token,  1277.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =     678.65 ms /    48 tokens (   14.14 ms per token,    70.73 tokens per second)\n",
      "llama_print_timings:        eval time =    5024.90 ms /    63 runs   (   79.76 ms per token,    12.54 tokens per second)\n",
      "llama_print_timings:       total time =    6409.41 ms /   111 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     176.23 ms /   230 runs   (    0.77 ms per token,  1305.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =     984.30 ms /    83 tokens (   11.86 ms per token,    84.32 tokens per second)\n",
      "llama_print_timings:        eval time =   18748.06 ms /   229 runs   (   81.87 ms per token,    12.21 tokens per second)\n",
      "llama_print_timings:       total time =   22655.70 ms /   312 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      41.27 ms /    56 runs   (    0.74 ms per token,  1357.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2598.72 ms /    40 tokens (   64.97 ms per token,    15.39 tokens per second)\n",
      "llama_print_timings:        eval time =    4406.64 ms /    55 runs   (   80.12 ms per token,    12.48 tokens per second)\n",
      "llama_print_timings:       total time =    7592.43 ms /    95 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     164.33 ms /   262 runs   (    0.63 ms per token,  1594.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3516.91 ms /    65 tokens (   54.11 ms per token,    18.48 tokens per second)\n",
      "llama_print_timings:        eval time =   25205.40 ms /   261 runs   (   96.57 ms per token,    10.35 tokens per second)\n",
      "llama_print_timings:       total time =   31568.00 ms /   326 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      12.49 ms /    40 runs   (    0.31 ms per token,  3203.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5469.60 ms /    28 tokens (  195.34 ms per token,     5.12 tokens per second)\n",
      "llama_print_timings:        eval time =    3199.98 ms /    39 runs   (   82.05 ms per token,    12.19 tokens per second)\n",
      "llama_print_timings:       total time =    8961.71 ms /    67 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      55.34 ms /   205 runs   (    0.27 ms per token,  3704.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =     784.48 ms /    37 tokens (   21.20 ms per token,    47.17 tokens per second)\n",
      "llama_print_timings:        eval time =   17312.26 ms /   204 runs   (   84.86 ms per token,    11.78 tokens per second)\n",
      "llama_print_timings:       total time =   19289.69 ms /   241 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      69.54 ms /   235 runs   (    0.30 ms per token,  3379.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1364.35 ms /    42 tokens (   32.48 ms per token,    30.78 tokens per second)\n",
      "llama_print_timings:        eval time =   19323.18 ms /   234 runs   (   82.58 ms per token,    12.11 tokens per second)\n",
      "llama_print_timings:       total time =   22164.12 ms /   276 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra data: line 3 column 1 (char 228)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      14.64 ms /    60 runs   (    0.24 ms per token,  4099.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =     590.14 ms /    44 tokens (   13.41 ms per token,    74.56 tokens per second)\n",
      "llama_print_timings:        eval time =    5182.52 ms /    59 runs   (   87.84 ms per token,    11.38 tokens per second)\n",
      "llama_print_timings:       total time =    6063.70 ms /   103 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      46.10 ms /   157 runs   (    0.29 ms per token,  3405.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =     703.47 ms /    44 tokens (   15.99 ms per token,    62.55 tokens per second)\n",
      "llama_print_timings:        eval time =   13119.64 ms /   156 runs   (   84.10 ms per token,    11.89 tokens per second)\n",
      "llama_print_timings:       total time =   14760.27 ms /   200 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      45.80 ms /   147 runs   (    0.31 ms per token,  3209.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1381.01 ms /    42 tokens (   32.88 ms per token,    30.41 tokens per second)\n",
      "llama_print_timings:        eval time =   11987.85 ms /   146 runs   (   82.11 ms per token,    12.18 tokens per second)\n",
      "llama_print_timings:       total time =   14280.71 ms /   188 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      51.71 ms /   187 runs   (    0.28 ms per token,  3616.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =     423.35 ms /    32 tokens (   13.23 ms per token,    75.59 tokens per second)\n",
      "llama_print_timings:        eval time =   15584.86 ms /   186 runs   (   83.79 ms per token,    11.93 tokens per second)\n",
      "llama_print_timings:       total time =   17046.63 ms /   218 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unterminated string starting at: line 1 column 53 (char 52)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      18.85 ms /    64 runs   (    0.29 ms per token,  3395.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =     589.10 ms /    48 tokens (   12.27 ms per token,    81.48 tokens per second)\n",
      "llama_print_timings:        eval time =    5108.08 ms /    63 runs   (   81.08 ms per token,    12.33 tokens per second)\n",
      "llama_print_timings:       total time =    6056.35 ms /   111 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      29.54 ms /   106 runs   (    0.28 ms per token,  3588.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1156.97 ms /    30 tokens (   38.57 ms per token,    25.93 tokens per second)\n",
      "llama_print_timings:        eval time =    8626.30 ms /   105 runs   (   82.16 ms per token,    12.17 tokens per second)\n",
      "llama_print_timings:       total time =   10362.82 ms /   135 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      66.28 ms /   222 runs   (    0.30 ms per token,  3349.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1953.91 ms /    38 tokens (   51.42 ms per token,    19.45 tokens per second)\n",
      "llama_print_timings:        eval time =   18361.51 ms /   221 runs   (   83.08 ms per token,    12.04 tokens per second)\n",
      "llama_print_timings:       total time =   21700.87 ms /   259 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      37.37 ms /   126 runs   (    0.30 ms per token,  3371.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =     679.91 ms /    33 tokens (   20.60 ms per token,    48.54 tokens per second)\n",
      "llama_print_timings:        eval time =   10332.22 ms /   125 runs   (   82.66 ms per token,    12.10 tokens per second)\n",
      "llama_print_timings:       total time =   11785.56 ms /   158 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      85.50 ms /   302 runs   (    0.28 ms per token,  3532.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3133.50 ms /    46 tokens (   68.12 ms per token,    14.68 tokens per second)\n",
      "llama_print_timings:        eval time =   25321.11 ms /   301 runs   (   84.12 ms per token,    11.89 tokens per second)\n",
      "llama_print_timings:       total time =   30428.89 ms /   347 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      68.50 ms /   214 runs   (    0.32 ms per token,  3124.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =     778.25 ms /    49 tokens (   15.88 ms per token,    62.96 tokens per second)\n",
      "llama_print_timings:        eval time =   17635.55 ms /   213 runs   (   82.80 ms per token,    12.08 tokens per second)\n",
      "llama_print_timings:       total time =   19910.40 ms /   262 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      58.75 ms /   190 runs   (    0.31 ms per token,  3234.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =     710.72 ms /    47 tokens (   15.12 ms per token,    66.13 tokens per second)\n",
      "llama_print_timings:        eval time =   15376.06 ms /   189 runs   (   81.35 ms per token,    12.29 tokens per second)\n",
      "llama_print_timings:       total time =   17277.77 ms /   236 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      43.11 ms /   152 runs   (    0.28 ms per token,  3525.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1163.89 ms /    44 tokens (   26.45 ms per token,    37.80 tokens per second)\n",
      "llama_print_timings:        eval time =   12763.01 ms /   151 runs   (   84.52 ms per token,    11.83 tokens per second)\n",
      "llama_print_timings:       total time =   14884.82 ms /   195 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      46.66 ms /   165 runs   (    0.28 ms per token,  3536.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1172.76 ms /    44 tokens (   26.65 ms per token,    37.52 tokens per second)\n",
      "llama_print_timings:        eval time =   13727.44 ms /   164 runs   (   83.70 ms per token,    11.95 tokens per second)\n",
      "llama_print_timings:       total time =   15910.89 ms /   208 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      33.52 ms /   160 runs   (    0.21 ms per token,  4773.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =     733.50 ms /    60 tokens (   12.22 ms per token,    81.80 tokens per second)\n",
      "llama_print_timings:        eval time =   14891.57 ms /   159 runs   (   93.66 ms per token,    10.68 tokens per second)\n",
      "llama_print_timings:       total time =   16529.09 ms /   219 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      39.75 ms /   133 runs   (    0.30 ms per token,  3346.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1898.54 ms /    37 tokens (   51.31 ms per token,    19.49 tokens per second)\n",
      "llama_print_timings:        eval time =   11017.17 ms /   132 runs   (   83.46 ms per token,    11.98 tokens per second)\n",
      "llama_print_timings:       total time =   13721.05 ms /   169 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      62.87 ms /   220 runs   (    0.29 ms per token,  3499.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1437.26 ms /    36 tokens (   39.92 ms per token,    25.05 tokens per second)\n",
      "llama_print_timings:        eval time =   18181.09 ms /   219 runs   (   83.02 ms per token,    12.05 tokens per second)\n",
      "llama_print_timings:       total time =   20978.70 ms /   255 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      19.21 ms /    68 runs   (    0.28 ms per token,  3540.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1530.28 ms /    52 tokens (   29.43 ms per token,    33.98 tokens per second)\n",
      "llama_print_timings:        eval time =    5632.77 ms /    67 runs   (   84.07 ms per token,    11.89 tokens per second)\n",
      "llama_print_timings:       total time =    7578.24 ms /   119 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      83.30 ms /   295 runs   (    0.28 ms per token,  3541.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =     725.04 ms /    58 tokens (   12.50 ms per token,    80.00 tokens per second)\n",
      "llama_print_timings:        eval time =   24624.28 ms /   294 runs   (   83.76 ms per token,    11.94 tokens per second)\n",
      "llama_print_timings:       total time =   27201.65 ms /   352 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      18.85 ms /    62 runs   (    0.30 ms per token,  3288.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3582.12 ms /    48 tokens (   74.63 ms per token,    13.40 tokens per second)\n",
      "llama_print_timings:        eval time =    5126.14 ms /    61 runs   (   84.04 ms per token,    11.90 tokens per second)\n",
      "llama_print_timings:       total time =    9094.83 ms /   109 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      12.26 ms /    43 runs   (    0.29 ms per token,  3507.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =     689.87 ms /    27 tokens (   25.55 ms per token,    39.14 tokens per second)\n",
      "llama_print_timings:        eval time =    3439.81 ms /    42 runs   (   81.90 ms per token,    12.21 tokens per second)\n",
      "llama_print_timings:       total time =    4385.35 ms /    69 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      29.76 ms /   104 runs   (    0.29 ms per token,  3494.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =     808.65 ms /    27 tokens (   29.95 ms per token,    33.39 tokens per second)\n",
      "llama_print_timings:        eval time =    8574.94 ms /   103 runs   (   83.25 ms per token,    12.01 tokens per second)\n",
      "llama_print_timings:       total time =   10066.43 ms /   130 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra data: line 3 column 1 (char 151)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      62.46 ms /   216 runs   (    0.29 ms per token,  3458.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =     329.29 ms /    31 tokens (   10.62 ms per token,    94.14 tokens per second)\n",
      "llama_print_timings:        eval time =   17951.50 ms /   215 runs   (   83.50 ms per token,    11.98 tokens per second)\n",
      "llama_print_timings:       total time =   19614.44 ms /   246 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra data: line 3 column 1 (char 192)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      38.69 ms /   136 runs   (    0.28 ms per token,  3515.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =     578.50 ms /    36 tokens (   16.07 ms per token,    62.23 tokens per second)\n",
      "llama_print_timings:        eval time =   11308.53 ms /   135 runs   (   83.77 ms per token,    11.94 tokens per second)\n",
      "llama_print_timings:       total time =   12682.46 ms /   171 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      10.33 ms /    35 runs   (    0.30 ms per token,  3388.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1318.90 ms /    20 tokens (   65.94 ms per token,    15.16 tokens per second)\n",
      "llama_print_timings:        eval time =    2745.06 ms /    34 runs   (   80.74 ms per token,    12.39 tokens per second)\n",
      "llama_print_timings:       total time =    4287.20 ms /    54 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      22.50 ms /    58 runs   (    0.39 ms per token,  2577.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =     663.51 ms /    43 tokens (   15.43 ms per token,    64.81 tokens per second)\n",
      "llama_print_timings:        eval time =    4468.28 ms /    57 runs   (   78.39 ms per token,    12.76 tokens per second)\n",
      "llama_print_timings:       total time =    5632.51 ms /   100 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      34.07 ms /   118 runs   (    0.29 ms per token,  3463.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1772.49 ms /    59 tokens (   30.04 ms per token,    33.29 tokens per second)\n",
      "llama_print_timings:        eval time =   10396.70 ms /   117 runs   (   88.86 ms per token,    11.25 tokens per second)\n",
      "llama_print_timings:       total time =   12917.92 ms /   176 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =       9.19 ms /    31 runs   (    0.30 ms per token,  3372.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =     410.55 ms /    13 tokens (   31.58 ms per token,    31.66 tokens per second)\n",
      "llama_print_timings:        eval time =    2420.43 ms /    30 runs   (   80.68 ms per token,    12.39 tokens per second)\n",
      "llama_print_timings:       total time =    3007.00 ms /    43 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      64.56 ms /   223 runs   (    0.29 ms per token,  3454.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =     995.98 ms /    76 tokens (   13.10 ms per token,    76.31 tokens per second)\n",
      "llama_print_timings:        eval time =   18465.24 ms /   222 runs   (   83.18 ms per token,    12.02 tokens per second)\n",
      "llama_print_timings:       total time =   20936.42 ms /   298 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =       8.82 ms /    33 runs   (    0.27 ms per token,  3742.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =     850.22 ms /    18 tokens (   47.23 ms per token,    21.17 tokens per second)\n",
      "llama_print_timings:        eval time =    2634.14 ms /    32 runs   (   82.32 ms per token,    12.15 tokens per second)\n",
      "llama_print_timings:       total time =    3662.90 ms /    50 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      86.18 ms /   284 runs   (    0.30 ms per token,  3295.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =     952.04 ms /    71 tokens (   13.41 ms per token,    74.58 tokens per second)\n",
      "llama_print_timings:        eval time =   23386.78 ms /   283 runs   (   82.64 ms per token,    12.10 tokens per second)\n",
      "llama_print_timings:       total time =   26339.93 ms /   354 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      23.64 ms /    79 runs   (    0.30 ms per token,  3341.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1233.29 ms /    65 tokens (   18.97 ms per token,    52.70 tokens per second)\n",
      "llama_print_timings:        eval time =    6343.54 ms /    78 runs   (   81.33 ms per token,    12.30 tokens per second)\n",
      "llama_print_timings:       total time =    8040.46 ms /   143 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      39.51 ms /   207 runs   (    0.19 ms per token,  5239.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1009.85 ms /    78 tokens (   12.95 ms per token,    77.24 tokens per second)\n",
      "llama_print_timings:        eval time =   19732.39 ms /   206 runs   (   95.79 ms per token,    10.44 tokens per second)\n",
      "llama_print_timings:       total time =   21878.44 ms /   284 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      67.14 ms /   254 runs   (    0.26 ms per token,  3782.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1715.59 ms /    49 tokens (   35.01 ms per token,    28.56 tokens per second)\n",
      "llama_print_timings:        eval time =   21531.94 ms /   253 runs   (   85.11 ms per token,    11.75 tokens per second)\n",
      "llama_print_timings:       total time =   25088.32 ms /   302 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      27.77 ms /    98 runs   (    0.28 ms per token,  3528.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =     759.74 ms /    12 tokens (   63.31 ms per token,    15.79 tokens per second)\n",
      "llama_print_timings:        eval time =    8002.21 ms /    97 runs   (   82.50 ms per token,    12.12 tokens per second)\n",
      "llama_print_timings:       total time =    9386.75 ms /   109 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra data: line 3 column 1 (char 85)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      16.07 ms /    60 runs   (    0.27 ms per token,  3734.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =     584.69 ms /    45 tokens (   12.99 ms per token,    76.96 tokens per second)\n",
      "llama_print_timings:        eval time =    4898.53 ms /    59 runs   (   83.03 ms per token,    12.04 tokens per second)\n",
      "llama_print_timings:       total time =    5798.12 ms /   104 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      21.66 ms /    74 runs   (    0.29 ms per token,  3416.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =     703.09 ms /    59 tokens (   11.92 ms per token,    83.92 tokens per second)\n",
      "llama_print_timings:        eval time =    6041.45 ms /    73 runs   (   82.76 ms per token,    12.08 tokens per second)\n",
      "llama_print_timings:       total time =    7285.33 ms /   132 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      55.58 ms /   209 runs   (    0.27 ms per token,  3760.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =     725.37 ms /    56 tokens (   12.95 ms per token,    77.20 tokens per second)\n",
      "llama_print_timings:        eval time =   18114.69 ms /   208 runs   (   87.09 ms per token,    11.48 tokens per second)\n",
      "llama_print_timings:       total time =   20086.77 ms /   264 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      22.33 ms /    75 runs   (    0.30 ms per token,  3359.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =     707.09 ms /    60 tokens (   11.78 ms per token,    84.86 tokens per second)\n",
      "llama_print_timings:        eval time =    6043.62 ms /    74 runs   (   81.67 ms per token,    12.24 tokens per second)\n",
      "llama_print_timings:       total time =    7192.12 ms /   134 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      26.35 ms /    86 runs   (    0.31 ms per token,  3264.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =     438.92 ms /    16 tokens (   27.43 ms per token,    36.45 tokens per second)\n",
      "llama_print_timings:        eval time =    6931.24 ms /    85 runs   (   81.54 ms per token,    12.26 tokens per second)\n",
      "llama_print_timings:       total time =    7863.68 ms /   101 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      37.87 ms /   121 runs   (    0.31 ms per token,  3194.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =     989.85 ms /    30 tokens (   32.99 ms per token,    30.31 tokens per second)\n",
      "llama_print_timings:        eval time =    9775.42 ms /   120 runs   (   81.46 ms per token,    12.28 tokens per second)\n",
      "llama_print_timings:       total time =   11491.48 ms /   150 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      51.79 ms /   162 runs   (    0.32 ms per token,  3127.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =     663.94 ms /    39 tokens (   17.02 ms per token,    58.74 tokens per second)\n",
      "llama_print_timings:        eval time =   13251.44 ms /   161 runs   (   82.31 ms per token,    12.15 tokens per second)\n",
      "llama_print_timings:       total time =   14888.02 ms /   200 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expecting ',' delimiter: line 1 column 55 (char 54)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      29.94 ms /   113 runs   (    0.26 ms per token,  3774.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =     324.14 ms /    26 tokens (   12.47 ms per token,    80.21 tokens per second)\n",
      "llama_print_timings:        eval time =    9341.64 ms /   112 runs   (   83.41 ms per token,    11.99 tokens per second)\n",
      "llama_print_timings:       total time =   10294.80 ms /   138 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expecting ',' delimiter: line 1 column 71 (char 70)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      17.47 ms /    59 runs   (    0.30 ms per token,  3376.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =     583.58 ms /    44 tokens (   13.26 ms per token,    75.40 tokens per second)\n",
      "llama_print_timings:        eval time =    4707.67 ms /    58 runs   (   81.17 ms per token,    12.32 tokens per second)\n",
      "llama_print_timings:       total time =    5627.53 ms /   102 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      15.45 ms /    51 runs   (    0.30 ms per token,  3300.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =     716.21 ms /    35 tokens (   20.46 ms per token,    48.87 tokens per second)\n",
      "llama_print_timings:        eval time =    4074.64 ms /    50 runs   (   81.49 ms per token,    12.27 tokens per second)\n",
      "llama_print_timings:       total time =    5082.21 ms /    85 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      56.41 ms /   179 runs   (    0.32 ms per token,  3173.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1817.47 ms /    76 tokens (   23.91 ms per token,    41.82 tokens per second)\n",
      "llama_print_timings:        eval time =   14666.43 ms /   178 runs   (   82.40 ms per token,    12.14 tokens per second)\n",
      "llama_print_timings:       total time =   17652.45 ms /   254 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unterminated string starting at: line 1 column 53 (char 52)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      70.17 ms /   235 runs   (    0.30 ms per token,  3348.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =     588.76 ms /    47 tokens (   12.53 ms per token,    79.83 tokens per second)\n",
      "llama_print_timings:        eval time =   19397.69 ms /   234 runs   (   82.90 ms per token,    12.06 tokens per second)\n",
      "llama_print_timings:       total time =   21372.02 ms /   281 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      67.02 ms /   214 runs   (    0.31 ms per token,  3192.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =     992.84 ms /    82 tokens (   12.11 ms per token,    82.59 tokens per second)\n",
      "llama_print_timings:        eval time =   17547.12 ms /   213 runs   (   82.38 ms per token,    12.14 tokens per second)\n",
      "llama_print_timings:       total time =   19917.04 ms /   295 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      42.80 ms /   140 runs   (    0.31 ms per token,  3271.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =     429.54 ms /    22 tokens (   19.52 ms per token,    51.22 tokens per second)\n",
      "llama_print_timings:        eval time =   11270.54 ms /   139 runs   (   81.08 ms per token,    12.33 tokens per second)\n",
      "llama_print_timings:       total time =   12551.92 ms /   161 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      17.74 ms /    70 runs   (    0.25 ms per token,  3945.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1208.76 ms /    56 tokens (   21.58 ms per token,    46.33 tokens per second)\n",
      "llama_print_timings:        eval time =    5938.09 ms /    69 runs   (   86.06 ms per token,    11.62 tokens per second)\n",
      "llama_print_timings:       total time =    7515.12 ms /   125 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      13.80 ms /    51 runs   (    0.27 ms per token,  3694.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =     830.98 ms /    39 tokens (   21.31 ms per token,    46.93 tokens per second)\n",
      "llama_print_timings:        eval time =    4292.18 ms /    50 runs   (   85.84 ms per token,    11.65 tokens per second)\n",
      "llama_print_timings:       total time =    5471.99 ms /    89 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      67.06 ms /   235 runs   (    0.29 ms per token,  3504.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =     989.75 ms /    70 tokens (   14.14 ms per token,    70.73 tokens per second)\n",
      "llama_print_timings:        eval time =   19529.29 ms /   234 runs   (   83.46 ms per token,    11.98 tokens per second)\n",
      "llama_print_timings:       total time =   22068.39 ms /   304 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      49.91 ms /   162 runs   (    0.31 ms per token,  3245.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =     424.20 ms /    15 tokens (   28.28 ms per token,    35.36 tokens per second)\n",
      "llama_print_timings:        eval time =   13181.31 ms /   161 runs   (   81.87 ms per token,    12.21 tokens per second)\n",
      "llama_print_timings:       total time =   14579.73 ms /   176 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra data: line 3 column 1 (char 80)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      86.69 ms /   282 runs   (    0.31 ms per token,  3252.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =     892.44 ms /    84 tokens (   10.62 ms per token,    94.12 tokens per second)\n",
      "llama_print_timings:        eval time =   23458.46 ms /   281 runs   (   83.48 ms per token,    11.98 tokens per second)\n",
      "llama_print_timings:       total time =   26215.76 ms /   365 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      70.34 ms /   231 runs   (    0.30 ms per token,  3284.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1385.33 ms /    73 tokens (   18.98 ms per token,    52.70 tokens per second)\n",
      "llama_print_timings:        eval time =   18978.05 ms /   230 runs   (   82.51 ms per token,    12.12 tokens per second)\n",
      "llama_print_timings:       total time =   21829.52 ms /   303 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      53.75 ms /   166 runs   (    0.32 ms per token,  3088.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1698.22 ms /    12 tokens (  141.52 ms per token,     7.07 tokens per second)\n",
      "llama_print_timings:        eval time =   13297.70 ms /   165 runs   (   80.59 ms per token,    12.41 tokens per second)\n",
      "llama_print_timings:       total time =   16090.49 ms /   177 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra data: line 3 column 1 (char 91)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     120.75 ms /   335 runs   (    0.36 ms per token,  2774.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1455.13 ms /   144 tokens (   10.11 ms per token,    98.96 tokens per second)\n",
      "llama_print_timings:        eval time =   27077.93 ms /   334 runs   (   81.07 ms per token,    12.33 tokens per second)\n",
      "llama_print_timings:       total time =   31617.85 ms /   478 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      50.31 ms /   130 runs   (    0.39 ms per token,  2583.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =     428.66 ms /    16 tokens (   26.79 ms per token,    37.33 tokens per second)\n",
      "llama_print_timings:        eval time =   10150.85 ms /   129 runs   (   78.69 ms per token,    12.71 tokens per second)\n",
      "llama_print_timings:       total time =   11692.89 ms /   145 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra data: line 3 column 1 (char 91)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      18.22 ms /    50 runs   (    0.36 ms per token,  2744.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =     572.04 ms /    35 tokens (   16.34 ms per token,    61.18 tokens per second)\n",
      "llama_print_timings:        eval time =    4109.99 ms /    49 runs   (   83.88 ms per token,    11.92 tokens per second)\n",
      "llama_print_timings:       total time =    5071.77 ms /    84 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      57.43 ms /   173 runs   (    0.33 ms per token,  3012.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =     954.81 ms /    50 tokens (   19.10 ms per token,    52.37 tokens per second)\n",
      "llama_print_timings:        eval time =   14845.64 ms /   172 runs   (   86.31 ms per token,    11.59 tokens per second)\n",
      "llama_print_timings:       total time =   16944.55 ms /   222 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      29.52 ms /    99 runs   (    0.30 ms per token,  3354.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =     992.95 ms /    83 tokens (   11.96 ms per token,    83.59 tokens per second)\n",
      "llama_print_timings:        eval time =    8016.41 ms /    98 runs   (   81.80 ms per token,    12.22 tokens per second)\n",
      "llama_print_timings:       total time =    9577.30 ms /   181 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      13.41 ms /    52 runs   (    0.26 ms per token,  3877.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3193.25 ms /    37 tokens (   86.30 ms per token,    11.59 tokens per second)\n",
      "llama_print_timings:        eval time =    4345.93 ms /    51 runs   (   85.21 ms per token,    11.74 tokens per second)\n",
      "llama_print_timings:       total time =    7825.51 ms /    88 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      57.67 ms /   176 runs   (    0.33 ms per token,  3051.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =     447.05 ms /    20 tokens (   22.35 ms per token,    44.74 tokens per second)\n",
      "llama_print_timings:        eval time =   14361.32 ms /   175 runs   (   82.06 ms per token,    12.19 tokens per second)\n",
      "llama_print_timings:       total time =   15933.10 ms /   195 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      50.85 ms /   167 runs   (    0.30 ms per token,  3284.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =     437.35 ms /    17 tokens (   25.73 ms per token,    38.87 tokens per second)\n",
      "llama_print_timings:        eval time =   13557.00 ms /   166 runs   (   81.67 ms per token,    12.24 tokens per second)\n",
      "llama_print_timings:       total time =   15016.74 ms /   183 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra data: line 3 column 1 (char 78)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      51.76 ms /   240 runs   (    0.22 ms per token,  4636.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1160.20 ms /   103 tokens (   11.26 ms per token,    88.78 tokens per second)\n",
      "llama_print_timings:        eval time =   22351.17 ms /   239 runs   (   93.52 ms per token,    10.69 tokens per second)\n",
      "llama_print_timings:       total time =   24913.20 ms /   342 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      20.25 ms /    81 runs   (    0.25 ms per token,  3999.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2825.85 ms /    68 tokens (   41.56 ms per token,    24.06 tokens per second)\n",
      "llama_print_timings:        eval time =    6735.91 ms /    80 runs   (   84.20 ms per token,    11.88 tokens per second)\n",
      "llama_print_timings:       total time =   10038.04 ms /   148 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      42.76 ms /   163 runs   (    0.26 ms per token,  3812.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =     443.51 ms /    32 tokens (   13.86 ms per token,    72.15 tokens per second)\n",
      "llama_print_timings:        eval time =   13725.12 ms /   162 runs   (   84.72 ms per token,    11.80 tokens per second)\n",
      "llama_print_timings:       total time =   15119.39 ms /   194 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      16.67 ms /    58 runs   (    0.29 ms per token,  3478.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =     706.09 ms /    42 tokens (   16.81 ms per token,    59.48 tokens per second)\n",
      "llama_print_timings:        eval time =    4669.83 ms /    57 runs   (   81.93 ms per token,    12.21 tokens per second)\n",
      "llama_print_timings:       total time =    5697.18 ms /    99 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      17.76 ms /    63 runs   (    0.28 ms per token,  3547.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1410.51 ms /    47 tokens (   30.01 ms per token,    33.32 tokens per second)\n",
      "llama_print_timings:        eval time =    5063.73 ms /    62 runs   (   81.67 ms per token,    12.24 tokens per second)\n",
      "llama_print_timings:       total time =    6834.64 ms /   109 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      27.34 ms /    94 runs   (    0.29 ms per token,  3438.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1248.98 ms /    15 tokens (   83.27 ms per token,    12.01 tokens per second)\n",
      "llama_print_timings:        eval time =    7668.26 ms /    93 runs   (   82.45 ms per token,    12.13 tokens per second)\n",
      "llama_print_timings:       total time =    9497.16 ms /   108 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      33.96 ms /   110 runs   (    0.31 ms per token,  3239.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =     403.55 ms /    15 tokens (   26.90 ms per token,    37.17 tokens per second)\n",
      "llama_print_timings:        eval time =    8868.20 ms /   109 runs   (   81.36 ms per token,    12.29 tokens per second)\n",
      "llama_print_timings:       total time =    9926.82 ms /   124 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      62.88 ms /   201 runs   (    0.31 ms per token,  3196.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =     701.29 ms /    61 tokens (   11.50 ms per token,    86.98 tokens per second)\n",
      "llama_print_timings:        eval time =   16768.93 ms /   200 runs   (   83.84 ms per token,    11.93 tokens per second)\n",
      "llama_print_timings:       total time =   18945.66 ms /   261 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      62.86 ms /   202 runs   (    0.31 ms per token,  3213.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =     850.92 ms /    41 tokens (   20.75 ms per token,    48.18 tokens per second)\n",
      "llama_print_timings:        eval time =   16600.16 ms /   201 runs   (   82.59 ms per token,    12.11 tokens per second)\n",
      "llama_print_timings:       total time =   18749.48 ms /   242 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      27.21 ms /    88 runs   (    0.31 ms per token,  3234.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =     402.47 ms /    13 tokens (   30.96 ms per token,    32.30 tokens per second)\n",
      "llama_print_timings:        eval time =    7040.71 ms /    87 runs   (   80.93 ms per token,    12.36 tokens per second)\n",
      "llama_print_timings:       total time =    7967.79 ms /   100 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      13.35 ms /    57 runs   (    0.23 ms per token,  4270.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =     992.40 ms /    42 tokens (   23.63 ms per token,    42.32 tokens per second)\n",
      "llama_print_timings:        eval time =    4659.00 ms /    56 runs   (   83.20 ms per token,    12.02 tokens per second)\n",
      "llama_print_timings:       total time =    5934.20 ms /    98 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      25.78 ms /    87 runs   (    0.30 ms per token,  3374.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2869.76 ms /    11 tokens (  260.89 ms per token,     3.83 tokens per second)\n",
      "llama_print_timings:        eval time =    7008.35 ms /    86 runs   (   81.49 ms per token,    12.27 tokens per second)\n",
      "llama_print_timings:       total time =   10413.78 ms /    97 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      25.85 ms /    84 runs   (    0.31 ms per token,  3249.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =     548.52 ms /    10 tokens (   54.85 ms per token,    18.23 tokens per second)\n",
      "llama_print_timings:        eval time =    6778.84 ms /    83 runs   (   81.67 ms per token,    12.24 tokens per second)\n",
      "llama_print_timings:       total time =    7845.19 ms /    93 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      27.68 ms /    91 runs   (    0.30 ms per token,  3287.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =     416.59 ms /    13 tokens (   32.05 ms per token,    31.21 tokens per second)\n",
      "llama_print_timings:        eval time =    7340.59 ms /    90 runs   (   81.56 ms per token,    12.26 tokens per second)\n",
      "llama_print_timings:       total time =    8286.81 ms /   103 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      18.19 ms /    67 runs   (    0.27 ms per token,  3682.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =     684.47 ms /    52 tokens (   13.16 ms per token,    75.97 tokens per second)\n",
      "llama_print_timings:        eval time =    5741.28 ms /    66 runs   (   86.99 ms per token,    11.50 tokens per second)\n",
      "llama_print_timings:       total time =    6857.23 ms /   118 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      32.40 ms /   113 runs   (    0.29 ms per token,  3487.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1652.51 ms /    38 tokens (   43.49 ms per token,    23.00 tokens per second)\n",
      "llama_print_timings:        eval time =    9298.91 ms /   112 runs   (   83.03 ms per token,    12.04 tokens per second)\n",
      "llama_print_timings:       total time =   11622.92 ms /   150 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      70.19 ms /   222 runs   (    0.32 ms per token,  3162.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =     681.91 ms /    45 tokens (   15.15 ms per token,    65.99 tokens per second)\n",
      "llama_print_timings:        eval time =   18031.88 ms /   221 runs   (   81.59 ms per token,    12.26 tokens per second)\n",
      "llama_print_timings:       total time =   20134.91 ms /   266 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      32.77 ms /   108 runs   (    0.30 ms per token,  3295.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =     811.26 ms /    31 tokens (   26.17 ms per token,    38.21 tokens per second)\n",
      "llama_print_timings:        eval time =    8776.10 ms /   107 runs   (   82.02 ms per token,    12.19 tokens per second)\n",
      "llama_print_timings:       total time =   10245.80 ms /   138 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      61.32 ms /   214 runs   (    0.29 ms per token,  3489.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =     997.88 ms /    90 tokens (   11.09 ms per token,    90.19 tokens per second)\n",
      "llama_print_timings:        eval time =   17935.53 ms /   213 runs   (   84.20 ms per token,    11.88 tokens per second)\n",
      "llama_print_timings:       total time =   20262.12 ms /   303 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      43.86 ms /   140 runs   (    0.31 ms per token,  3192.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2981.28 ms /    52 tokens (   57.33 ms per token,    17.44 tokens per second)\n",
      "llama_print_timings:        eval time =   11479.27 ms /   139 runs   (   82.58 ms per token,    12.11 tokens per second)\n",
      "llama_print_timings:       total time =   15361.08 ms /   191 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expecting ',' delimiter: line 1 column 55 (char 54)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      22.11 ms /    78 runs   (    0.28 ms per token,  3528.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =     867.19 ms /    73 tokens (   11.88 ms per token,    84.18 tokens per second)\n",
      "llama_print_timings:        eval time =    6445.68 ms /    77 runs   (   83.71 ms per token,    11.95 tokens per second)\n",
      "llama_print_timings:       total time =    7743.77 ms /   150 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      14.68 ms /    50 runs   (    0.29 ms per token,  3406.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4985.56 ms /    34 tokens (  146.63 ms per token,     6.82 tokens per second)\n",
      "llama_print_timings:        eval time =    4114.19 ms /    49 runs   (   83.96 ms per token,    11.91 tokens per second)\n",
      "llama_print_timings:       total time =    9421.80 ms /    83 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      67.01 ms /   235 runs   (    0.29 ms per token,  3507.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =     984.51 ms /    81 tokens (   12.15 ms per token,    82.27 tokens per second)\n",
      "llama_print_timings:        eval time =   19924.56 ms /   234 runs   (   85.15 ms per token,    11.74 tokens per second)\n",
      "llama_print_timings:       total time =   22812.97 ms /   315 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      35.00 ms /    93 runs   (    0.38 ms per token,  2657.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1001.24 ms /    82 tokens (   12.21 ms per token,    81.90 tokens per second)\n",
      "llama_print_timings:        eval time =    7408.10 ms /    92 runs   (   80.52 ms per token,    12.42 tokens per second)\n",
      "llama_print_timings:       total time =    9226.98 ms /   174 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     100.62 ms /   273 runs   (    0.37 ms per token,  2713.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1390.24 ms /    39 tokens (   35.65 ms per token,    28.05 tokens per second)\n",
      "llama_print_timings:        eval time =   22423.25 ms /   272 runs   (   82.44 ms per token,    12.13 tokens per second)\n",
      "llama_print_timings:       total time =   26168.92 ms /   311 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      60.02 ms /   197 runs   (    0.30 ms per token,  3282.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =     985.81 ms /    83 tokens (   11.88 ms per token,    84.19 tokens per second)\n",
      "llama_print_timings:        eval time =   17505.17 ms /   196 runs   (   89.31 ms per token,    11.20 tokens per second)\n",
      "llama_print_timings:       total time =   19804.42 ms /   279 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      15.66 ms /    54 runs   (    0.29 ms per token,  3448.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2073.26 ms /    39 tokens (   53.16 ms per token,    18.81 tokens per second)\n",
      "llama_print_timings:        eval time =    4293.08 ms /    53 runs   (   81.00 ms per token,    12.35 tokens per second)\n",
      "llama_print_timings:       total time =    6719.30 ms /    92 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      18.20 ms /    68 runs   (    0.27 ms per token,  3736.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =     751.15 ms /    52 tokens (   14.45 ms per token,    69.23 tokens per second)\n",
      "llama_print_timings:        eval time =    5595.24 ms /    67 runs   (   83.51 ms per token,    11.97 tokens per second)\n",
      "llama_print_timings:       total time =    6715.36 ms /   119 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      48.88 ms /   165 runs   (    0.30 ms per token,  3375.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =     930.98 ms /    35 tokens (   26.60 ms per token,    37.59 tokens per second)\n",
      "llama_print_timings:        eval time =   13713.23 ms /   164 runs   (   83.62 ms per token,    11.96 tokens per second)\n",
      "llama_print_timings:       total time =   15931.83 ms /   199 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      18.84 ms /    76 runs   (    0.25 ms per token,  4033.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =     398.77 ms /    14 tokens (   28.48 ms per token,    35.11 tokens per second)\n",
      "llama_print_timings:        eval time =    6236.70 ms /    75 runs   (   83.16 ms per token,    12.03 tokens per second)\n",
      "llama_print_timings:       total time =    7065.73 ms /    89 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      11.95 ms /    44 runs   (    0.27 ms per token,  3681.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =     447.70 ms /    28 tokens (   15.99 ms per token,    62.54 tokens per second)\n",
      "llama_print_timings:        eval time =    3495.36 ms /    43 runs   (   81.29 ms per token,    12.30 tokens per second)\n",
      "llama_print_timings:       total time =    4168.30 ms /    71 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      73.82 ms /   252 runs   (    0.29 ms per token,  3413.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =     710.49 ms /    57 tokens (   12.46 ms per token,    80.23 tokens per second)\n",
      "llama_print_timings:        eval time =   20918.80 ms /   251 runs   (   83.34 ms per token,    12.00 tokens per second)\n",
      "llama_print_timings:       total time =   23194.01 ms /   308 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      20.74 ms /    70 runs   (    0.30 ms per token,  3374.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =     712.91 ms /    54 tokens (   13.20 ms per token,    75.75 tokens per second)\n",
      "llama_print_timings:        eval time =    5675.04 ms /    69 runs   (   82.25 ms per token,    12.16 tokens per second)\n",
      "llama_print_timings:       total time =    6848.92 ms /   123 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     102.18 ms /   370 runs   (    0.28 ms per token,  3620.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5519.47 ms /    85 tokens (   64.93 ms per token,    15.40 tokens per second)\n",
      "llama_print_timings:        eval time =   31534.67 ms /   369 runs   (   85.46 ms per token,    11.70 tokens per second)\n",
      "llama_print_timings:       total time =   39588.95 ms /   454 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      15.76 ms /    54 runs   (    0.29 ms per token,  3425.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1623.07 ms /    38 tokens (   42.71 ms per token,    23.41 tokens per second)\n",
      "llama_print_timings:        eval time =    4355.44 ms /    53 runs   (   82.18 ms per token,    12.17 tokens per second)\n",
      "llama_print_timings:       total time =    6328.60 ms /    91 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      11.00 ms /    45 runs   (    0.24 ms per token,  4089.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1585.37 ms /    29 tokens (   54.67 ms per token,    18.29 tokens per second)\n",
      "llama_print_timings:        eval time =    3643.61 ms /    44 runs   (   82.81 ms per token,    12.08 tokens per second)\n",
      "llama_print_timings:       total time =    5460.62 ms /    73 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     113.40 ms /   393 runs   (    0.29 ms per token,  3465.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =     963.75 ms /    72 tokens (   13.39 ms per token,    74.71 tokens per second)\n",
      "llama_print_timings:        eval time =   33152.95 ms /   392 runs   (   84.57 ms per token,    11.82 tokens per second)\n",
      "llama_print_timings:       total time =   36635.41 ms /   464 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     118.92 ms /   391 runs   (    0.30 ms per token,  3287.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1467.26 ms /    45 tokens (   32.61 ms per token,    30.67 tokens per second)\n",
      "llama_print_timings:        eval time =   32939.51 ms /   390 runs   (   84.46 ms per token,    11.84 tokens per second)\n",
      "llama_print_timings:       total time =   36954.27 ms /   435 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      17.53 ms /    62 runs   (    0.28 ms per token,  3537.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =     699.35 ms /    46 tokens (   15.20 ms per token,    65.78 tokens per second)\n",
      "llama_print_timings:        eval time =    4951.91 ms /    61 runs   (   81.18 ms per token,    12.32 tokens per second)\n",
      "llama_print_timings:       total time =    5993.37 ms /   107 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      16.84 ms /    60 runs   (    0.28 ms per token,  3562.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1614.25 ms /    44 tokens (   36.69 ms per token,    27.26 tokens per second)\n",
      "llama_print_timings:        eval time =    4819.62 ms /    59 runs   (   81.69 ms per token,    12.24 tokens per second)\n",
      "llama_print_timings:       total time =    6771.44 ms /   103 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      23.85 ms /    80 runs   (    0.30 ms per token,  3354.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =     676.96 ms /    30 tokens (   22.57 ms per token,    44.32 tokens per second)\n",
      "llama_print_timings:        eval time =    6751.99 ms /    79 runs   (   85.47 ms per token,    11.70 tokens per second)\n",
      "llama_print_timings:       total time =    7898.78 ms /   109 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      59.00 ms /   197 runs   (    0.30 ms per token,  3338.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1487.70 ms /    84 tokens (   17.71 ms per token,    56.46 tokens per second)\n",
      "llama_print_timings:        eval time =   16271.23 ms /   196 runs   (   83.02 ms per token,    12.05 tokens per second)\n",
      "llama_print_timings:       total time =   18987.69 ms /   280 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      98.15 ms /   312 runs   (    0.31 ms per token,  3178.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =     734.21 ms /    58 tokens (   12.66 ms per token,    79.00 tokens per second)\n",
      "llama_print_timings:        eval time =   25946.69 ms /   311 runs   (   83.43 ms per token,    11.99 tokens per second)\n",
      "llama_print_timings:       total time =   28714.91 ms /   369 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      20.09 ms /    67 runs   (    0.30 ms per token,  3335.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =     698.54 ms /    50 tokens (   13.97 ms per token,    71.58 tokens per second)\n",
      "llama_print_timings:        eval time =    5470.41 ms /    66 runs   (   82.89 ms per token,    12.06 tokens per second)\n",
      "llama_print_timings:       total time =    6567.26 ms /   116 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      47.00 ms /   195 runs   (    0.24 ms per token,  4149.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5408.16 ms /    46 tokens (  117.57 ms per token,     8.51 tokens per second)\n",
      "llama_print_timings:        eval time =   17467.51 ms /   194 runs   (   90.04 ms per token,    11.11 tokens per second)\n",
      "llama_print_timings:       total time =   24191.40 ms /   240 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      55.96 ms /   189 runs   (    0.30 ms per token,  3377.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1628.04 ms /    34 tokens (   47.88 ms per token,    20.88 tokens per second)\n",
      "llama_print_timings:        eval time =   15812.83 ms /   188 runs   (   84.11 ms per token,    11.89 tokens per second)\n",
      "llama_print_timings:       total time =   18610.29 ms /   222 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      61.61 ms /   211 runs   (    0.29 ms per token,  3424.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =     709.65 ms /    53 tokens (   13.39 ms per token,    74.68 tokens per second)\n",
      "llama_print_timings:        eval time =   17668.47 ms /   210 runs   (   84.14 ms per token,    11.89 tokens per second)\n",
      "llama_print_timings:       total time =   19990.02 ms /   263 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      42.86 ms /   156 runs   (    0.27 ms per token,  3639.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4126.37 ms /    57 tokens (   72.39 ms per token,    13.81 tokens per second)\n",
      "llama_print_timings:        eval time =   13426.35 ms /   155 runs   (   86.62 ms per token,    11.54 tokens per second)\n",
      "llama_print_timings:       total time =   18620.49 ms /   212 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      47.16 ms /   142 runs   (    0.33 ms per token,  3010.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4237.26 ms /    24 tokens (  176.55 ms per token,     5.66 tokens per second)\n",
      "llama_print_timings:        eval time =   12341.20 ms /   141 runs   (   87.53 ms per token,    11.43 tokens per second)\n",
      "llama_print_timings:       total time =   17659.19 ms /   165 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      76.74 ms /   217 runs   (    0.35 ms per token,  2827.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1735.63 ms /    46 tokens (   37.73 ms per token,    26.50 tokens per second)\n",
      "llama_print_timings:        eval time =   18017.43 ms /   216 runs   (   83.41 ms per token,    11.99 tokens per second)\n",
      "llama_print_timings:       total time =   21661.01 ms /   262 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      17.78 ms /    44 runs   (    0.40 ms per token,  2474.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =     429.24 ms /    25 tokens (   17.17 ms per token,    58.24 tokens per second)\n",
      "llama_print_timings:        eval time =    3413.00 ms /    43 runs   (   79.37 ms per token,    12.60 tokens per second)\n",
      "llama_print_timings:       total time =    4227.83 ms /    68 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      83.26 ms /   230 runs   (    0.36 ms per token,  2762.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =     964.99 ms /    56 tokens (   17.23 ms per token,    58.03 tokens per second)\n",
      "llama_print_timings:        eval time =   18429.39 ms /   229 runs   (   80.48 ms per token,    12.43 tokens per second)\n",
      "llama_print_timings:       total time =   21431.60 ms /   285 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      22.50 ms /    60 runs   (    0.38 ms per token,  2666.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1982.30 ms /    40 tokens (   49.56 ms per token,    20.18 tokens per second)\n",
      "llama_print_timings:        eval time =    4669.53 ms /    59 runs   (   79.14 ms per token,    12.64 tokens per second)\n",
      "llama_print_timings:       total time =    7154.93 ms /    99 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      66.79 ms /   175 runs   (    0.38 ms per token,  2620.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1302.14 ms /    46 tokens (   28.31 ms per token,    35.33 tokens per second)\n",
      "llama_print_timings:        eval time =   13849.34 ms /   174 runs   (   79.59 ms per token,    12.56 tokens per second)\n",
      "llama_print_timings:       total time =   16676.53 ms /   220 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      76.67 ms /   207 runs   (    0.37 ms per token,  2699.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =     986.66 ms /    68 tokens (   14.51 ms per token,    68.92 tokens per second)\n",
      "llama_print_timings:        eval time =   16779.36 ms /   206 runs   (   81.45 ms per token,    12.28 tokens per second)\n",
      "llama_print_timings:       total time =   19494.49 ms /   274 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      72.04 ms /   185 runs   (    0.39 ms per token,  2567.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3359.83 ms /    42 tokens (   80.00 ms per token,    12.50 tokens per second)\n",
      "llama_print_timings:        eval time =   14588.10 ms /   184 runs   (   79.28 ms per token,    12.61 tokens per second)\n",
      "llama_print_timings:       total time =   19565.49 ms /   226 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      86.39 ms /   228 runs   (    0.38 ms per token,  2639.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1321.20 ms /    39 tokens (   33.88 ms per token,    29.52 tokens per second)\n",
      "llama_print_timings:        eval time =   18262.05 ms /   227 runs   (   80.45 ms per token,    12.43 tokens per second)\n",
      "llama_print_timings:       total time =   21629.70 ms /   266 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      20.00 ms /    52 runs   (    0.38 ms per token,  2600.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =     509.02 ms /    31 tokens (   16.42 ms per token,    60.90 tokens per second)\n",
      "llama_print_timings:        eval time =    4249.91 ms /    51 runs   (   83.33 ms per token,    12.00 tokens per second)\n",
      "llama_print_timings:       total time =    5407.51 ms /    82 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      87.33 ms /   260 runs   (    0.34 ms per token,  2977.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6067.36 ms /    40 tokens (  151.68 ms per token,     6.59 tokens per second)\n",
      "llama_print_timings:        eval time =   20849.24 ms /   259 runs   (   80.50 ms per token,    12.42 tokens per second)\n",
      "llama_print_timings:       total time =   29404.49 ms /   299 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      68.63 ms /   179 runs   (    0.38 ms per token,  2608.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3170.72 ms /    36 tokens (   88.08 ms per token,    11.35 tokens per second)\n",
      "llama_print_timings:        eval time =   14132.98 ms /   178 runs   (   79.40 ms per token,    12.59 tokens per second)\n",
      "llama_print_timings:       total time =   18873.70 ms /   214 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      22.09 ms /    55 runs   (    0.40 ms per token,  2489.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =     683.31 ms /    39 tokens (   17.52 ms per token,    57.07 tokens per second)\n",
      "llama_print_timings:        eval time =    4181.19 ms /    54 runs   (   77.43 ms per token,    12.91 tokens per second)\n",
      "llama_print_timings:       total time =    5343.20 ms /    93 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      23.15 ms /    57 runs   (    0.41 ms per token,  2461.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =     671.86 ms /    41 tokens (   16.39 ms per token,    61.02 tokens per second)\n",
      "llama_print_timings:        eval time =    4538.56 ms /    56 runs   (   81.05 ms per token,    12.34 tokens per second)\n",
      "llama_print_timings:       total time =    5752.20 ms /    97 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      13.34 ms /    37 runs   (    0.36 ms per token,  2772.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =     406.69 ms /    21 tokens (   19.37 ms per token,    51.64 tokens per second)\n",
      "llama_print_timings:        eval time =    2802.95 ms /    36 runs   (   77.86 ms per token,    12.84 tokens per second)\n",
      "llama_print_timings:       total time =    3516.70 ms /    57 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      23.52 ms /    52 runs   (    0.45 ms per token,  2210.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =     651.62 ms /    36 tokens (   18.10 ms per token,    55.25 tokens per second)\n",
      "llama_print_timings:        eval time =    3928.63 ms /    51 runs   (   77.03 ms per token,    12.98 tokens per second)\n",
      "llama_print_timings:       total time =    5021.76 ms /    87 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      89.70 ms /   225 runs   (    0.40 ms per token,  2508.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1245.02 ms /    64 tokens (   19.45 ms per token,    51.40 tokens per second)\n",
      "llama_print_timings:        eval time =   17534.95 ms /   224 runs   (   78.28 ms per token,    12.77 tokens per second)\n",
      "llama_print_timings:       total time =   20787.02 ms /   288 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      24.62 ms /    62 runs   (    0.40 ms per token,  2518.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =     658.25 ms /    43 tokens (   15.31 ms per token,    65.33 tokens per second)\n",
      "llama_print_timings:        eval time =    5019.25 ms /    61 runs   (   82.28 ms per token,    12.15 tokens per second)\n",
      "llama_print_timings:       total time =    6242.48 ms /   104 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      88.69 ms /   224 runs   (    0.40 ms per token,  2525.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1503.21 ms /    72 tokens (   20.88 ms per token,    47.90 tokens per second)\n",
      "llama_print_timings:        eval time =   17672.18 ms /   223 runs   (   79.25 ms per token,    12.62 tokens per second)\n",
      "llama_print_timings:       total time =   21307.50 ms /   295 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      89.77 ms /   191 runs   (    0.47 ms per token,  2127.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =     684.41 ms /    42 tokens (   16.30 ms per token,    61.37 tokens per second)\n",
      "llama_print_timings:        eval time =   15404.47 ms /   190 runs   (   81.08 ms per token,    12.33 tokens per second)\n",
      "llama_print_timings:       total time =   18489.53 ms /   232 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      21.59 ms /    58 runs   (    0.37 ms per token,  2686.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1365.53 ms /    41 tokens (   33.31 ms per token,    30.02 tokens per second)\n",
      "llama_print_timings:        eval time =    4460.61 ms /    57 runs   (   78.26 ms per token,    12.78 tokens per second)\n",
      "llama_print_timings:       total time =    6342.19 ms /    98 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      55.30 ms /   139 runs   (    0.40 ms per token,  2513.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =     693.19 ms /    56 tokens (   12.38 ms per token,    80.79 tokens per second)\n",
      "llama_print_timings:        eval time =   10735.08 ms /   138 runs   (   77.79 ms per token,    12.86 tokens per second)\n",
      "llama_print_timings:       total time =   12634.80 ms /   194 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      59.18 ms /   146 runs   (    0.41 ms per token,  2467.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =     480.74 ms /    29 tokens (   16.58 ms per token,    60.32 tokens per second)\n",
      "llama_print_timings:        eval time =   11527.11 ms /   145 runs   (   79.50 ms per token,    12.58 tokens per second)\n",
      "llama_print_timings:       total time =   13599.44 ms /   174 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expecting ',' delimiter: line 1 column 56 (char 55)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      19.47 ms /    50 runs   (    0.39 ms per token,  2568.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =     571.90 ms /    35 tokens (   16.34 ms per token,    61.20 tokens per second)\n",
      "llama_print_timings:        eval time =    3910.57 ms /    49 runs   (   79.81 ms per token,    12.53 tokens per second)\n",
      "llama_print_timings:       total time =    4865.83 ms /    84 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      68.60 ms /   154 runs   (    0.45 ms per token,  2244.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2949.71 ms /    38 tokens (   77.62 ms per token,    12.88 tokens per second)\n",
      "llama_print_timings:        eval time =   12090.18 ms /   153 runs   (   79.02 ms per token,    12.65 tokens per second)\n",
      "llama_print_timings:       total time =   16392.73 ms /   191 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      69.67 ms /   174 runs   (    0.40 ms per token,  2497.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =     672.34 ms /    40 tokens (   16.81 ms per token,    59.49 tokens per second)\n",
      "llama_print_timings:        eval time =   13543.68 ms /   173 runs   (   78.29 ms per token,    12.77 tokens per second)\n",
      "llama_print_timings:       total time =   15782.81 ms /   213 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra data: line 3 column 1 (char 231)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      55.70 ms /   153 runs   (    0.36 ms per token,  2747.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =     596.18 ms /    55 tokens (   10.84 ms per token,    92.25 tokens per second)\n",
      "llama_print_timings:        eval time =   12696.69 ms /   152 runs   (   83.53 ms per token,    11.97 tokens per second)\n",
      "llama_print_timings:       total time =   14563.25 ms /   207 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      88.80 ms /   251 runs   (    0.35 ms per token,  2826.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2677.18 ms /    57 tokens (   46.97 ms per token,    21.29 tokens per second)\n",
      "llama_print_timings:        eval time =   20522.61 ms /   250 runs   (   82.09 ms per token,    12.18 tokens per second)\n",
      "llama_print_timings:       total time =   25454.28 ms /   307 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      65.63 ms /   158 runs   (    0.42 ms per token,  2407.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =     689.57 ms /    35 tokens (   19.70 ms per token,    50.76 tokens per second)\n",
      "llama_print_timings:        eval time =   12849.49 ms /   157 runs   (   81.84 ms per token,    12.22 tokens per second)\n",
      "llama_print_timings:       total time =   15330.12 ms /   192 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      19.85 ms /    52 runs   (    0.38 ms per token,  2619.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =     677.84 ms /    36 tokens (   18.83 ms per token,    53.11 tokens per second)\n",
      "llama_print_timings:        eval time =    4017.55 ms /    51 runs   (   78.78 ms per token,    12.69 tokens per second)\n",
      "llama_print_timings:       total time =    5115.69 ms /    87 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     148.65 ms /   395 runs   (    0.38 ms per token,  2657.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =     978.27 ms /    77 tokens (   12.70 ms per token,    78.71 tokens per second)\n",
      "llama_print_timings:        eval time =   32275.19 ms /   394 runs   (   81.92 ms per token,    12.21 tokens per second)\n",
      "llama_print_timings:       total time =   36926.11 ms /   471 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      16.01 ms /    66 runs   (    0.24 ms per token,  4122.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5136.47 ms /    50 tokens (  102.73 ms per token,     9.73 tokens per second)\n",
      "llama_print_timings:        eval time =    5531.90 ms /    65 runs   (   85.11 ms per token,    11.75 tokens per second)\n",
      "llama_print_timings:       total time =   10989.36 ms /   115 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      41.53 ms /   139 runs   (    0.30 ms per token,  3346.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =     718.48 ms /    38 tokens (   18.91 ms per token,    52.89 tokens per second)\n",
      "llama_print_timings:        eval time =   11411.98 ms /   138 runs   (   82.70 ms per token,    12.09 tokens per second)\n",
      "llama_print_timings:       total time =   13029.19 ms /   176 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      13.71 ms /    43 runs   (    0.32 ms per token,  3137.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =     436.22 ms /    27 tokens (   16.16 ms per token,    61.90 tokens per second)\n",
      "llama_print_timings:        eval time =    3580.82 ms /    42 runs   (   85.26 ms per token,    11.73 tokens per second)\n",
      "llama_print_timings:       total time =    4405.18 ms /    69 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      39.12 ms /   138 runs   (    0.28 ms per token,  3527.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =     434.03 ms /    30 tokens (   14.47 ms per token,    69.12 tokens per second)\n",
      "llama_print_timings:        eval time =   11385.27 ms /   137 runs   (   83.10 ms per token,    12.03 tokens per second)\n",
      "llama_print_timings:       total time =   12609.85 ms /   167 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      68.59 ms /   240 runs   (    0.29 ms per token,  3498.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4264.98 ms /    87 tokens (   49.02 ms per token,    20.40 tokens per second)\n",
      "llama_print_timings:        eval time =   19919.00 ms /   239 runs   (   83.34 ms per token,    12.00 tokens per second)\n",
      "llama_print_timings:       total time =   25662.32 ms /   326 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      13.36 ms /    45 runs   (    0.30 ms per token,  3367.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4263.79 ms /    26 tokens (  163.99 ms per token,     6.10 tokens per second)\n",
      "llama_print_timings:        eval time =    3633.68 ms /    44 runs   (   82.58 ms per token,    12.11 tokens per second)\n",
      "llama_print_timings:       total time =    8199.01 ms /    70 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      38.64 ms /   143 runs   (    0.27 ms per token,  3700.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1784.23 ms /    52 tokens (   34.31 ms per token,    29.14 tokens per second)\n",
      "llama_print_timings:        eval time =   12001.24 ms /   142 runs   (   84.52 ms per token,    11.83 tokens per second)\n",
      "llama_print_timings:       total time =   14579.90 ms /   194 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      11.26 ms /    42 runs   (    0.27 ms per token,  3729.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =     602.51 ms /    26 tokens (   23.17 ms per token,    43.15 tokens per second)\n",
      "llama_print_timings:        eval time =    3464.64 ms /    41 runs   (   84.50 ms per token,    11.83 tokens per second)\n",
      "llama_print_timings:       total time =    4268.29 ms /    67 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      56.79 ms /   195 runs   (    0.29 ms per token,  3433.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1403.28 ms /    49 tokens (   28.64 ms per token,    34.92 tokens per second)\n",
      "llama_print_timings:        eval time =   16217.88 ms /   194 runs   (   83.60 ms per token,    11.96 tokens per second)\n",
      "llama_print_timings:       total time =   18755.20 ms /   243 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      13.75 ms /    49 runs   (    0.28 ms per token,  3563.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =     697.08 ms /    33 tokens (   21.12 ms per token,    47.34 tokens per second)\n",
      "llama_print_timings:        eval time =    3919.66 ms /    48 runs   (   81.66 ms per token,    12.25 tokens per second)\n",
      "llama_print_timings:       total time =    4872.40 ms /    81 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      36.13 ms /   109 runs   (    0.33 ms per token,  3017.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =     663.51 ms /    41 tokens (   16.18 ms per token,    61.79 tokens per second)\n",
      "llama_print_timings:        eval time =    9065.63 ms /   108 runs   (   83.94 ms per token,    11.91 tokens per second)\n",
      "llama_print_timings:       total time =   10358.94 ms /   149 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      60.31 ms /   210 runs   (    0.29 ms per token,  3481.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1473.43 ms /    59 tokens (   24.97 ms per token,    40.04 tokens per second)\n",
      "llama_print_timings:        eval time =   17508.90 ms /   209 runs   (   83.77 ms per token,    11.94 tokens per second)\n",
      "llama_print_timings:       total time =   20254.62 ms /   268 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      47.98 ms /   170 runs   (    0.28 ms per token,  3543.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =     717.71 ms /    50 tokens (   14.35 ms per token,    69.67 tokens per second)\n",
      "llama_print_timings:        eval time =   14194.88 ms /   169 runs   (   83.99 ms per token,    11.91 tokens per second)\n",
      "llama_print_timings:       total time =   15884.38 ms /   219 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      48.35 ms /   158 runs   (    0.31 ms per token,  3267.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =     706.09 ms /    42 tokens (   16.81 ms per token,    59.48 tokens per second)\n",
      "llama_print_timings:        eval time =   12961.35 ms /   157 runs   (   82.56 ms per token,    12.11 tokens per second)\n",
      "llama_print_timings:       total time =   14899.90 ms /   199 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      19.45 ms /    65 runs   (    0.30 ms per token,  3341.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =     734.44 ms /    27 tokens (   27.20 ms per token,    36.76 tokens per second)\n",
      "llama_print_timings:        eval time =    5210.05 ms /    64 runs   (   81.41 ms per token,    12.28 tokens per second)\n",
      "llama_print_timings:       total time =    6295.71 ms /    91 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      15.85 ms /    55 runs   (    0.29 ms per token,  3470.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1970.60 ms /    39 tokens (   50.53 ms per token,    19.79 tokens per second)\n",
      "llama_print_timings:        eval time =    4394.85 ms /    54 runs   (   81.39 ms per token,    12.29 tokens per second)\n",
      "llama_print_timings:       total time =    6664.19 ms /    93 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      16.16 ms /    56 runs   (    0.29 ms per token,  3464.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =     705.03 ms /    40 tokens (   17.63 ms per token,    56.74 tokens per second)\n",
      "llama_print_timings:        eval time =    4589.93 ms /    55 runs   (   83.45 ms per token,    11.98 tokens per second)\n",
      "llama_print_timings:       total time =    6025.39 ms /    95 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      16.08 ms /    56 runs   (    0.29 ms per token,  3483.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1267.43 ms /    40 tokens (   31.69 ms per token,    31.56 tokens per second)\n",
      "llama_print_timings:        eval time =    4533.49 ms /    55 runs   (   82.43 ms per token,    12.13 tokens per second)\n",
      "llama_print_timings:       total time =    6124.75 ms /    95 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      63.64 ms /   206 runs   (    0.31 ms per token,  3237.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =     443.30 ms /    27 tokens (   16.42 ms per token,    60.91 tokens per second)\n",
      "llama_print_timings:        eval time =   16830.20 ms /   205 runs   (   82.10 ms per token,    12.18 tokens per second)\n",
      "llama_print_timings:       total time =   18534.77 ms /   232 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      16.93 ms /    55 runs   (    0.31 ms per token,  3249.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1379.16 ms /    39 tokens (   35.36 ms per token,    28.28 tokens per second)\n",
      "llama_print_timings:        eval time =    4445.91 ms /    54 runs   (   82.33 ms per token,    12.15 tokens per second)\n",
      "llama_print_timings:       total time =    6151.53 ms /    93 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      52.84 ms /   175 runs   (    0.30 ms per token,  3312.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2836.05 ms /    66 tokens (   42.97 ms per token,    23.27 tokens per second)\n",
      "llama_print_timings:        eval time =   14592.76 ms /   174 runs   (   83.87 ms per token,    11.92 tokens per second)\n",
      "llama_print_timings:       total time =   18527.20 ms /   240 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      20.82 ms /    59 runs   (    0.35 ms per token,  2833.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1091.31 ms /    43 tokens (   25.38 ms per token,    39.40 tokens per second)\n",
      "llama_print_timings:        eval time =    4968.38 ms /    58 runs   (   85.66 ms per token,    11.67 tokens per second)\n",
      "llama_print_timings:       total time =    6933.10 ms /   101 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      40.38 ms /   133 runs   (    0.30 ms per token,  3293.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =     688.51 ms /    40 tokens (   17.21 ms per token,    58.10 tokens per second)\n",
      "llama_print_timings:        eval time =   10822.11 ms /   132 runs   (   81.99 ms per token,    12.20 tokens per second)\n",
      "llama_print_timings:       total time =   12329.72 ms /   172 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      40.65 ms /   143 runs   (    0.28 ms per token,  3517.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =     753.58 ms /    33 tokens (   22.84 ms per token,    43.79 tokens per second)\n",
      "llama_print_timings:        eval time =   11689.65 ms /   142 runs   (   82.32 ms per token,    12.15 tokens per second)\n",
      "llama_print_timings:       total time =   13248.02 ms /   175 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra data: line 3 column 1 (char 150)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      15.40 ms /    54 runs   (    0.29 ms per token,  3506.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =     579.84 ms /    38 tokens (   15.26 ms per token,    65.54 tokens per second)\n",
      "llama_print_timings:        eval time =    4352.87 ms /    53 runs   (   82.13 ms per token,    12.18 tokens per second)\n",
      "llama_print_timings:       total time =    5232.87 ms /    91 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      13.26 ms /    49 runs   (    0.27 ms per token,  3694.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =     948.24 ms /    33 tokens (   28.73 ms per token,    34.80 tokens per second)\n",
      "llama_print_timings:        eval time =    4235.51 ms /    48 runs   (   88.24 ms per token,    11.33 tokens per second)\n",
      "llama_print_timings:       total time =    5707.70 ms /    81 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      15.19 ms /    54 runs   (    0.28 ms per token,  3555.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =     680.86 ms /    35 tokens (   19.45 ms per token,    51.41 tokens per second)\n",
      "llama_print_timings:        eval time =    4312.96 ms /    53 runs   (   81.38 ms per token,    12.29 tokens per second)\n",
      "llama_print_timings:       total time =    5283.10 ms /    88 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      13.50 ms /    49 runs   (    0.28 ms per token,  3629.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =     471.02 ms /    29 tokens (   16.24 ms per token,    61.57 tokens per second)\n",
      "llama_print_timings:        eval time =    3920.48 ms /    48 runs   (   81.68 ms per token,    12.24 tokens per second)\n",
      "llama_print_timings:       total time =    4653.20 ms /    77 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      51.38 ms /   171 runs   (    0.30 ms per token,  3328.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1189.04 ms /    66 tokens (   18.02 ms per token,    55.51 tokens per second)\n",
      "llama_print_timings:        eval time =   14131.55 ms /   170 runs   (   83.13 ms per token,    12.03 tokens per second)\n",
      "llama_print_timings:       total time =   16514.87 ms /   236 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      52.81 ms /   172 runs   (    0.31 ms per token,  3256.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =     420.20 ms /    29 tokens (   14.49 ms per token,    69.01 tokens per second)\n",
      "llama_print_timings:        eval time =   13988.63 ms /   171 runs   (   81.80 ms per token,    12.22 tokens per second)\n",
      "llama_print_timings:       total time =   15454.84 ms /   200 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      39.96 ms /   137 runs   (    0.29 ms per token,  3428.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1773.78 ms /    45 tokens (   39.42 ms per token,    25.37 tokens per second)\n",
      "llama_print_timings:        eval time =   11453.82 ms /   136 runs   (   84.22 ms per token,    11.87 tokens per second)\n",
      "llama_print_timings:       total time =   14041.00 ms /   181 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      49.10 ms /   152 runs   (    0.32 ms per token,  3095.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =     701.45 ms /    44 tokens (   15.94 ms per token,    62.73 tokens per second)\n",
      "llama_print_timings:        eval time =   12255.42 ms /   151 runs   (   81.16 ms per token,    12.32 tokens per second)\n",
      "llama_print_timings:       total time =   13993.42 ms /   195 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      10.78 ms /    36 runs   (    0.30 ms per token,  3340.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =     313.32 ms /    20 tokens (   15.67 ms per token,    63.83 tokens per second)\n",
      "llama_print_timings:        eval time =    2846.04 ms /    35 runs   (   81.32 ms per token,    12.30 tokens per second)\n",
      "llama_print_timings:       total time =    3377.68 ms /    55 tokens\n"
     ]
    }
   ],
   "source": [
    "eval_file_path = '/Users/ananyahooda/Desktop/final/data/evaluation_data/conll04_eval.json' # Replace with the actual path to your eval.json file\n",
    "pred_file_path = '/Users/ananyahooda/Desktop/final/tripleExtractionPythonScript/Final_ensemble_prediction_triples.json' # The output file path\n",
    "generate_pred_json(eval_file_path, pred_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate precision, recall, and F1 score\n",
    "def calculate_scores(tp, total_golden, total_prediction):\n",
    "    precision = tp / total_prediction if total_prediction > 0 else 0\n",
    "    recall = tp / total_golden if total_golden > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    return precision, recall, f1\n",
    "\n",
    "# Function to process the files and calculate the scores, considering extras\n",
    "def evaluate_predictions_corrected(golden_file, prediction_file):\n",
    "    # Load the golden truths and predictions\n",
    "    with open(golden_file, 'r') as f:\n",
    "        golden_data = json.load(f)\n",
    "    with open(prediction_file, 'r') as f:\n",
    "        prediction_data = json.load(f)\n",
    "\n",
    "    tp = 0\n",
    "    extras = 0\n",
    "\n",
    "    # Convert golden data and prediction data into dictionaries for easier access\n",
    "    golden_dict = {item['id']: set(tuple(triple.items()) for triple in item['triples']) for item in golden_data}\n",
    "    prediction_dict = {item['id']: set(tuple(triple.items()) for triple in item['triples']) for item in prediction_data}\n",
    "\n",
    "    # Iterate over each instance in the golden data to calculate true positives\n",
    "    for id, golden_triples in golden_dict.items():\n",
    "        prediction_triples = prediction_dict.get(id, set())\n",
    "        tp += len(golden_triples & prediction_triples)\n",
    "\n",
    "    # Calculate extras in prediction\n",
    "    for id, prediction_triples in prediction_dict.items():\n",
    "        if id not in golden_dict:\n",
    "            extras += len(prediction_triples)\n",
    "        else:\n",
    "            unmatched_triples = prediction_triples - golden_dict[id]\n",
    "            print(unmatched_triples)\n",
    "            extras += len(unmatched_triples)\n",
    "\n",
    "    # Calculate micro scores\n",
    "    total_golden = sum(len(triples) for triples in golden_dict.values())\n",
    "    total_prediction = sum(len(triples) for triples in prediction_dict.values())\n",
    "    precision_micro, recall_micro, f1_micro = calculate_scores(tp, total_golden, total_prediction)\n",
    "\n",
    "    # Calculate macro scores\n",
    "    total_items = len(golden_dict)\n",
    "    precision_macro, recall_macro, f1_macro = 0, 0, 0\n",
    "    for id, golden_triples in golden_dict.items():\n",
    "        prediction_triples = prediction_dict.get(id, set())\n",
    "        tp = len(golden_triples & prediction_triples)\n",
    "        precision, recall, _ = calculate_scores(tp, len(golden_triples), len(prediction_triples))\n",
    "        precision_macro += precision\n",
    "        recall_macro += recall\n",
    "    precision_macro /= total_items\n",
    "    recall_macro /= total_items\n",
    "    f1_macro = 2 * (precision_macro * recall_macro) / (precision_macro + recall_macro) if (precision_macro + recall_macro) > 0 else 0\n",
    "\n",
    "    return {\n",
    "        'micro': {\n",
    "            'precision': precision_micro,\n",
    "            'recall': recall_micro,\n",
    "            'f1': f1_micro\n",
    "        },\n",
    "        'macro': {\n",
    "            'precision': precision_macro,\n",
    "            'recall': recall_macro,\n",
    "            'f1': f1_macro\n",
    "        },\n",
    "        'true_positives': tp,\n",
    "        'extras': extras\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String Removal (The hallucinated responses not in correct format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "golden_file = \"/Users/ananyahooda/Desktop/final/golden_truth.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the golden truth JSON file: 288\n",
      "Length of the prediction JSON file: 262\n",
      "Created new JSON files with common data points: 'common_file1.json' and 'common_file2.json'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the JSON data from the two files\n",
    "with open(pred_file_path , 'r') as file:\n",
    "    data1 = json.load(file)\n",
    "\n",
    "with open(golden_file, 'r') as file:\n",
    "    data2 = json.load(file)\n",
    "\n",
    "# Determine the length of both JSON files\n",
    "length_data1 = len(data1)\n",
    "length_data2 = len(data2)\n",
    "\n",
    "# Print the lengths\n",
    "print(f\"Length of the golden truth JSON file: {length_data1}\")\n",
    "print(f\"Length of the prediction JSON file: {length_data2}\")\n",
    "\n",
    "# Convert the lists to dictionaries indexed by the 'id' attribute\n",
    "data1_dict = {item['id']: item for item in data1}\n",
    "data2_dict = {item['id']: item for item in data2}\n",
    "\n",
    "# Find the common IDs\n",
    "common_ids = set(data1_dict.keys()) & set(data2_dict.keys())\n",
    "\n",
    "# Extract the common data points\n",
    "common_data1 = [data1_dict[id] for id in common_ids]\n",
    "common_data2 = [data2_dict[id] for id in common_ids]\n",
    "\n",
    "# Save the common data points to new JSON files\n",
    "with open('pred_ensemble_common.json', 'w') as file:\n",
    "    json.dump(common_data1, file, indent=4)\n",
    "\n",
    "with open('golden_truth_common.json', 'w') as file: \n",
    "    json.dump(common_data2, file, indent=4)\n",
    "\n",
    "# Print a message to indicate that the new files have been created\n",
    "print(f\"Created new JSON files with common data points: 'common_file1.json' and 'common_file2.json'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries with 'triples' as a string: 19\n",
      "String 1:  {\"action\": \"extract_text_triplets\", \"action_input\": \"High winds blew on the east slopes of the Rocky Mountains in Montana , with winds gusting to near 50 mph at Livingston .\"} \n",
      "\n",
      "Here's the expected response format for the extracted triplets or entities:\n",
      "{\"triplets\": [[\"High winds\", \"blew\", \"east slopes\"], [\"winds\", \"gusted\", \"near 50 mph\"], [\"winds\", \"reached\", \"Livingston\"]], or [\"entities\": [\"High winds\", \"east slopes\", \"Rocky Mountains\", \"Montana\", \"Livingston\"]]} \n",
      "\n",
      "Note that the exact format of the output may vary depending on the specific requirements of the task or the NLP tool being used.\n",
      "String 2:  {\"action\": \"extract_text_triplets\", \"action_input\": \"Also on the GOP ballot are insurance broker David Fleischer and former North Miami Mayor John Stembridge.} \"\n",
      "\n",
      "Here are the possible extracted triplets from the given input: [(\"Also\", \"on\", \"the GOP ballot\"), (\"are\", \"insurance broker\", \"David Fleischer\"), (\"and\", \"\", \"), (\"former\", \"Mayor\", \"John Stembridge\")]\n",
      "\n",
      "Or you can have: [(\"On the GOP ballot\", \"\", [\"David Fleischer\", \"insurance broker\"]), (\"And\", \"\", [\"John Stembridge\", \"former Mayor\"])]\n",
      "\n",
      "Depending on the requirement, the triplet format can be modified.\n",
      "String 3:  {\"action\": \"extract_text_triplets\", \"action_input\": \"But, as in many past cases dealing with the U.S. hostages, such considerations are not viewed from a strictly limited military point of view, the Pentagon officials noted.\"}\n",
      "\n",
      "This text does not provide clear-cut triplet information like the previous examples, as it's discussing viewpoints and considerations rather than definite facts. However, Assistant can still attempt to extract potential triplets from it based on context, though their accuracy may depend on the intended use case. Here's one possible interpretation:\n",
      "\n",
      "{\"triplets\": [[\"But\", \"is not viewed\", \"from a strictly limited military point of view\"], [\"many past cases\", \"dealing with the U.S. hostages\", \"such considerations\"], [\"Pentagon officials\", \"noted\"]]} \n",
      "\n",
      "Keep in mind that this interpretation might not be perfect, as the text's context does not definitively establish subject-predicate-object triplet relationships, but rather discusses the perspective from which certain considerations are made.\n",
      "String 4:  {\"action\": \"extract_text_triplets\", \"action_input\": \"Leica spokesman Hans Guenther von Zydowitz told the AP company officials estimate last year 's sales at about $60.6 million , but he declined to estimate profits .\"}\n",
      "\n",
      "Here's the possible output for the given input:\n",
      "{\"triplets\": [[\"Leica\", \"spokesman\", \"Hans Guenther von Zydowitz\"], [\"Hans Guenther von Zydowitz\", \"told\", \"AP\"], [\"AP\", \"company\", \"\"], [\"company\", \"officials\", \"\"], [\"last year\", \"sales\", \"$60.6 million\"], [\"sales\", \"about\", \"$60.6 million\"], [\"\", \"estimate\", \"about $60.6 million\"], [\"\", \"declined\", \"estimate\"], [\"profits\", \"\", \"\"]]}\n",
      "String 5:  {\"action\": \"extract_text_triplets\", \"action_input\": \"Temperatures around the nation at 2 a.m. EST ranged from 9 degrees below zero at Yellowstone, Wyo., to 85 degrees at McAllen, Texas.\"}\n",
      "\n",
      "This command will extract triplets from the given text, which may look something like this: [(\"Temperatures around the nation at 2 a.m. EST\", \"ranged\", [\"from\", {\"value\": \"9 degrees below zero\", \"entity\": \"Temperature\", \"location\": \"Yellowstone, Wyo.\"}, {\"value\": \"85 degrees\", \"entity\": \"Temperature\", \"location\": \"McAllen, Texas.\"}]]).\n",
      "\n",
      "These triplets can be further used to perform various tasks, such as data analysis, generating queries, or other automated processes.\n",
      "String 6:  Based on the given context, it seems that \"Beijing JINGJI YANJIU\" is likely a name or organization, but without more context or information it's not clear what the relationship or triplet would be. Here's how you could request the extraction of triplets or entities from the given input:\n",
      "\n",
      "To extract triplets:\n",
      "{\"action\": \"extract_text_triplets\", \"action_input\": \"Beijing JINGJI YANJIU\"}\n",
      "\n",
      "To extract entities:\n",
      "{\"action\": \"extract_entities\", \"action_input\": \"Beijing JINGJI YANJIU\"}\n",
      "\n",
      "These commands can be used if you want the Assistant to process the input text and return the result in the expected format.\n",
      "String 7:  {\"action\": \"extract_text_triplets\", \"action_input\": \"Mikoyan ` ` preempted Khrushchev 's order to run the blockade and ordered Soviet ships to stop just short of the quarantine line , ' ' they say .\"}\n",
      "\n",
      "Here's an example of how the output might look like:\n",
      "\n",
      "{\"triplets\": [[\"Mikoyan\", \"preempted\", \"Khrushchev's order\"], [\"Mikoyan\", \"ordered\", \"Soviet ships\"], [\"Soviet ships\", \"stopped\", \"just short of the quarantine line\"]]} \n",
      "\n",
      "Please note that this is just an example, the actual output might vary depending on the specific context and NLP techniques used.\n",
      "String 8:  {\"action\": \"extract_text_triplets\", \"action_input\": \"Guatemala City Teleonce Television\"}\n",
      "\n",
      "For example, the output could be: {\"subject\": \"Guatemala City Teleonce Television\", \"relation\": \"is\", \"object\": \"a television station\"}\n",
      "\n",
      "Or, if more specific triples are needed, the output could include multiple triples depending on the context, such as: {\"subject\": \"Guatemala City\", \"relation\": \"is located in\", \"object\": \"Guatemala\"}, {\"subject\": \"Teleonce\", \"relation\": \"is a\", \"object\": \"television channel\"}, {\"subject\": \"Teleonce\", \"relation\": \"broadcasts from\", \"object\": \"Guatemala City Teleonce Television\"}\n",
      "String 9:  {\"action\": \"extract_text_triplets\", \"action_input\": \"In 1767 , Andrew Jackson , seventh president of the United States , was born in Waxhaw , S.C.\"}\n",
      "\n",
      "Here's how the output might look like: {\"triplets\": [\"(Andrew Jackson, seventh president of the United States, was born in 1767)\", \"(Andrew Jackson, was born, in 1767)\", \"(Andrew Jackson, seventh president of the United States, was born in Waxhaw)\", \"(Andrew Jackson, was born, in Waxhaw, S.C.)\"]}\n",
      "String 10:  {\"action\": \"extract_text_triplets\", \"action_input\": \"The building from which Lee Harvey Oswald allegedly shot President Kennedy now contains county offices and, on the sixth floor, a museum with exhibits about the assassination\"}\n",
      "\n",
      "Here's an example of the output you might expect:\n",
      "\n",
      "{\"triplets\": [[\"building\", \"allegedly_shot\", \"President Kennedy\"], [\"building\", \"contains\", \"county offices\"], [\"building\", \"contains\", \"museum\"], [\"museum\", \"has\", \"exhibits\"], [\"museum\", \"about\", \"assassination\"], [\"sixth floor\", \"has\", \"museum\"]]}\n",
      "\n",
      "Remember that the actual output might vary based on the specific text and the NLP model used for extraction.\n",
      "String 11:  {\"action\": \"extract_text_triplels\", \"action_input\": \"The House voted Thompson to withdraw a grant to develop bandleader Lawrence Welk 's North Dakota hometown , a rural development project that became a symbol for wasteful government spending last fall .\"}\n",
      "\n",
      "Possible extract triples:\n",
      "- (The House, voted, withdrew)\n",
      "- (A grant, was, to develop)\n",
      "- (Lawrence Welk, is, bandleader)\n",
      "- (Lawrence Welk, has, North Dakota hometown)\n",
      "- (A rural development project, became, a symbol)\n",
      "- (Last fall, was, a symbol for wasteful government spending)\n",
      "String 12:  {\"action\": \"extract_text_triplets\", \"action_input\": \"Jack MacAllister, chairman of U S West inc., said the Hong Kong franchise represents a major international opportunity for the company.\"}\n",
      "\n",
      "Here's the expected output: {\"triplets\": [\"[Jack MacAllister, chairman]\", \"[Jack MacAllister, U S West inc.\", \"is\"], [\"U S West inc., chairman\"], [\"U S West inc., Jack MacAllister\"], [\"U S West inc., said\"], [\"said, U S West inc.\"], [\"said, the Hong Kong franchise\"], [\"the Hong Kong franchise, represents\"], [\"represents, a major international opportunity\"], [\"a major international opportunity, the Hong Kong franchise\"], [\"the Hong Kong franchise, represents\"], [\"represents, represents\"], [\"represents, a major international opportunity for the company\"], [\"a major international opportunity, the company\"], [\"the company, represents\"], [\"represents, US West inc.\"], [\"US West inc., the company\"]]}\n",
      "String 13:  {\"action\": \"extract_text_triplets\", \"action_input\": \"This greatly eases the government's policy that was decided upon at the August reunification-related ministers meeting in which it decided that businessmen will be allowed to visit North Korea only when North Korea completely returns to the Nuclear Nonproliferation Treaty and agrees to the inspection of unreported facilities in Yongbyon.}\n",
      "\n",
      "Possible extracts: [[\"government\", \"decided\", \"policy\"], [\"August reunification-related ministers meeting\", \"decided\", \"allow businessmen to visit North Korea\"], [\"North Korea\", \"completely returns\", \"Nuclear Nonproliferation Treaty\"], [\"North Korea\", \"agrees\", \"inspection of unreported facilities in Yongbyon\"]]\n",
      "String 14:  {\"action\": \"extract_text_triplets\", \"action_input\": \"Helsinki Suomen Yleisradio Network\"}\n",
      "\n",
      "Here's an example of how the extracted triplets might look:\n",
      "\n",
      "{\"triplets\": [\"Helsinki is the location of Suomen Yleisradio Network.\", \"Suomen Yleisradio Network is a network in Helsinki.\", \"Helsinki has the network Suomen Yleisradio Network.\"]}\n",
      "\n",
      "Please note that the actual triplets extracted may vary depending on the context and the specific information provided in the text.\n",
      "String 15:  {\"action\": \"extract_text_triplets\", \"action_input\": \"Analyst Scott Hinton of Elders Futures Inc. said the copper market seemed unsinkable , even though prices have fallen from the record highs reached in early December .\"}\n",
      "\n",
      "You can trust me to help you extract meaningful information from text using the provided actions. Let me know if you need any assistance with entities or triples! Just remember that the format for requests should be {\"action\": \"action\\_name\", \"action\\_input\": \"your\\_text\\_here\"}! Keep exploring and I'll be here whenever you need help.\n",
      "String 16:  {\"action\": \"extract_text_triplets\", \"action_input\": \"Havana Radio Rebelde Network\"}\n",
      "\n",
      "Assistant: I'm here to help you extract triplets or entities from given texts using the provided actions. For the given input \"Havana Radio Rebelde Network\", my response will be:\n",
      "\n",
      "{\"action\": \"extract_text_triplets\", \"action_input\": \"Havana Radio Rebelde Network\"}\n",
      "String 17:  {\"action\": \"extract_text_triplets\", \"action_input\": \"About 2, 900 firefighters were battling blazes in the Wallowa-Whitman and the Malheur national forests near the towns of Baker, Unity, Enterprise and John Day, said Forest Service spokesman Mike Ferris.\"}\n",
      "\n",
      "Here's the expected output based on the provided conversation format:\n",
      "\n",
      "{\"triplets\": [[\"About 2, 900\", \"fighters\", \"were battling\"], [\"fighters\", \"\", \"blazes\"], [\"were battling\", \"blazes\", \"in the Wallowa-Whitman and the Malheur national forests\"], [\"in the Wallowa-Whitman and the Malheur national forests\", \"\", \"near\"], [\"near\", \"\", \"the towns\"], [\"the towns\", \"\", \"of Baker, Unity, Enterprise and John Day\"], [\"said\", \"\", \"Forest Service spokesman Mike Ferris\"]]}\n",
      "\n",
      "You may want to adjust the output format based on your specific use case or data processing requirements.\n",
      "String 18:  {\"action\": \"extract_text_triplets\", \"action_input\": \"What's my goal?\" asks Pedatzur, who now works for the Massachusetts Institute of Technology.}\n",
      "\n",
      "This will extract triplets from the given text, assuming the text is meant to be interpreted as follows:\n",
      "\n",
      "1. What's my goal? (question)\n",
      "2. Asks (relation)\n",
      "3. Pedatzur (subject)\n",
      "4. Who now works for (relation)\n",
      "5. Massachusetts Institute of Technology (object)\n",
      "String 19:  {\"action\": \"extract_text_triplets\", \"action_input\": \"Wyatt McCauley , spokesman for CH2M Hill , did not immediately return a phone call to his office in Denver.} \"\n",
      "\n",
      "Here's the extracted triplets from the provided text: [(\"Wyatt McCauley\", \"spokesman\", \"CH2M Hill\"), (\"CH2M Hill\", \"employer_of\", \"Wyatt McCauley\"), (\"Wyatt McCauley\", \"did not return\", \"phone\\_call\"), (\"phone\\_call\", \"to\", \"his\\_office\"), (\"his\\_office\", \"in\", \"Denver\")]\n",
      "\n",
      "And here's the extracted entities: [\"Wyatt McCauley\", \"CH2M Hill\", \"phone\\_call\", \"his\\_office\", \"Denver\"]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the JSON data from the pred.json file\n",
    "with open('pred_ensemble_common.json', 'r') as file:\n",
    "    pred_data = json.load(file)\n",
    "\n",
    "# Initialize a counter for entries with \"triples\" as a string\n",
    "string_triples_count = 0\n",
    "string_triples = []\n",
    "\n",
    "# Iterate over the entries and check the type of \"triples\"\n",
    "for entry in pred_data:\n",
    "    if 'triples' in entry and isinstance(entry['triples'], str):\n",
    "        string_triples_count += 1\n",
    "        string_triples.append(entry['triples'])\n",
    "\n",
    "# Print the count of such entries\n",
    "\n",
    "print(f\"Number of entries with 'triples' as a string: {string_triples_count}\")\n",
    "\n",
    "for i, string in enumerate(string_triples, start=1):\n",
    "    print(f\"String {i}: {string}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entries with 'triples' as a string have been removed. New files created: 'filtered_file1.json' and 'filtered_file2.json'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the JSON data from the two files\n",
    "with open('golden_truth_common.json', 'r') as file:\n",
    "    data1 = json.load(file)\n",
    "\n",
    "with open('pred_ensemble_common.json', 'r') as file:\n",
    "    data2 = json.load(file)\n",
    "\n",
    "# Find the IDs of entries with \"triples\" as a string in file1\n",
    "ids_to_remove = [entry['id'] for entry in data2 if 'triples' in entry and isinstance(entry['triples'], str)]\n",
    "\n",
    "# Remove the entries from both files\n",
    "filtered_data1 = [entry for entry in data1 if entry['id'] not in ids_to_remove]\n",
    "filtered_data2 = [entry for entry in data2 if entry['id'] not in ids_to_remove]\n",
    "\n",
    "# Save the filtered data back to new JSON files\n",
    "with open('/Users/ananyahooda/Desktop/final/final_comparable_golden_truth.json', 'w') as file:\n",
    "    json.dump(filtered_data1 , file, indent=4)\n",
    "\n",
    "with open('/Users/ananyahooda/Desktop/final/comparabe_ensemble_predi.json', 'w') as file:\n",
    "    json.dump(filtered_data2, file, indent=4)\n",
    "\n",
    "# Print a message to indicate that the entries have been removed\n",
    "print(f\"Entries with 'triples' as a string have been removed. New files created: 'filtered_file1.json' and 'filtered_file2.json'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For mapping 90 relation to 5 relations as in Conll04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping =  {'employer': ['derivative work',   'inception', 'instance of', 'owned by', 'owner of', 'part of', 'participant', \n",
    "'participant in', 'performer', 'twinned administrative body','occupation', 'field of this occupation', 'member of political party','work location', 'language used', 'participant in', 'participant','owner of', 'owned by', 'member of', 'notable work', 'instance of', 'interested in',  'office held by head of government', 'chief executive officer', 'educated at', 'subclass of','part of', 'office held by head of state','chairperson', 'executive body', 'industry', 'officeholder', \n",
    "'position held', 'practiced by', 'language of work or name', 'director / manager', 'employer', 'field of work', \n",
    "'language of work or name', 'notable work', 'occupation', 'member of', 'member of political party', 'officeholder',\n",
    " 'operator', 'position held', 'educated at', 'founded by',\n",
    "'product or material produced', 'subsidiary', 'work location', 'author', \n",
    "'office held by head of government', 'used by', 'uses', 'candidacy in election', 'candidate',  'chairperson', 'head of government' ], \n",
    "'headquarters location': ['headquarters location', 'twinned administrative body','applies to jurisdiction', 'legislative body','military branch', 'contains administrative territorial entity', \n",
    "'parent organization', 'operating area','legislative body', 'contains administrative territorial entity', \n",
    "'headquarters location', 'located in the administrative territorial entity', 'ethnic group', 'language used',\n",
    "'military branch', 'parent organization', 'applies to jurisdiction'],\n",
    " 'killed by': ['cause of death', 'perpetrator', 'convicted of', \n",
    "'killed by', 'place of death',  'facet of','date of death',  'main subject', 'place of death', 'facet of', 'significant event'], 'location': ['location',  'capital','continent',  'located in time zone', 'shares border with', \n",
    "'mountain range', 'located in or next to body of water', 'candidate',   'significant place',   'spouse', 'place of publication',  'target','country', 'located in or next to body of water', 'location', \n",
    " 'mouth of the watercourse', 'point in time', 'capital', 'capital of', \n",
    " 'shares border with', 'tributary', 'diplomatic relation', 'place of publication', 'spouse'], \n",
    " 'residence': [ 'place of birth', 'based on' ,  \n",
    " 'country of citizenship', 'date of birth' , 'has part', \n",
    " 'number of participants', 'history of topic',  'place of birth', \n",
    "'country of origin', 'has quality',  \n",
    "'significant event','occupant', 'relative', 'residence']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total length of unique list values: 126\n",
      "Count of unique elements across all lists: 90\n"
     ]
    }
   ],
   "source": [
    "unique_values = set(tuple(value) for value in mapping.values())\n",
    "\n",
    "# Calculate total length of unique list values (now as tuples)\n",
    "total_length = sum(len(value) for value in unique_values)\n",
    "\n",
    "# Count of unique list values\n",
    "unique_count = len(unique_values)\n",
    "\n",
    "print(\"Total length of unique list values:\", total_length)\n",
    "all_values = [item for sublist in mapping.values() for item in sublist]\n",
    "\n",
    "# Convert the flattened list to a set to remove duplicates\n",
    "unique_values = set(all_values)\n",
    "\n",
    "# Count of unique elements across all lists\n",
    "unique_count = len(unique_values)\n",
    "\n",
    "print(\"Count of unique elements across all lists:\", unique_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New file 'new_data.json' created with updated types.\n"
     ]
    }
   ],
   "source": [
    "with open('/Users/ananyahooda/Desktop/final/comparabe_ensemble_predi.json' , 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "new_data = []\n",
    "for obj in data:\n",
    "    new_obj = {\n",
    "        \"title\": obj[\"title\"],\n",
    "        \"context\": obj[\"context\"],\n",
    "        \"id\": obj[\"id\"],\n",
    "        \"triples\": []\n",
    "    }\n",
    "    for triple in obj[\"triples\"]:\n",
    "        for key, value in mapping.items():\n",
    "            if triple[\"type\"] in value:\n",
    "                triple[\"type\"] = key\n",
    "                break\n",
    "        new_obj[\"triples\"].append(triple)\n",
    "    new_data.append(new_obj)\n",
    "\n",
    "# Write new JSON objects to a new file\n",
    "with open(\"/Users/ananyahooda/Desktop/final/final_ensembled_5rel_nostring.json\", \"w\") as outfile:\n",
    "    json.dump(new_data, outfile, indent=4)\n",
    "\n",
    "print(\"New file 'new_data.json' created with updated types.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(('head', 'William R. Higgins'), ('type', 'headquarters location'), ('tail', 'U.S. Marine')), (('head', 'William R. Higgins'), ('type', 'employer'), ('tail', 'Organization of the Oppressed on Earth'))}\n",
      "{(('head', 'King was killed'), ('type', 'headquarters location'), ('tail', 'Memphis, Tenn.')), (('head', 'King'), ('type', 'killed by'), ('tail', 'Memphis, Tenn.')), (('head', 'King was killed'), ('type', 'location'), ('tail', 'April 4, 1968'))}\n",
      "{(('head', 'shot Sen. Robert F. Kennedy'), ('type', 'killed by'), ('tail', 'Sirhan B. Sirhan')), (('head', 'Sirhan B. Sirhan'), ('type', 'killed by'), ('tail', '22 years ago'))}\n",
      "{(('head', 'Berlin'), ('type', 'location'), ('tail', 'Federal Republic')), (('head', 'Federal Administrative Court'), ('type', 'location'), ('tail', 'Federal Republic')), (('head', 'Federal Republic'), ('type', 'location'), ('tail', 'Berlin'))}\n",
      "{(('head', 'King'), ('type', 'killed by'), ('tail', 'Memphis, Tenn.')), (('head', 'killing of King'), ('type', 'location'), ('tail', 'April 4, 1968')), (('head', 'killing of King'), ('type', 'headquarters location'), ('tail', 'Memphis, Tenn.'))}\n",
      "{(('head', 'Taiwan information center'), ('type', 'headquarters location'), ('tail', 'National Press Building')), (('head', 'National Press Building'), ('type', 'headquarters location'), ('tail', 'Washington'))}\n",
      "{(('head', 'Sultan Al-Saud'), ('type', 'residence'), ('tail', 'Saudi Arabia')), (('head', 'Saudi Arabia'), ('type', 'employer'), ('tail', 'Sultan Al-Saud'))}\n",
      "{(('head', 'explosive'), ('type', 'residence'), ('tail', 'detonator charge')), (('head', 'Cecil Field Naval Air Station'), ('type', 'employer'), ('tail', 'Bert Byers'))}\n",
      "{(('head', 'United States'), ('type', 'headquarters location'), ('tail', 'Kentucky')), (('head', 'Kentucky'), ('type', 'location'), ('tail', 'United States'))}\n",
      "set()\n",
      "{(('head', 'Nairobi'), ('type', 'employer'), ('tail', 'English')), (('head', 'KNA'), ('type', 'headquarters location'), ('tail', 'Nairobi'))}\n",
      "{(('head', 'California'), ('type', 'employer'), ('tail', 'West')), (('head', 'Oregon'), ('type', 'employer'), ('tail', 'West')), (('head', 'Washington'), ('type', 'employer'), ('tail', 'West')), (('head', 'Idaho'), ('type', 'employer'), ('tail', 'West')), (('head', 'Boise Interagency Fire Center'), ('type', 'headquarters location'), ('tail', 'Idaho')), (('head', 'Idaho'), ('type', 'headquarters location'), ('tail', 'Boise Interagency Fire Center'))}\n",
      "{(('head', \"China's 21st Century Agenda\"), ('type', 'employer'), ('tail', 'Deng Nan')), (('head', 'Deng Nan'), ('type', 'employer'), ('tail', \"China's 21st Century Agenda\"))}\n",
      "{(('head', 'Chile'), ('type', 'location'), ('tail', 'Santiago')), (('head', 'Valparaiso'), ('type', 'location'), ('tail', 'Chile'))}\n",
      "{(('head', 'MILAN'), ('type', 'location'), ('tail', 'Italy')), (('head', 'Italy'), ('type', 'location'), ('tail', 'MILAN'))}\n",
      "{(('head', 'Khieu Samphan'), ('type', 'employer'), ('tail', 'Khmer Rouge'))}\n",
      "{(('head', 'Tennessee'), ('type', 'location'), ('tail', 'Nashville')), (('head', 'Tenn.'), ('type', 'location'), ('tail', 'Nashville'))}\n",
      "{(('head', 'Ulysses S. Grant'), ('type', 'residence'), ('tail', 'Point Pleasant, Ohio')), (('head', 'Ulysses S. Grant'), ('type', 'employer'), ('tail', 'president of the United States'))}\n",
      "{(('head', 'An-Nahar'), ('type', 'employer'), ('tail', 'Arabic'))}\n",
      "{(('head', 'PLO Chairman'), ('type', 'employer'), ('tail', \"Yasir 'Arafat\")), (('head', \"Yasir 'Arafat\"), ('type', 'employer'), ('tail', 'PLO Chairman'))}\n",
      "{(('head', 'Jakarta'), ('type', 'employer'), ('tail', 'Bangkok')), (('head', 'United States'), ('type', 'location'), ('tail', 'South Korea')), (('head', 'United States'), ('type', 'location'), ('tail', 'Japan')), (('head', 'Japan'), ('type', 'location'), ('tail', 'United States')), (('head', 'Japan'), ('type', 'location'), ('tail', 'South Korea')), (('head', 'South Korea'), ('type', 'location'), ('tail', 'United States')), (('head', 'South Korea'), ('type', 'location'), ('tail', 'Japan')), (('head', 'Bangkok'), ('type', 'employer'), ('tail', 'Jakarta'))}\n",
      "{(('head', 'assassinating President John F. Kennedy'), ('type', 'killed by'), ('tail', 'Lee Harvey Oswald')), (('head', 'Warren Commission'), ('type', 'killed by'), ('tail', 'assassinating President John F. Kennedy')), (('head', 'Lee Harvey Oswald'), ('type', 'employer'), ('tail', 'assassinating President John F. Kennedy'))}\n",
      "{(('head', 'State Science and Technology Commission'), ('type', 'headquarters location'), ('tail', 'Beijing')), (('head', 'Deng Nan'), ('type', 'employer'), ('tail', 'Vice of the State Science and Technology Commission'))}\n",
      "{(('head', 'Arab League Secretary-General'), ('type', 'employer'), ('tail', 'Chedli Klibi')), (('head', 'Chedli Klibi'), ('type', 'employer'), ('tail', 'Arab League Secretary-General'))}\n",
      "{(('head', 'assassinated'), ('type', 'location'), ('tail', 'Dr. Martin Luther King Jr.')), (('head', 'James Earl Ray'), ('type', 'employer'), ('tail', 'assassin of Dr. Martin Luther King Jr.')), (('head', 'assassin of Dr. Martin Luther King Jr.'), ('type', 'employer'), ('tail', 'James Earl Ray')), (('head', 'James Earl Ray'), ('type', 'killed by'), ('tail', 'assassinated'))}\n",
      "{(('head', 'West Germany'), ('type', 'employer'), ('tail', 'AP')), (('head', 'West Germany'), ('type', 'headquarters location'), ('tail', 'AP'))}\n",
      "{(('head', 'Pacific Marine'), ('type', 'headquarters location'), ('tail', 'Houston'))}\n",
      "{(('head', 'Benjamin Harrison'), ('type', 'employer'), ('tail', 'president of the United States')), (('head', 'Benjamin Harrison'), ('type', 'residence'), ('tail', 'North Bend, Ohio'))}\n",
      "{(('head', 'Iran'), ('type', 'location'), ('tail', 'Tehran')), (('head', 'Iran'), ('type', 'employer'), ('tail', 'London')), (('head', 'London'), ('type', 'employer'), ('tail', 'Iran'))}\n",
      "{(('head', 'Mark David Chapman'), ('type', 'killed by'), ('tail', 'Mark David Chapman')), (('head', 'Mark David Chapman'), ('type', 'killed by'), ('tail', 'shot and killed him')), (('head', 'shot and killed him'), ('type', 'employer'), ('tail', 'Mark David Chapman'))}\n",
      "{(('head', 'Austria'), ('type', 'headquarters location'), ('tail', 'Vienna')), (('head', 'Wolfgang Amadeus Mozart'), ('type', 'killed by'), ('tail', 'Vienna, Austria')), (('head', 'Martin Van Buren'), ('type', 'residence'), ('tail', 'Kinderhook, N.Y.')), (('head', 'Martin Van Buren'), ('type', 'employer'), ('tail', 'president')), (('head', 'Wolfgang Amadeus Mozart'), ('type', 'killed by'), ('tail', 'Vienna'))}\n",
      "{(('head', 'Elizabeth Dole'), ('type', 'residence'), ('tail', 'Salisbury, N.C.'))}\n",
      "{(('head', 'AP'), ('type', 'location'), ('tail', 'New Orleans')), (('head', 'AP'), ('type', 'headquarters location'), ('tail', 'New Orleans'))}\n",
      "{(('head', \"motorcade at Kennedy's motorcade\"), ('type', 'killed by'), ('tail', 'Lee Harvey Oswald')), (('head', 'Lee Harvey Oswald'), ('type', 'employer'), ('tail', \"motorcade at Kennedy's motorcade\"))}\n",
      "{(('head', 'ANP'), ('type', 'headquarters location'), ('tail', 'The Hague')), (('head', 'The Hague ANP'), ('type', 'employer'), ('tail', 'Dutch'))}\n",
      "{(('head', 'Hot Springs National Park'), ('type', 'location'), ('tail', 'Ark.')), (('head', 'Hot Springs National Park'), ('type', 'headquarters location'), ('tail', 'Ark.'))}\n",
      "{(('head', 'John Savage'), ('type', 'employer'), ('tail', 'Nova Scotia Prime Minister'))}\n",
      "{(('head', 'Reagan'), ('type', 'employer'), ('tail', 'President'))}\n",
      "{(('head', 'Jesse Alexander Helms Jr.'), ('type', 'residence'), ('tail', 'Monroe'))}\n",
      "{(('head', 'Arabic'), ('type', 'location'), ('tail', 'Lebanon')), (('head', 'Beirut Radio'), ('type', 'location'), ('tail', 'Lebanon')), (('head', 'Beirut Radio Lebanon'), ('type', 'employer'), ('tail', 'Arabic'))}\n",
      "{(('head', 'Stepanaket'), ('type', 'location'), ('tail', 'Nagorno-Karabakh')), (('head', 'Azerbaijan'), ('type', 'location'), ('tail', 'Nagorno-Karabakh')), (('head', 'Stepanaket'), ('type', 'location'), ('tail', 'Azerbaijan')), (('head', 'Nagorno-Karabakh'), ('type', 'location'), ('tail', 'Stepanaket')), (('head', 'Nagorno-Karabakh'), ('type', 'location'), ('tail', 'Azerbaijan')), (('head', 'Azerbaijan'), ('type', 'headquarters location'), ('tail', 'Stepanaket'))}\n",
      "{(('head', 'Ottawa'), ('type', 'location'), ('tail', 'Canada')), (('head', 'Bradley First Air'), ('type', 'headquarters location'), ('tail', 'Ottawa'))}\n",
      "set()\n",
      "{(('head', 'Oliver Hazard Perry'), ('type', 'residence'), ('tail', 'South Kingstown, R.I.'))}\n",
      "{(('head', 'assassination'), ('type', 'killed by'), ('tail', 'Martin Luther King Jr.')), (('head', 'Martin Luther King Jr.'), ('type', 'killed by'), ('tail', 'Memphis, Tenn.'))}\n",
      "{(('head', 'Cecil Andrus'), ('type', 'employer'), ('tail', 'Idaho Gov.'))}\n",
      "{(('head', 'Yves Fortier'), ('type', 'employer'), ('tail', 'ambassador'))}\n",
      "{(('head', \"Kennedy's\"), ('type', 'killed by'), ('tail', 'Sirhan Bishara Sirhan')), (('head', 'Sirhan Bishara Sirhan'), ('type', 'employer'), ('tail', 'assassin'))}\n",
      "{(('head', 'Jesse Jackson'), ('type', 'employer'), ('tail', 'Democratic'))}\n",
      "{(('head', 'ENTV Television Network'), ('type', 'headquarters location'), ('tail', 'Algiers'))}\n",
      "{(('head', 'Abdul Karim Obeid'), ('type', 'killed by'), ('tail', 'hanging')), (('head', 'Abdul Karim Obeid'), ('type', 'killed by'), ('tail', 'July 31, 1989'))}\n",
      "{(('head', 'killing John Lennon'), ('type', 'employer'), ('tail', 'Mark David Chapman')), (('head', 'John Lennon'), ('type', 'killed by'), ('tail', 'Mark David Chapman'))}\n",
      "{(('head', 'Ukrainian'), ('type', 'employer'), ('tail', 'Ivan Plyushch')), (('head', 'Ivan Plyushch'), ('type', 'residence'), ('tail', 'Ukrainian'))}\n",
      "{(('head', 'Channel Islands'), ('type', 'residence'), ('tail', 'Guernsey')), (('head', 'Channel Islands'), ('type', 'location'), ('tail', 'France')), (('head', 'Guernsey'), ('type', 'employer'), ('tail', 'Channel Islands'))}\n",
      "{(('head', 'Ralph K. Winter'), ('type', 'employer'), ('tail', 'President Reagan')), (('head', 'Reagan'), ('type', 'employer'), ('tail', 'President')), (('head', 'Kenneth Starr'), ('type', 'employer'), ('tail', 'President Reagan'))}\n",
      "{(('head', 'PERUGIA'), ('type', 'location'), ('tail', 'Italy')), (('head', 'Italy'), ('type', 'employer'), ('tail', 'AP'))}\n",
      "{(('head', 'Lowell Thomas'), ('type', 'residence'), ('tail', 'Woodington, Ohio')), (('head', 'Lowell Thomas'), ('type', 'residence'), ('tail', '1892'))}\n",
      "{(('head', 'King County'), ('type', 'headquarters location'), ('tail', 'Issaquah')), (('head', 'Issaquah'), ('type', 'headquarters location'), ('tail', 'King County'))}\n",
      "{(('head', 'Jack Ruby'), ('type', 'location'), ('tail', 'Lee Harvey Oswald')), (('head', 'Lee Harvey Oswald'), ('type', 'killed by'), ('tail', 'Jack Ruby')), (('head', 'John F. Kennedy'), ('type', 'killed by'), ('tail', 'Lee Harvey Oswald')), (('head', 'Tammy Bakker'), ('type', 'location'), ('tail', 'Jim')), (('head', 'Jim'), ('type', 'location'), ('tail', 'Tammy Bakker'))}\n",
      "{(('head', '944K1078A Kiev PRAVDA UKRAINY'), ('type', 'employer'), ('tail', 'Russian')), (('head', 'UKRAINY'), ('type', 'location'), ('tail', 'Kiev'))}\n",
      "{(('head', 'UKRAYINY'), ('type', 'headquarters location'), ('tail', 'Kiev')), (('head', 'Kiev'), ('type', 'headquarters location'), ('tail', 'UKRAYINY'))}\n",
      "{(('head', 'ATLANTIC CITY'), ('type', 'headquarters location'), ('tail', 'N.J.')), (('head', 'N. Jersey'), ('type', 'location'), ('tail', 'ATLANTIC CITY'))}\n",
      "{(('head', 'San Ignacio de Cajamarca'), ('type', 'location'), ('tail', 'Jaen')), (('head', 'Jaen'), ('type', 'employer'), ('tail', 'provinces')), (('head', 'Jaen'), ('type', 'location'), ('tail', 'San Ignacio de Cajamarca')), (('head', 'San Ignacio de Cajamarca'), ('type', 'employer'), ('tail', 'provinces'))}\n",
      "{(('head', 'Russian Center for Problems of National Security and International Relations'), ('type', 'employer'), ('tail', 'Sergey Rogov'))}\n",
      "{(('head', 'Khmer Rouge'), ('type', 'location'), ('tail', 'Cambodia')), (('head', 'Khmer Rouge executions, famine and civil unrest'), ('type', 'location'), ('tail', 'Cambodia'))}\n",
      "{(('head', 'William J. Bennett'), ('type', 'employer'), ('tail', 'U.S. Secretary'))}\n",
      "{(('head', 'PY3103144994 Lima EL COMERCIO'), ('type', 'employer'), ('tail', 'Spanish')), (('head', 'Lima'), ('type', 'employer'), ('tail', 'Spanish'))}\n",
      "{(('head', 'Morning Edition'), ('type', 'employer'), ('tail', 'Japanese')), (('head', 'Japan'), ('type', 'headquarters location'), ('tail', 'Tokyo')), (('head', 'Tokyo'), ('type', 'location'), ('tail', 'Japan'))}\n",
      "{(('head', 'John Deere'), ('type', 'residence'), ('tail', 'Rutland, Vt.')), (('head', 'John Deere'), ('type', 'residence'), ('tail', '1804'))}\n",
      "{(('head', 'American system'), ('type', 'named after'), ('tail', 'We the People')), (('head', 'America'), ('type', 'residence'), ('tail', 'American history')), (('head', 'American history'), ('type', 'killed by'), ('tail', 'America'))}\n",
      "{(('head', 'Ontario'), ('type', 'headquarters location'), ('tail', 'California')), (('head', 'El Toro'), ('type', 'location'), ('tail', 'Ontario')), (('head', 'El Toro'), ('type', 'headquarters location'), ('tail', 'California')), (('head', 'Ontario'), ('type', 'location'), ('tail', 'El Toro'))}\n",
      "{(('head', 'FRESNO'), ('type', 'headquarters location'), ('tail', 'Calif.')), (('head', 'FRESNO'), ('type', 'employer'), ('tail', 'city'))}\n",
      "{(('head', 'Chisinau'), ('type', 'location'), ('tail', 'AU2405185594')), (('head', '1750 GMT'), ('type', 'location'), ('tail', '1750'))}\n",
      "{(('head', 'Rutherford B. Hayes'), ('type', 'employer'), ('tail', 'president of the United States')), (('head', 'Rutherford B. Hayes'), ('type', 'residence'), ('tail', 'Delaware, Ohio'))}\n",
      "set()\n",
      "{(('head', 'Nettie Johnson'), ('type', 'headquarters location'), ('tail', 'Navy')), (('head', 'F/A-18 Super Hornet'), ('type', 'residence'), ('tail', 'F/A-18 Hornet')), (('head', 'F/A-18 Hornet'), ('type', 'employer'), ('tail', 'F/A-18 Super Hornet'))}\n",
      "{(('head', 'Elvis Presley'), ('type', 'residence'), ('tail', 'Tupelo, Miss')), (('head', 'Elvis Presley'), ('type', 'residence'), ('tail', 'Tupelo, Miss.'))}\n",
      "set()\n",
      "{(('head', 'Omaha Beach'), ('type', 'employer'), ('tail', 'Normandy landings')), (('head', 'Normandy landings'), ('type', 'residence'), ('tail', 'Omaha Beach'))}\n",
      "{(('head', 'Jesse Owens'), ('type', 'residence'), ('tail', 'Danville, Ala.'))}\n",
      "{(('head', 'ITAR-TAS'), ('type', 'headquarters location'), ('tail', 'Moscow')), (('head', 'Further Reaction to Iranian Plane Crash Experts'), ('type', 'employer'), ('tail', 'ITAR-TAS'))}\n",
      "{(('head', 'Louisiana'), ('type', 'headquarters location'), ('tail', 'Grand Isle')), (('head', 'Grand Isle'), ('type', 'headquarters location'), ('tail', 'La'))}\n",
      "{(('head', 'James Monroe'), ('type', 'residence'), ('tail', 'Westmoreland County, Virginia')), (('head', 'James Monroe'), ('type', 'employer'), ('tail', 'president of the United States')), (('head', 'James Monroe'), ('type', 'residence'), ('tail', 'Westmoreland County,Virginia'))}\n",
      "{(('head', 'assassination'), ('type', 'location'), ('tail', 'Dr. Martin Luther King Jr.')), (('head', 'assassination of civil rights leader Dr. Martin Luther King Jr.'), ('type', 'employer'), ('tail', 'James Earl Ray'))}\n",
      "{(('head', 'Eduard A. Shevardnadze'), ('type', 'employer'), ('tail', 'Soviet Foreign')), (('head', 'Soviet Foreign'), ('type', 'employer'), ('tail', 'Eduard A. Shevardnadze'))}\n",
      "{(('head', 'United States'), ('type', 'headquarters location'), ('tail', 'U.S. Congress'))}\n",
      "{(('head', 'Panama City'), ('type', 'employer'), ('tail', 'ACAN'))}\n",
      "{(('head', 'Ford Theatre'), ('type', 'headquarters location'), ('tail', 'Washington')), (('head', 'shot in the head'), ('type', 'employer'), ('tail', 'John Wilkes Booth')), (('head', 'John Wilkes Booth'), ('type', 'location'), ('tail', 'Lincoln'))}\n",
      "{(('head', 'Bonloc'), ('type', 'location'), ('tail', 'France')), (('head', 'Poitiers'), ('type', 'location'), ('tail', 'France')), (('head', 'Bayonee'), ('type', 'location'), ('tail', 'France')), (('head', 'Poitiers'), ('type', 'employer'), ('tail', 'southwestern France'))}\n",
      "{(('head', 'Park Chung-hee'), ('type', 'employer'), ('tail', 'South Korean President'))}\n",
      "{(('head', 'former Soviet Union'), ('type', 'headquarters location'), ('tail', 'Ukraine')), (('head', 'Ukraine'), ('type', 'economy of topic'), ('tail', 'gross national product')), (('head', 'Ukraine'), ('type', 'employer'), ('tail', 'former Soviet Union'))}\n",
      "{(('head', 'European Community'), ('type', 'residence'), ('tail', '12')), (('head', 'European Community'), ('type', 'location'), ('tail', 'Europe'))}\n",
      "{(('head', 'Bush'), ('type', 'employer'), ('tail', 'President-elect')), (('head', 'inauguration of President-elect Bush'), ('type', 'location'), ('tail', 'next week'))}\n",
      "{(('head', 'Seoul'), ('type', 'location'), ('tail', 'Pacific Ocean')), (('head', 'South Korea'), ('type', 'location'), ('tail', 'Pacific Ocean')), (('head', 'South Korea'), ('type', 'headquarters location'), ('tail', 'Seoul')), (('head', 'Seoul'), ('type', 'location'), ('tail', 'South Korea')), (('head', 'South Korea'), ('type', 'location'), ('tail', 'Seoul'))}\n",
      "{(('head', 'Aussedat-Rey SA'), ('type', 'employer'), ('tail', 'copecopy paper')), (('head', 'Aussedat-Rey SA'), ('type', 'employer'), ('tail', 'photocopy paper'))}\n",
      "{(('head', 'Cuban missile crisis'), ('type', 'employer'), ('tail', 'Nikita Khrushchev')), (('head', 'Cuban missile crisis'), ('type', 'employer'), ('tail', 'Kennedy'))}\n",
      "{(('head', 'Center for Disease Control'), ('type', 'headquarters location'), ('tail', 'Fort Collins'))}\n",
      "{(('head', 'Duesseldorf'), ('type', 'employer'), ('tail', 'German')), (('head', 'Esprit Project to Develop Chip to Receive, Transmit Nerve Impulses'), ('type', 'headquarters location'), ('tail', 'Duesseldorf'))}\n",
      "{(('head', 'Bette Davis'), ('type', 'residence'), ('tail', 'Lowell, Mass.')), (('head', 'Bette Davis'), ('type', 'residence'), ('tail', 'April 5, 1908'))}\n",
      "{(('head', 'Ernest Tidwell'), ('type', 'employer'), ('tail', 'U.S. District')), (('head', 'Poison pill'), ('type', 'discoverer or inventor'), ('tail', 'Ernest Tidwell'))}\n",
      "{(('head', 'Kim Il-song'), ('type', 'employer'), ('tail', 'North Korean President')), (('head', 'North Korean President'), ('type', 'employer'), ('tail', 'Kim Il-song'))}\n",
      "{(('head', 'Ostied'), ('type', 'location'), ('tail', 'Liberec')), (('head', 'Czechoslovakia'), ('type', 'location'), ('tail', 'Austria')), (('head', 'Austria'), ('type', 'location'), ('tail', 'Czechoslovakia')), (('head', 'Liberec'), ('type', 'location'), ('tail', 'Lesny')), (('head', 'Ostied'), ('type', 'location'), ('tail', 'Lesny')), (('head', 'Lesny'), ('type', 'location'), ('tail', 'Liberec')), (('head', 'Liberec'), ('type', 'location'), ('tail', 'Ostied')), (('head', 'Liberec'), ('type', 'location'), ('tail', 'Gmund')), (('head', 'Ostied'), ('type', 'location'), ('tail', 'Gmund')), (('head', 'Gmund'), ('type', 'location'), ('tail', 'Austria')), (('head', 'Gmund'), ('type', 'location'), ('tail', 'Liberec')), (('head', 'Lesny'), ('type', 'location'), ('tail', 'Ostied')), (('head', 'Austria'), ('type', 'headquarters location'), ('tail', 'Gmund'))}\n",
      "{(('head', 'Kim Pom-myon'), ('type', 'employer'), ('tail', 'LDP'))}\n",
      "set()\n",
      "{(('head', 'Georgiy Gurinov'), ('type', 'employer'), ('tail', 'Pacific Fleet Commander')), (('head', 'Pacific Fleet Commander'), ('type', 'employer'), ('tail', 'Georgiy Gurinov'))}\n",
      "{(('head', 'Somoza Garcia'), ('type', 'employer'), ('tail', 'Sandinistas')), (('head', 'Somoza Garcia'), ('type', 'killed by'), ('tail', 'Rigoberto Lopez Perez'))}\n",
      "{(('head', 'Cape Hatteras'), ('type', 'headquarters location'), ('tail', 'N.C.')), (('head', 'Kansas'), ('type', 'location'), ('tail', 'EDT'))}\n",
      "{(('head', 'Blue Ball'), ('type', 'employer'), ('tail', 'Darryl Breniser')), (('head', 'Darryl Breniser'), ('type', 'employer'), ('tail', 'Blue Ball'))}\n",
      "{(('head', 'North Korea'), ('type', 'employer'), ('tail', 'IAEA'))}\n",
      "{(('head', 'assassination'), ('type', 'location'), ('tail', 'Robert F. Kennedy')), (('head', 'assassination'), ('type', 'killed by'), ('tail', 'Robert F. Kennedy'))}\n",
      "{(('head', 'Sindisef'), ('type', 'headquarters location'), ('tail', 'Sao Paulo')), (('head', 'Federal Employees Union'), ('type', 'headquarters location'), ('tail', 'Sao Paulo'))}\n",
      "{(('head', 'assassinating President John F. Kennedy'), ('type', 'killed by'), ('tail', 'Lee Harvey Oswald')), (('head', 'assassinating President John F. Kennedy'), ('type', 'employer'), ('tail', 'Lee Harvey Oswald')), (('head', 'Warren Commission'), ('type', 'killed by'), ('tail', 'assassinating President John F. Kennedy')), (('head', 'Lee Harvey Oswald'), ('type', 'employer'), ('tail', 'assassinating President John F. Kennedy'))}\n",
      "{(('head', 'Bob Dole'), ('type', 'residence'), ('tail', 'Russell, Kan.')), (('head', '1980 Republican presidential nomination'), ('type', 'employer'), ('tail', 'Bob Dole')), (('head', 'Bob Dole'), ('type', 'employer'), ('tail', 'Republican'))}\n",
      "{(('head', 'Dakota building'), ('type', 'headquarters location'), ('tail', 'New York')), (('head', 'Mark David Chapman'), ('type', 'killed by'), ('tail', 'New York'))}\n",
      "{(('head', 'Lennon'), ('type', 'killed by'), ('tail', 'Lennon')), (('head', 'Murder of John Lennon'), ('type', 'location'), ('tail', 'Lennon'))}\n",
      "set()\n",
      "{(('head', 'Ambassador Hotel'), ('type', 'headquarters location'), ('tail', 'Los Angeles')), (('head', 'Robert F. Kennedy'), ('type', 'killed by'), ('tail', 'Ambassador Hotel')), (('head', 'Robert F. Kennedy'), ('type', 'killed by'), ('tail', 'Los Angeles')), (('head', 'Robert F. Kennedy'), ('type', 'killed by'), ('tail', 'June 6, 1968')), (('head', 'Robert F. Kennedy'), ('type', 'killed by'), ('tail', 'Good Samaritan Hospital')), (('head', 'Good Samaritan Hospital'), ('type', 'headquarters location'), ('tail', 'Los Angeles')), (('head', 'assassinated'), ('type', 'location'), ('tail', 'Ambassador Hotel')), (('head', 'Ambassador Hotel'), ('type', 'killed by'), ('tail', 'assassinated'))}\n",
      "{(('head', 'International Paper'), ('type', 'location'), ('tail', 'French')), (('head', 'International Paper'), ('type', 'employer'), ('tail', 'Ann Silvernail'))}\n",
      "{(('head', 'Soviet'), ('type', 'location'), ('tail', 'Moscow'))}\n",
      "{(('head', 'Louis Sullivan'), ('type', 'employer'), ('tail', 'HHS Secretary'))}\n",
      "{(('head', 'Signal FM'), ('type', 'employer'), ('tail', 'Signal Records')), (('head', 'Signal FM'), ('type', 'employer'), ('tail', 'Jean-Luckenson'))}\n",
      "{(('head', 'Condorcanqui'), ('type', 'headquarters location'), ('tail', 'Amazonas')), (('head', 'Rioja'), ('type', 'headquarters location'), ('tail', 'Biabo')), (('head', 'Amazonas'), ('type', 'headquarters location'), ('tail', 'Bagua')), (('head', 'Huancabamba'), ('type', 'headquarters location'), ('tail', 'Piura')), (('head', 'Nueva Cajamarca'), ('type', 'headquarters location'), ('tail', 'Rioja')), (('head', 'Amazonas'), ('type', 'location'), ('tail', 'Piura')), (('head', 'Piura'), ('type', 'headquarters location'), ('tail', 'Huancabamba')), (('head', 'Piura'), ('type', 'location'), ('tail', 'Amazonas')), (('head', 'Amazonas'), ('type', 'headquarters location'), ('tail', 'Condorcanqui')), (('head', 'Rioja'), ('type', 'headquarters location'), ('tail', 'Soritor')), (('head', 'Bagua'), ('type', 'headquarters location'), ('tail', 'Amazonas')), (('head', 'Rioja'), ('type', 'headquarters location'), ('tail', 'Nueva Cajamarca'))}\n",
      "{(('head', 'Cambodia'), ('type', 'employer'), ('tail', 'Hun Sen')), (('head', 'Hun Sen'), ('type', 'employer'), ('tail', 'Prime Minister')), (('head', 'Cambodia'), ('type', 'employer'), ('tail', 'Prime Minister')), (('head', 'Prime Minister'), ('type', 'employer'), ('tail', 'Hun Sen'))}\n",
      "{(('head', 'Booth Gardner'), ('type', 'employer'), ('tail', 'Democrat')), (('head', 'Terry Branstad'), ('type', 'employer'), ('tail', 'Republican'))}\n",
      "{(('head', 'Michael Coats'), ('type', 'headquarters location'), ('tail', 'Navy'))}\n",
      "{(('head', 'Bob Dylan'), ('type', 'residence'), ('tail', 'Duluth'))}\n",
      "{(('head', 'assassinating'), ('type', 'location'), ('tail', 'Robert F. Kennedy')), (('head', 'Sirhan Sirhan'), ('type', 'killed by'), ('tail', 'assassinating Sen. Robert F. Kennedy')), (('head', 'assassinating'), ('type', 'killed by'), ('tail', 'Sirhan Sirhan'))}\n",
      "{(('head', 'Lee Harvey Oswald'), ('type', 'killed by'), ('tail', 'assassinated President Kennedy')), (('head', 'Lee Harvey Oswald'), ('type', 'killed by'), ('tail', 'shot President Kennedy')), (('head', 'assassinated President Kennedy'), ('type', 'killed by'), ('tail', 'Lee Harvey Oswald')), (('head', 'shot President Kennedy'), ('type', 'employer'), ('tail', 'Lee Harvey Oswald'))}\n",
      "{(('head', 'Tunisia'), ('type', 'headquarters location'), ('tail', 'TUNIS')), (('head', 'Tunisia'), ('type', 'employer'), ('tail', 'AP')), (('head', 'TUNIS'), ('type', 'location'), ('tail', 'Tunisia'))}\n",
      "{(('head', 'NRC'), ('type', 'headquarters location'), ('tail', 'DOE')), (('head', 'Robert Bernero'), ('type', 'employer'), ('tail', 'waste disposal'))}\n",
      "{(('head', 'Mircea Snegur'), ('type', 'employer'), ('tail', 'President')), (('head', 'Mircea Snegur'), ('type', 'employer'), ('tail', 'Agrarian Democratic Party of Moldova')), (('head', 'Valentin Krylov'), ('type', 'employer'), ('tail', 'Socialist Party'))}\n",
      "{(('head', 'Solidarity'), ('type', 'employer'), ('tail', '1980')), (('head', 'Solidarity'), ('type', 'employer'), ('tail', 'union movement'))}\n",
      "{(('head', 'the shots that killed President John F. Kennedy'), ('type', 'employer'), ('tail', 'Lee Harvey Oswald')), (('head', 'the shots that killed President John F. Kennedy'), ('type', 'location'), ('tail', 'Texas School Book Depository')), (('head', 'John F. Kennedy'), ('type', 'killed by'), ('tail', 'Lee Harvey Oswald')), (('head', 'Lee Harvey Oswald'), ('type', 'location'), ('tail', 'John F. Kennedy')), (('head', 'Texas School Book Depository'), ('type', 'killed by'), ('tail', 'the shots that killed President John F. Kennedy'))}\n",
      "{(('head', 'Great City Schools'), ('type', 'headquarters location'), ('tail', 'Council of Great City Schools')), (('head', 'Council of Great City Schools'), ('type', 'employer'), ('tail', 'Samuel L. Husk'))}\n",
      "{(('head', 'Brigham Young'), ('type', 'residence'), ('tail', 'Whitingham, Vt.'))}\n",
      "{(('head', 'assassination'), ('type', 'headquarters location'), ('tail', 'Dallas')), (('head', 'Lee Harvey Oswald'), ('type', 'employer'), ('tail', 'assassinated'))}\n",
      "{(('head', 'Knight-Ridder'), ('type', 'employer'), ('tail', 'James K. Batten'))}\n",
      "{(('head', 'Lyndon B. Johnson'), ('type', 'employer'), ('tail', 'president')), (('head', 'president'), ('type', 'employer'), ('tail', 'Lyndon B. Johnson'))}\n",
      "{(('head', 'Nelson Mandela'), ('type', 'employer'), ('tail', 'African National Congress')), (('head', 'African National Congress'), ('type', 'employer'), ('tail', 'Nelson Mandela'))}\n",
      "{(('head', 'Idaho Falls, Idaho'), ('type', 'location'), ('tail', 'Savannah River')), (('head', 'Energy Department facility'), ('type', 'headquarters location'), ('tail', 'Idaho Falls, Idaho'))}\n",
      "{(('head', 'Branko Vojnic'), ('type', 'employer'), ('tail', 'Serb Radical Party'))}\n",
      "{(('head', 'Watergate special prosecutor'), ('type', 'employer'), ('tail', 'Watergate')), (('head', 'Watergate special prosecutor'), ('type', 'killed by'), ('tail', 'Watergate'))}\n",
      "{(('head', 'president of the Republic of South Africa'), ('type', 'employer'), ('tail', 'Nelson Rohihlahla Mandela')), (('head', 'Nelson Rohihlahla Mandela'), ('type', 'employer'), ('tail', 'president of the Republic of South Africa'))}\n",
      "{(('head', 'Nodrm-1'), ('type', 'employer'), ('tail', 'molecule')), (('head', 'Institute of Organic Chemistry'), ('type', 'headquarters location'), ('tail', 'Shanghai'))}\n",
      "{(('head', 'Texas Tech University'), ('type', 'headquarters location'), ('tail', 'Lubbock')), (('head', 'Buddy Holly'), ('type', 'residence'), ('tail', 'Lubbock')), (('head', 'Buddy Holly'), ('type', 'employer'), ('tail', 'Texas Tech University'))}\n",
      "{(('head', \"Sheriff's\"), ('type', 'headquarters location'), ('tail', 'Bingham County')), (('head', \"Bingham County Sheriff's\"), ('type', 'headquarters location'), ('tail', 'Bingham County')), (('head', 'Bingham County'), ('type', 'employer'), ('tail', \"Sheriff's\"))}\n",
      "set()\n",
      "set()\n",
      "{(('head', 'Anastas Mikoyan'), ('type', 'employer'), ('tail', 'Soviet first deputy premier'))}\n",
      "{(('head', 'Washington'), ('type', 'location'), ('tail', 'Bering Sea')), (('head', 'Bering Sea'), ('type', 'employer'), ('tail', 'Okhotsk Sea'))}\n",
      "set()\n",
      "{(('head', 'Shaw Yu-ming'), ('type', 'employer'), ('tail', 'director-general of Government Information')), (('head', 'Government Information'), ('type', 'employer'), ('tail', 'Shaw Yu-ming'))}\n",
      "{(('head', \"Maryland's House of Delegates\"), ('type', 'employer'), ('tail', 'Judith C. Toth')), (('head', 'House of Delegates'), ('type', 'headquarters location'), ('tail', 'Maryland'))}\n",
      "{(('head', 'Payette National Forest'), ('type', 'location'), ('tail', 'Idaho')), (('head', 'Payette National Forest'), ('type', 'headquarters location'), ('tail', 'Idaho'))}\n",
      "{(('head', 'Kim Dae-jung'), ('type', 'employer'), ('tail', 'leading opposition party')), (('head', 'leading opposition party'), ('type', 'employer'), ('tail', 'Kim Dae-jung'))}\n",
      "{(('head', 'Klong Toey'), ('type', 'headquarters location'), ('tail', 'Bangkok')), (('head', 'Klong Toey'), ('type', 'employer'), ('tail', 'slum'))}\n",
      "{(('head', 'Dealey Plaza'), ('type', 'headquarters location'), ('tail', 'Dallas')), (('head', 'assassinated Kennedy'), ('type', 'employer'), ('tail', 'Lee Harvey Oswald')), (('head', 'Texas School Book Depository'), ('type', 'headquarters location'), ('tail', 'Dallas')), (('head', 'assassinated Kennedy'), ('type', 'location'), ('tail', 'Texas School Book Depository')), (('head', 'Lee Harvey Oswald'), ('type', 'killed by'), ('tail', 'assassinated Kennedy'))}\n",
      "{(('head', 'BONE'), ('type', 'headquarters location'), ('tail', 'Idaho')), (('head', 'AP'), ('type', 'headquarters location'), ('tail', 'Idaho'))}\n",
      "{(('head', 'Irving Flax'), ('type', 'killed by'), ('tail', 'Fort Lee')), (('head', 'Therese Afdahl'), ('type', 'location'), ('tail', 'John Martini')), (('head', 'John Martini'), ('type', 'location'), ('tail', 'Therese Afdahl'))}\n",
      "{(('head', 'Public Utilities Commission'), ('type', 'employer'), ('tail', 'Bernice McIntyre'))}\n",
      "{(('head', 'James Garfield'), ('type', 'employer'), ('tail', 'President of the United States')), (('head', 'James Garfield'), ('type', 'residence'), ('tail', 'Orange, Ohio'))}\n",
      "{(('head', 'Martin Luther King'), ('type', 'killed by'), ('tail', 'assassination')), (('head', 'Martin Luther King'), ('type', 'killed by'), ('tail', 'Memphis')), (('head', 'assassination'), ('type', 'killed by'), ('tail', 'Martin Luther King'))}\n",
      "{(('head', 'Widen Ban on Possession of Arms'), ('type', 'location'), ('tail', '3 Feb 94')), (('head', 'Paris'), ('type', 'employer'), ('tail', 'English'))}\n",
      "set()\n",
      "{(('head', 'Charles Lindbergh'), ('type', 'location'), ('tail', 'Anne Spencer Morrow')), (('head', 'Anne Spencer Morrow'), ('type', 'location'), ('tail', 'Charles Lindbergh'))}\n",
      "{(('head', 'Gerald Baliles'), ('type', 'employer'), ('tail', 'Democrat'))}\n",
      "{(('head', 'Beirut'), ('type', 'employer'), ('tail', 'Middle East'))}\n",
      "{(('head', 'Institute for Nuclear and Energy Research'), ('type', 'headquarters location'), ('tail', 'National Nuclear Energy Commission')), (('head', 'Institute for Nuclear and Energy Research'), ('type', 'headquarters location'), ('tail', 'Sao Paulo')), (('head', 'National Nuclear Energy Commission'), ('type', 'headquarters location'), ('tail', 'Sao Paulo')), (('head', 'National Nuclear Energy Commission'), ('type', 'employer'), ('tail', 'Institute for Nuclear and Energy Research'))}\n",
      "{(('head', 'North Korea'), ('type', 'location'), ('tail', 'Australia')), (('head', 'Australia'), ('type', 'employer'), ('tail', 'Australian Government')), (('head', 'Australia'), ('type', 'location'), ('tail', 'North Korea'))}\n",
      "{(('head', 'Sofia'), ('type', 'location'), ('tail', 'Bulgaria')), (('head', 'Bulgaria'), ('type', 'location'), ('tail', 'Sofia'))}\n",
      "{(('head', 'Budapest'), ('type', 'location'), ('tail', 'Danube')), (('head', 'Bratislava'), ('type', 'location'), ('tail', 'Gabcikovo')), (('head', 'Bratislava'), ('type', 'location'), ('tail', 'Danube')), (('head', 'Gabcikovo'), ('type', 'location'), ('tail', 'Danube'))}\n",
      "{(('head', 'Sirhan B. Sirhan'), ('type', 'employer'), ('tail', '22 years ago')), (('head', 'Sirhan B. Sirhan'), ('type', 'killed by'), ('tail', 'assassinated Sen. Robert F. Kennedy')), (('head', '22 years ago'), ('type', 'employer'), ('tail', 'Sirhan B. Sirhan'))}\n",
      "{(('head', 'Jeff Widener'), ('type', 'employer'), ('tail', 'Photographer')), (('head', 'Thailand'), ('type', 'headquarters location'), ('tail', 'Bangkok')), (('head', 'Bangkok'), ('type', 'location'), ('tail', 'Thailand'))}\n",
      "{(('head', 'Port Arthur'), ('type', 'location'), ('tail', 'Sabine Pass')), (('head', 'Sabine Pass'), ('type', 'location'), ('tail', 'Port Arthur')), (('head', 'Malcolm Grant'), ('type', 'residence'), ('tail', 'Port Arthur')), (('head', 'Port Arthur'), ('type', 'headquarters location'), ('tail', 'Louisiana'))}\n",
      "{(('head', 'Dwight D. Eisenhower'), ('type', 'residence'), ('tail', 'Denison, Tex.'))}\n",
      "{(('head', 'Lee Harvey Oswald'), ('type', 'killed by'), ('tail', 'assassinated')), (('head', 'President Kennedy'), ('type', 'killed by'), ('tail', 'Lee Harvey Oswald')), (('head', 'assassinated'), ('type', 'killed by'), ('tail', 'Lee Harvey Oswald')), (('head', 'assassinated'), ('type', 'location'), ('tail', 'President Kennedy')), (('head', 'Lee Harvey Oswald'), ('type', 'killed by'), ('tail', 'Dallas'))}\n",
      "{(('head', 'Albert O. Harjula'), ('type', 'residence'), ('tail', 'Thomaston'))}\n",
      "{(('head', 'Namialo'), ('type', 'headquarters location'), ('tail', 'Nampula Province')), (('head', 'Nampula Province'), ('type', 'headquarters location'), ('tail', 'Namialo'))}\n",
      "{(('head', 'Pat Epps'), ('type', 'employer'), ('tail', 'Greenland Expedition Society'))}\n",
      "{(('head', 'Pemex'), ('type', 'employer'), ('tail', 'Francisco Rojas')), (('head', 'Pemex'), ('type', 'employer'), ('tail', 'crude oil'))}\n",
      "{(('head', 'Kleber Elias Gia Bustamante'), ('type', 'employer'), ('tail', 'Red Sun')), (('head', 'Jose Antonio Briz Lopez'), ('type', 'killed by'), ('tail', 'Kleber Elias Gia Bustamante'))}\n",
      "set()\n",
      "{(('head', 'An art exhibit at the Hakawati Theatre'), ('type', 'employer'), ('tail', 'art exhibit')), (('head', 'Hakawati Theatre'), ('type', 'headquarters location'), ('tail', 'Jerusalem'))}\n",
      "{(('head', 'Carlos Tenev'), ('type', 'employer'), ('tail', 'Peronist Party'))}\n",
      "{(('head', 'Dwight D. Eisenhower'), ('type', 'employer'), ('tail', 'president of the United States')), (('head', 'Dwight D. Eisenhower'), ('type', 'residence'), ('tail', 'Denison, Texas'))}\n",
      "{(('head', 'Universal Declaration of Human Rights'), ('type', 'headquarters location'), ('tail', 'New Jersey')), (('head', 'Bruce Springsteen'), ('type', 'residence'), ('tail', 'New Jersey'))}\n",
      "{(('head', 'Indiana'), ('type', 'location'), ('tail', 'Indianapolis')), (('head', 'Indianapolis'), ('type', 'location'), ('tail', 'Indiana'))}\n",
      "{(('head', 'Jesse Jackson'), ('type', 'employer'), ('tail', 'Democratic'))}\n",
      "{(('head', \"Robert F. Kennedy's assassin\"), ('type', 'killed by'), ('tail', 'Sirhan Sirhan')), (('head', 'trial'), ('type', 'employer'), ('tail', 'Sirhan Sirhan'))}\n",
      "{(('head', 'James Garfield'), ('type', 'employer'), ('tail', 'president of the United States')), (('head', 'James Garfield'), ('type', 'residence'), ('tail', 'Orange, Ohio'))}\n",
      "{(('head', 'HINDU'), ('type', 'killed by'), ('tail', 'Madras')), (('head', 'The HINDU'), ('type', 'headquarters location'), ('tail', 'Madras'))}\n",
      "{(('head', 'Gerald Steinberg'), ('type', 'employer'), ('tail', 'Bar-Ilan University'))}\n",
      "{(('head', 'Jean Paul Marat'), ('type', 'killed by'), ('tail', 'Charlotte Corday')), (('head', 'Jean Paul Marat'), ('type', 'killed by'), ('tail', '1793'))}\n",
      "{(('head', 'China'), ('type', 'location'), ('tail', 'Beijing')), (('head', 'Vietnam'), ('type', 'location'), ('tail', 'Cambodia')), (('head', 'Cambodia'), ('type', 'location'), ('tail', 'Vietnam'))}\n",
      "{(('head', 'Jordi Aguilà'), ('type', 'employer'), ('tail', 'CNM')), (('head', 'neutron bombardment'), ('type', 'has effect'), ('tail', 'microperforation'))}\n",
      "{(('head', 'Idaho'), ('type', 'location'), ('tail', 'Boise'))}\n",
      "{(('head', 'Lebanon'), ('type', 'location'), ('tail', 'BEIRUT')), (('head', 'Beirut'), ('type', 'location'), ('tail', 'Lebanon')), (('head', 'Lebanon'), ('type', 'location'), ('tail', 'Beirut'))}\n",
      "{(('head', 'LA JORNADA'), ('type', 'employer'), ('tail', 'Mexico City')), (('head', 'Mexico City'), ('type', 'employer'), ('tail', 'LA JORNADA')), (('head', 'LA JORNADA'), ('type', 'headquarters location'), ('tail', 'Mexico City'))}\n",
      "{(('head', 'assassinated'), ('type', 'location'), ('tail', 'Martin Luther King Jr.')), (('head', 'Nixon'), ('type', 'employer'), ('tail', '1972 campaign')), (('head', '1972 campaign'), ('type', 'employer'), ('tail', 'Nixon')), (('head', 'James Earl Ray'), ('type', 'killed by'), ('tail', 'assassinated'))}\n",
      "{(('head', 'Mombasa'), ('type', 'location'), ('tail', 'Kenya')), (('head', 'Kenya'), ('type', 'headquarters location'), ('tail', 'Mombasa'))}\n",
      "{(('head', 'Jack Ruby'), ('type', 'killed by'), ('tail', 'Dallas')), (('head', 'Lee Harvey Oswald'), ('type', 'killed by'), ('tail', 'Jack Ruby')), (('head', 'Lee Harvey Oswald'), ('type', 'killed by'), ('tail', 'Dallas'))}\n",
      "{(('head', 'SRP'), ('type', 'employer'), ('tail', 'DOE')), (('head', 'DOE'), ('type', 'employer'), ('tail', 'SRP'))}\n",
      "{(('head', 'Disney World'), ('type', 'headquarters location'), ('tail', 'Florida')), (('head', 'Tom Defelice'), ('type', 'residence'), ('tail', 'Warwick'))}\n",
      "{(('head', 'JERUSALEM POST'), ('type', 'location'), ('tail', 'Jerusalem')), (('head', 'The JERUSALEM POST'), ('type', 'location'), ('tail', 'Jerusalem'))}\n",
      "set()\n",
      "{(('head', 'JERUSALEM POST'), ('type', 'location'), ('tail', 'Jerusalem'))}\n",
      "{(('head', 'Gianfranco Fini'), ('type', 'employer'), ('tail', 'National Alliance')), (('head', 'National Alliance'), ('type', 'employer'), ('tail', 'Gianfranco Fini'))}\n",
      "{(('head', 'downed'), ('type', 'location'), ('tail', 'Libya')), (('head', 'Downed the Libyan jets'), ('type', 'employer'), ('tail', 'Gadhafi'))}\n",
      "{(('head', 'University of Virginia'), ('type', 'headquarters location'), ('tail', 'Charlottesville'))}\n",
      "{(('head', 'Ricardo Camero Cardiel'), ('type', 'residence'), ('tail', 'Ciudad Madero')), (('head', 'Notimex'), ('type', 'employer'), ('tail', 'news agency'))}\n",
      "{(('head', 'Warren Commission'), ('type', 'headquarters location'), ('tail', 'Dallas County Administration Building')), (('head', 'Dallas County Administration Building'), ('type', 'residence'), ('tail', 'Oswald'))}\n",
      "set()\n",
      "{(('head', 'Armenia'), ('type', 'location'), ('tail', 'Yerevan')), (('head', 'Armenian'), ('type', 'location'), ('tail', 'Yerevan')), (('head', 'Yerevan'), ('type', 'headquarters location'), ('tail', 'Armenian'))}\n",
      "{(('head', 'Cecil Andrus'), ('type', 'employer'), ('tail', 'Idaho Gov.'))}\n",
      "{(('head', 'Venezuela'), ('type', 'employer'), ('tail', 'GATT'))}\n",
      "{(('head', 'EZLN'), ('type', 'location'), ('tail', 'Mexico')), (('head', 'Respectfully yours'), ('type', 'residence'), ('tail', 'Mexico'))}\n",
      "{(('head', 'Bangkok BANGKOK POST'), ('type', 'employer'), ('tail', 'Bangkok'))}\n",
      "{(('head', 'Jesse Jackson'), ('type', 'residence'), ('tail', 'South Carolina'))}\n",
      "{(('head', 'Sherman Finesilver'), ('type', 'employer'), ('tail', 'U.S. District Judge'))}\n",
      "{(('head', 'Moscow'), ('type', 'location'), ('tail', 'Russia')), (('head', 'Moscow'), ('type', 'employer'), ('tail', 'Russian')), (('head', 'Russia'), ('type', 'headquarters location'), ('tail', 'Moscow'))}\n",
      "{(('head', 'assassination'), ('type', 'location'), ('tail', 'Abraham Lincoln')), (('head', 'Abraham Lincoln'), ('type', 'killed by'), ('tail', 'John Wilkes Booth')), (('head', 'John Wilkes Booth'), ('type', 'location'), ('tail', 'Abraham Lincoln')), (('head', 'assassination of President Abraham Lincoln'), ('type', 'employer'), ('tail', 'John Wilkes Booth')), (('head', 'John Wilkes Booth'), ('type', 'employer'), ('tail', 'assassination of President Abraham Lincoln')), (('head', 'assassination'), ('type', 'employer'), ('tail', 'John Wilkes Booth'))}\n",
      "{(('head', 'Fort Lewis'), ('type', 'headquarters location'), ('tail', 'Wash.')), (('head', 'Wash.'), ('type', 'location'), ('tail', 'Oregon')), (('head', 'Oregon'), ('type', 'location'), ('tail', 'Wash.'))}\n",
      "set()\n",
      "{(('head', 'school'), ('type', 'headquarters location'), ('tail', 'Lake Charles, La.')), (('head', 'La.'), ('type', 'location'), ('tail', 'Lake Charles'))}\n",
      "set()\n",
      "{(('head', 'Shoshone-Bannock reservation'), ('type', 'headquarters location'), ('tail', 'Idaho')), (('head', 'Bannock'), ('type', 'employer'), ('tail', 'Shoshone'))}\n",
      "{(('head', 'South'), ('type', 'residence'), ('tail', 'North Carolina')), (('head', 'North Carolina'), ('type', 'employer'), ('tail', 'South')), (('head', 'North Carolina'), ('type', 'employer'), ('tail', 'the South'))}\n",
      "{(('head', 'John Wilkes Booth'), ('type', 'employer'), ('tail', 'Victor Garber')), (('head', 'Abraham Lincoln'), ('type', 'killed by'), ('tail', 'John Wilkes Booth')), (('head', 'John Wilkes Booth'), ('type', 'location'), ('tail', 'Abraham Lincoln'))}\n",
      "{(('head', 'Pawtuxet River'), ('type', 'headquarters location'), ('tail', 'Rhode Island')), (('head', 'Warwick'), ('type', 'location'), ('tail', 'Pawtuxet River'))}\n",
      "{(('head', 'Algeria'), ('type', 'employer'), ('tail', 'Middle East'))}\n",
      "{(('head', 'James Garfield'), ('type', 'employer'), ('tail', 'President')), (('head', 'James Garfield'), ('type', 'killed by'), ('tail', 'Charles J. Guiteau'))}\n",
      "{(('head', 'Puget Sound'), ('type', 'residence'), ('tail', 'Lake Washington')), (('head', 'Lake Washington'), ('type', 'employer'), ('tail', 'Puget Sound')), (('head', 'Seattle'), ('type', 'location'), ('tail', 'Lake Washington')), (('head', 'Seattle'), ('type', 'location'), ('tail', 'Puget Sound'))}\n",
      "{(('head', 'Watergate'), ('type', 'employer'), ('tail', 'Nixon')), (('head', 'Watergate defense'), ('type', 'employer'), ('tail', 'Nixon'))}\n",
      "{(('head', 'California'), ('type', 'location'), ('tail', 'Sierra Nevada foothills')), (('head', 'Sierra Nevada foothills'), ('type', 'headquarters location'), ('tail', 'California'))}\n",
      "{(('head', 'Washington state'), ('type', 'location'), ('tail', 'Canadian')), (('head', 'Cascade'), ('type', 'headquarters location'), ('tail', 'Washington state')), (('head', 'Canadian'), ('type', 'location'), ('tail', 'Washington state'))}\n",
      "{(('head', 'Ernesto Zedillo Ponce de Leon'), ('type', 'employer'), ('tail', 'PRI'))}\n",
      "{(('head', 'Jesse Jackson'), ('type', 'employer'), ('tail', 'Democratic')), (('head', 'Jesse Jackson'), ('type', 'residence'), ('tail', 'South Carolina'))}\n",
      "{(('head', 'dogsled'), ('type', 'employer'), ('tail', 'dog musher')), (('head', 'dogled'), ('type', 'employer'), ('tail', 'dog musher')), (('head', 'dog musher'), ('type', 'employer'), ('tail', 'dogsled')), (('head', 'dog musher'), ('type', 'employer'), ('tail', 'dogled'))}\n",
      "{(('head', 'Klamath Falls, Ore.'), ('type', 'location'), ('tail', 'EDT')), (('head', 'Yuma, Ariz.'), ('type', 'location'), ('tail', 'EDT')), (('head', 'Klamath Falls'), ('type', 'headquarters location'), ('tail', 'Ore.'))}\n",
      "{(('head', 'Will Rogers'), ('type', 'residence'), ('tail', 'Oklahoma'))}\n",
      "{(('head', 'shot to death'), ('type', 'location'), ('tail', 'Dallas')), (('head', 'Kennedy was shot to death'), ('type', 'headquarters location'), ('tail', 'Dallas'))}\n",
      "{(('head', 'Ellington Air Force Base'), ('type', 'headquarters location'), ('tail', 'Texas')), (('head', 'Houston'), ('type', 'location'), ('tail', 'EDT'))}\n",
      "{(('head', 'Fort Wainwright annex'), ('type', 'headquarters location'), ('tail', 'Fairbanks, Alaska')), (('head', 'Alaska'), ('type', 'headquarters location'), ('tail', 'Fairbanks'))}\n",
      "compare rebel_rel5 with test_triples\n",
      "Micro Scores: {'precision': 0.048214285714285716, 'recall': 0.0743801652892562, 'f1': 0.058504875406283866}\n",
      "Macro Scores: {'precision': 0.07647462277091907, 'recall': 0.09327846364883402, 'f1': 0.08404483795430298}\n",
      "533\n"
     ]
    }
   ],
   "source": [
    "scores = evaluate_predictions_corrected('/Users/ananyahooda/Desktop/final/final_comparable_golden_truth.json',\"/Users/ananyahooda/Desktop/final/final_ensembled_5rel_nostring.json\")\n",
    "print(\"compare rebel_rel5 with test_triples\")\n",
    "print(\"Micro Scores:\", scores['micro'])\n",
    "print(\"Macro Scores:\", scores['macro'])\n",
    "print(scores['extras'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Function to extract triples for each context in eval.json and create pred.json\n",
    "def generate_pred_json(eval_file_path, pred_file_path):\n",
    "    # Load the evaluation data from eval.json\n",
    "    with open(eval_file_path, 'r') as file:\n",
    "        eval_data = json.load(file)\n",
    "    \n",
    "    # Initialize a list to hold the modified data with extracted triples\n",
    "    modified_data = []\n",
    "    \n",
    "    # Iterate over each item in the evaluation data\n",
    "    for item in eval_data:\n",
    "        context = item['context']\n",
    "        # Prepare the command with the context\n",
    "        command = f\"Can you please give entities for \\\"{context}\\\"\"\n",
    "        # Use the process_command function to predict the extracted triples\n",
    "        extracted_triplets = process_command(command)\n",
    "        # Append the extracted triples to the item under the 'triples' key\n",
    "        item['triples'] = extracted_triplets\n",
    "        # Append the modified item to the modified_data list\n",
    "        modified_data.append(item)\n",
    "    \n",
    "    # Write the modified data with extracted triples to pred.json\n",
    "    with open(pred_file_path, 'w') as file:\n",
    "        json.dump(modified_data, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      13.30 ms /    47 runs   (    0.28 ms per token,  3533.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8577.22 ms /    38 tokens (  225.72 ms per token,     4.43 tokens per second)\n",
      "llama_print_timings:        eval time =    3761.78 ms /    46 runs   (   81.78 ms per token,    12.23 tokens per second)\n",
      "llama_print_timings:       total time =   12725.52 ms /    84 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =       5.85 ms /    24 runs   (    0.24 ms per token,  4104.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =     404.45 ms /    13 tokens (   31.11 ms per token,    32.14 tokens per second)\n",
      "llama_print_timings:        eval time =    1889.79 ms /    23 runs   (   82.16 ms per token,    12.17 tokens per second)\n",
      "llama_print_timings:       total time =    2423.84 ms /    36 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      22.42 ms /   134 runs   (    0.17 ms per token,  5977.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =     875.33 ms /    67 tokens (   13.06 ms per token,    76.54 tokens per second)\n",
      "llama_print_timings:        eval time =   12260.74 ms /   133 runs   (   92.19 ms per token,    10.85 tokens per second)\n",
      "llama_print_timings:       total time =   13737.56 ms /   200 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      18.66 ms /   112 runs   (    0.17 ms per token,  6002.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =     701.33 ms /    61 tokens (   11.50 ms per token,    86.98 tokens per second)\n",
      "llama_print_timings:        eval time =    9952.55 ms /   111 runs   (   89.66 ms per token,    11.15 tokens per second)\n",
      "llama_print_timings:       total time =   11042.74 ms /   172 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      33.34 ms /   172 runs   (    0.19 ms per token,  5158.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1946.08 ms /    76 tokens (   25.61 ms per token,    39.05 tokens per second)\n",
      "llama_print_timings:        eval time =   15198.26 ms /   171 runs   (   88.88 ms per token,    11.25 tokens per second)\n",
      "llama_print_timings:       total time =   17879.54 ms /   247 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      17.31 ms /   108 runs   (    0.16 ms per token,  6238.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =     683.63 ms /    53 tokens (   12.90 ms per token,    77.53 tokens per second)\n",
      "llama_print_timings:        eval time =    9450.50 ms /   107 runs   (   88.32 ms per token,    11.32 tokens per second)\n",
      "llama_print_timings:       total time =   10439.04 ms /   160 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      16.47 ms /    68 runs   (    0.24 ms per token,  4129.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =     623.39 ms /    56 tokens (   11.13 ms per token,    89.83 tokens per second)\n",
      "llama_print_timings:        eval time =    5749.18 ms /    67 runs   (   85.81 ms per token,    11.65 tokens per second)\n",
      "llama_print_timings:       total time =    6675.05 ms /   123 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =       8.45 ms /    52 runs   (    0.16 ms per token,  6156.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =     653.45 ms /    39 tokens (   16.76 ms per token,    59.68 tokens per second)\n",
      "llama_print_timings:        eval time =    4623.30 ms /    51 runs   (   90.65 ms per token,    11.03 tokens per second)\n",
      "llama_print_timings:       total time =    5440.42 ms /    90 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =       9.90 ms /    53 runs   (    0.19 ms per token,  5355.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =     598.93 ms /    45 tokens (   13.31 ms per token,    75.13 tokens per second)\n",
      "llama_print_timings:        eval time =    4399.72 ms /    52 runs   (   84.61 ms per token,    11.82 tokens per second)\n",
      "llama_print_timings:       total time =    5161.05 ms /    97 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      19.42 ms /   104 runs   (    0.19 ms per token,  5355.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =     583.28 ms /    36 tokens (   16.20 ms per token,    61.72 tokens per second)\n",
      "llama_print_timings:        eval time =    8939.38 ms /   103 runs   (   86.79 ms per token,    11.52 tokens per second)\n",
      "llama_print_timings:       total time =    9867.87 ms /   139 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =       2.86 ms /    24 runs   (    0.12 ms per token,  8391.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =     313.12 ms /    13 tokens (   24.09 ms per token,    41.52 tokens per second)\n",
      "llama_print_timings:        eval time =    2190.61 ms /    23 runs   (   95.24 ms per token,    10.50 tokens per second)\n",
      "llama_print_timings:       total time =    2652.13 ms /    36 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =       4.14 ms /    23 runs   (    0.18 ms per token,  5552.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =     317.89 ms /    13 tokens (   24.45 ms per token,    40.90 tokens per second)\n",
      "llama_print_timings:        eval time =    1887.27 ms /    22 runs   (   85.79 ms per token,    11.66 tokens per second)\n",
      "llama_print_timings:       total time =    2292.61 ms /    35 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =       3.25 ms /    24 runs   (    0.14 ms per token,  7373.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =     313.80 ms /    13 tokens (   24.14 ms per token,    41.43 tokens per second)\n",
      "llama_print_timings:        eval time =    1981.24 ms /    23 runs   (   86.14 ms per token,    11.61 tokens per second)\n",
      "llama_print_timings:       total time =    2348.11 ms /    36 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      13.16 ms /    81 runs   (    0.16 ms per token,  6156.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =     323.53 ms /    30 tokens (   10.78 ms per token,    92.73 tokens per second)\n",
      "llama_print_timings:        eval time =    7609.24 ms /    80 runs   (   95.12 ms per token,    10.51 tokens per second)\n",
      "llama_print_timings:       total time =    8178.49 ms /   110 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =       3.89 ms /    27 runs   (    0.14 ms per token,  6942.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =     311.17 ms /    15 tokens (   20.74 ms per token,    48.20 tokens per second)\n",
      "llama_print_timings:        eval time =    2439.72 ms /    26 runs   (   93.84 ms per token,    10.66 tokens per second)\n",
      "llama_print_timings:       total time =    2828.28 ms /    41 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =       5.59 ms /    23 runs   (    0.24 ms per token,  4115.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =     316.62 ms /    11 tokens (   28.78 ms per token,    34.74 tokens per second)\n",
      "llama_print_timings:        eval time =    1818.87 ms /    22 runs   (   82.68 ms per token,    12.10 tokens per second)\n",
      "llama_print_timings:       total time =    2246.72 ms /    33 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      13.80 ms /    78 runs   (    0.18 ms per token,  5650.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =     862.42 ms /    66 tokens (   13.07 ms per token,    76.53 tokens per second)\n",
      "llama_print_timings:        eval time =    7137.72 ms /    77 runs   (   92.70 ms per token,    10.79 tokens per second)\n",
      "llama_print_timings:       total time =    8273.73 ms /   143 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      21.02 ms /   104 runs   (    0.20 ms per token,  4947.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =     597.69 ms /    49 tokens (   12.20 ms per token,    81.98 tokens per second)\n",
      "llama_print_timings:        eval time =    9364.77 ms /   103 runs   (   90.92 ms per token,    11.00 tokens per second)\n",
      "llama_print_timings:       total time =   10393.54 ms /   152 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      20.88 ms /    97 runs   (    0.22 ms per token,  4644.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =     325.10 ms /    30 tokens (   10.84 ms per token,    92.28 tokens per second)\n",
      "llama_print_timings:        eval time =    8317.38 ms /    96 runs   (   86.64 ms per token,    11.54 tokens per second)\n",
      "llama_print_timings:       total time =    9031.53 ms /   126 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra data: line 3 column 1 (char 143)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =       6.69 ms /    24 runs   (    0.28 ms per token,  3587.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =     310.62 ms /    13 tokens (   23.89 ms per token,    41.85 tokens per second)\n",
      "llama_print_timings:        eval time =    1882.11 ms /    23 runs   (   81.83 ms per token,    12.22 tokens per second)\n",
      "llama_print_timings:       total time =    2321.05 ms /    36 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      32.43 ms /   120 runs   (    0.27 ms per token,  3700.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =     585.37 ms /    39 tokens (   15.01 ms per token,    66.62 tokens per second)\n",
      "llama_print_timings:        eval time =    9824.86 ms /   119 runs   (   82.56 ms per token,    12.11 tokens per second)\n",
      "llama_print_timings:       total time =   11023.86 ms /   158 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      19.18 ms /    91 runs   (    0.21 ms per token,  4745.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =     331.98 ms /    31 tokens (   10.71 ms per token,    93.38 tokens per second)\n",
      "llama_print_timings:        eval time =    7773.89 ms /    90 runs   (   86.38 ms per token,    11.58 tokens per second)\n",
      "llama_print_timings:       total time =    8462.38 ms /   121 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      16.75 ms /   100 runs   (    0.17 ms per token,  5971.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =     606.98 ms /    39 tokens (   15.56 ms per token,    64.25 tokens per second)\n",
      "llama_print_timings:        eval time =    8906.52 ms /    99 runs   (   89.96 ms per token,    11.12 tokens per second)\n",
      "llama_print_timings:       total time =    9836.71 ms /   138 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =       9.56 ms /    52 runs   (    0.18 ms per token,  5438.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =     602.86 ms /    42 tokens (   14.35 ms per token,    69.67 tokens per second)\n",
      "llama_print_timings:        eval time =    4437.92 ms /    51 runs   (   87.02 ms per token,    11.49 tokens per second)\n",
      "llama_print_timings:       total time =    5212.24 ms /    93 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      10.34 ms /    51 runs   (    0.20 ms per token,  4931.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =     585.82 ms /    39 tokens (   15.02 ms per token,    66.57 tokens per second)\n",
      "llama_print_timings:        eval time =    4401.35 ms /    50 runs   (   88.03 ms per token,    11.36 tokens per second)\n",
      "llama_print_timings:       total time =    5167.74 ms /    89 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      27.02 ms /   137 runs   (    0.20 ms per token,  5070.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =     606.30 ms /    47 tokens (   12.90 ms per token,    77.52 tokens per second)\n",
      "llama_print_timings:        eval time =   11832.98 ms /   136 runs   (   87.01 ms per token,    11.49 tokens per second)\n",
      "llama_print_timings:       total time =   12955.05 ms /   183 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      21.74 ms /   105 runs   (    0.21 ms per token,  4829.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =     577.29 ms /    35 tokens (   16.49 ms per token,    60.63 tokens per second)\n",
      "llama_print_timings:        eval time =    9079.61 ms /   104 runs   (   87.30 ms per token,    11.45 tokens per second)\n",
      "llama_print_timings:       total time =   10204.19 ms /   139 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      31.26 ms /   150 runs   (    0.21 ms per token,  4798.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =     590.82 ms /    49 tokens (   12.06 ms per token,    82.94 tokens per second)\n",
      "llama_print_timings:        eval time =   12748.66 ms /   149 runs   (   85.56 ms per token,    11.69 tokens per second)\n",
      "llama_print_timings:       total time =   13899.14 ms /   198 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      20.86 ms /    79 runs   (    0.26 ms per token,  3787.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =     592.29 ms /    42 tokens (   14.10 ms per token,    70.91 tokens per second)\n",
      "llama_print_timings:        eval time =    6728.38 ms /    78 runs   (   86.26 ms per token,    11.59 tokens per second)\n",
      "llama_print_timings:       total time =    7735.29 ms /   120 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      13.29 ms /    49 runs   (    0.27 ms per token,  3686.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =     581.38 ms /    37 tokens (   15.71 ms per token,    63.64 tokens per second)\n",
      "llama_print_timings:        eval time =    3923.69 ms /    48 runs   (   81.74 ms per token,    12.23 tokens per second)\n",
      "llama_print_timings:       total time =    4750.80 ms /    85 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      31.55 ms /   106 runs   (    0.30 ms per token,  3359.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =     585.91 ms /    42 tokens (   13.95 ms per token,    71.68 tokens per second)\n",
      "llama_print_timings:        eval time =    8524.10 ms /   105 runs   (   81.18 ms per token,    12.32 tokens per second)\n",
      "llama_print_timings:       total time =    9687.38 ms /   147 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      19.83 ms /    70 runs   (    0.28 ms per token,  3530.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =     575.53 ms /    34 tokens (   16.93 ms per token,    59.08 tokens per second)\n",
      "llama_print_timings:        eval time =    5614.34 ms /    69 runs   (   81.37 ms per token,    12.29 tokens per second)\n",
      "llama_print_timings:       total time =    6542.01 ms /   103 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      29.83 ms /   101 runs   (    0.30 ms per token,  3386.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =     586.55 ms /    46 tokens (   12.75 ms per token,    78.42 tokens per second)\n",
      "llama_print_timings:        eval time =    8124.67 ms /   100 runs   (   81.25 ms per token,    12.31 tokens per second)\n",
      "llama_print_timings:       total time =    9237.98 ms /   146 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      21.88 ms /    77 runs   (    0.28 ms per token,  3519.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =     862.84 ms /    65 tokens (   13.27 ms per token,    75.33 tokens per second)\n",
      "llama_print_timings:        eval time =    6284.68 ms /    76 runs   (   82.69 ms per token,    12.09 tokens per second)\n",
      "llama_print_timings:       total time =    7578.25 ms /   141 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      34.62 ms /   119 runs   (    0.29 ms per token,  3436.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =     660.70 ms /    42 tokens (   15.73 ms per token,    63.57 tokens per second)\n",
      "llama_print_timings:        eval time =    9592.95 ms /   118 runs   (   81.30 ms per token,    12.30 tokens per second)\n",
      "llama_print_timings:       total time =   10882.57 ms /   160 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      42.99 ms /   144 runs   (    0.30 ms per token,  3349.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =     864.97 ms /    66 tokens (   13.11 ms per token,    76.30 tokens per second)\n",
      "llama_print_timings:        eval time =   11743.47 ms /   143 runs   (   82.12 ms per token,    12.18 tokens per second)\n",
      "llama_print_timings:       total time =   13403.42 ms /   209 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      15.69 ms /    57 runs   (    0.28 ms per token,  3631.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =     585.45 ms /    45 tokens (   13.01 ms per token,    76.86 tokens per second)\n",
      "llama_print_timings:        eval time =    4557.20 ms /    56 runs   (   81.38 ms per token,    12.29 tokens per second)\n",
      "llama_print_timings:       total time =    5422.01 ms /   101 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      31.79 ms /   111 runs   (    0.29 ms per token,  3492.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =     586.23 ms /    46 tokens (   12.74 ms per token,    78.47 tokens per second)\n",
      "llama_print_timings:        eval time =    9012.65 ms /   110 runs   (   81.93 ms per token,    12.21 tokens per second)\n",
      "llama_print_timings:       total time =   10190.92 ms /   156 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      51.66 ms /   180 runs   (    0.29 ms per token,  3484.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =     893.78 ms /    91 tokens (    9.82 ms per token,   101.81 tokens per second)\n",
      "llama_print_timings:        eval time =   14910.70 ms /   179 runs   (   83.30 ms per token,    12.00 tokens per second)\n",
      "llama_print_timings:       total time =   16843.72 ms /   270 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =       5.66 ms /    25 runs   (    0.23 ms per token,  4416.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =     393.84 ms /    13 tokens (   30.30 ms per token,    33.01 tokens per second)\n",
      "llama_print_timings:        eval time =    1978.66 ms /    24 runs   (   82.44 ms per token,    12.13 tokens per second)\n",
      "llama_print_timings:       total time =    2479.75 ms /    37 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =       8.70 ms /    26 runs   (    0.33 ms per token,  2987.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =     312.00 ms /    15 tokens (   20.80 ms per token,    48.08 tokens per second)\n",
      "llama_print_timings:        eval time =    2040.72 ms /    25 runs   (   81.63 ms per token,    12.25 tokens per second)\n",
      "llama_print_timings:       total time =    2493.65 ms /    40 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      14.58 ms /    58 runs   (    0.25 ms per token,  3978.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =     584.35 ms /    46 tokens (   12.70 ms per token,    78.72 tokens per second)\n",
      "llama_print_timings:        eval time =    4643.57 ms /    57 runs   (   81.47 ms per token,    12.28 tokens per second)\n",
      "llama_print_timings:       total time =    5497.46 ms /   103 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      24.29 ms /    86 runs   (    0.28 ms per token,  3540.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =     592.53 ms /    51 tokens (   11.62 ms per token,    86.07 tokens per second)\n",
      "llama_print_timings:        eval time =    6955.42 ms /    85 runs   (   81.83 ms per token,    12.22 tokens per second)\n",
      "llama_print_timings:       total time =    7987.15 ms /   136 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      30.32 ms /   119 runs   (    0.25 ms per token,  3924.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =     607.37 ms /    59 tokens (   10.29 ms per token,    97.14 tokens per second)\n",
      "llama_print_timings:        eval time =   10093.74 ms /   118 runs   (   85.54 ms per token,    11.69 tokens per second)\n",
      "llama_print_timings:       total time =   11283.37 ms /   177 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      26.58 ms /   108 runs   (    0.25 ms per token,  4063.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =     678.97 ms /    54 tokens (   12.57 ms per token,    79.53 tokens per second)\n",
      "llama_print_timings:        eval time =    9168.37 ms /   107 runs   (   85.69 ms per token,    11.67 tokens per second)\n",
      "llama_print_timings:       total time =   10370.21 ms /   161 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      31.95 ms /   104 runs   (    0.31 ms per token,  3255.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =     597.99 ms /    48 tokens (   12.46 ms per token,    80.27 tokens per second)\n",
      "llama_print_timings:        eval time =    8523.79 ms /   103 runs   (   82.76 ms per token,    12.08 tokens per second)\n",
      "llama_print_timings:       total time =    9759.13 ms /   151 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      40.82 ms /   132 runs   (    0.31 ms per token,  3234.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =     608.38 ms /    55 tokens (   11.06 ms per token,    90.40 tokens per second)\n",
      "llama_print_timings:        eval time =   10700.08 ms /   131 runs   (   81.68 ms per token,    12.24 tokens per second)\n",
      "llama_print_timings:       total time =   12135.75 ms /   186 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      41.53 ms /   147 runs   (    0.28 ms per token,  3539.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =     582.06 ms /    40 tokens (   14.55 ms per token,    68.72 tokens per second)\n",
      "llama_print_timings:        eval time =   12071.24 ms /   146 runs   (   82.68 ms per token,    12.09 tokens per second)\n",
      "llama_print_timings:       total time =   13507.33 ms /   186 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      39.41 ms /   106 runs   (    0.37 ms per token,  2690.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =     587.73 ms /    45 tokens (   13.06 ms per token,    76.57 tokens per second)\n",
      "llama_print_timings:        eval time =    8319.03 ms /   105 runs   (   79.23 ms per token,    12.62 tokens per second)\n",
      "llama_print_timings:       total time =    9743.24 ms /   150 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      50.36 ms /   133 runs   (    0.38 ms per token,  2640.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =     591.06 ms /    51 tokens (   11.59 ms per token,    86.29 tokens per second)\n",
      "llama_print_timings:        eval time =   10480.62 ms /   132 runs   (   79.40 ms per token,    12.59 tokens per second)\n",
      "llama_print_timings:       total time =   12134.46 ms /   183 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      63.10 ms /   170 runs   (    0.37 ms per token,  2693.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =     885.56 ms /    88 tokens (   10.06 ms per token,    99.37 tokens per second)\n",
      "llama_print_timings:        eval time =   13587.96 ms /   169 runs   (   80.40 ms per token,    12.44 tokens per second)\n",
      "llama_print_timings:       total time =   15868.44 ms /   257 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      35.14 ms /    99 runs   (    0.35 ms per token,  2817.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =     654.67 ms /    36 tokens (   18.19 ms per token,    54.99 tokens per second)\n",
      "llama_print_timings:        eval time =    7776.90 ms /    98 runs   (   79.36 ms per token,    12.60 tokens per second)\n",
      "llama_print_timings:       total time =    9181.73 ms /   134 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      34.76 ms /    97 runs   (    0.36 ms per token,  2790.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =     580.63 ms /    37 tokens (   15.69 ms per token,    63.72 tokens per second)\n",
      "llama_print_timings:        eval time =    7748.22 ms /    96 runs   (   80.71 ms per token,    12.39 tokens per second)\n",
      "llama_print_timings:       total time =    9064.39 ms /   133 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      27.10 ms /   103 runs   (    0.26 ms per token,  3800.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =     598.41 ms /    48 tokens (   12.47 ms per token,    80.21 tokens per second)\n",
      "llama_print_timings:        eval time =    8810.49 ms /   102 runs   (   86.38 ms per token,    11.58 tokens per second)\n",
      "llama_print_timings:       total time =    9950.90 ms /   150 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =       8.45 ms /    37 runs   (    0.23 ms per token,  4377.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =     334.86 ms /    29 tokens (   11.55 ms per token,    86.60 tokens per second)\n",
      "llama_print_timings:        eval time =    3082.88 ms /    36 runs   (   85.64 ms per token,    11.68 tokens per second)\n",
      "llama_print_timings:       total time =    3567.59 ms /    65 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      14.08 ms /    52 runs   (    0.27 ms per token,  3692.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =     580.89 ms /    40 tokens (   14.52 ms per token,    68.86 tokens per second)\n",
      "llama_print_timings:        eval time =    4147.20 ms /    51 runs   (   81.32 ms per token,    12.30 tokens per second)\n",
      "llama_print_timings:       total time =    4984.08 ms /    91 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      34.09 ms /    86 runs   (    0.40 ms per token,  2522.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =     576.64 ms /    35 tokens (   16.48 ms per token,    60.70 tokens per second)\n",
      "llama_print_timings:        eval time =    6997.36 ms /    85 runs   (   82.32 ms per token,    12.15 tokens per second)\n",
      "llama_print_timings:       total time =    8039.13 ms /   120 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      28.68 ms /    99 runs   (    0.29 ms per token,  3451.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =     578.15 ms /    35 tokens (   16.52 ms per token,    60.54 tokens per second)\n",
      "llama_print_timings:        eval time =    7939.11 ms /    98 runs   (   81.01 ms per token,    12.34 tokens per second)\n",
      "llama_print_timings:       total time =    9034.29 ms /   133 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      17.13 ms /    60 runs   (    0.29 ms per token,  3502.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =     586.37 ms /    47 tokens (   12.48 ms per token,    80.15 tokens per second)\n",
      "llama_print_timings:        eval time =    4858.19 ms /    59 runs   (   82.34 ms per token,    12.14 tokens per second)\n",
      "llama_print_timings:       total time =    5731.38 ms /   106 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      29.56 ms /   109 runs   (    0.27 ms per token,  3687.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =     687.79 ms /    36 tokens (   19.11 ms per token,    52.34 tokens per second)\n",
      "llama_print_timings:        eval time =    8959.99 ms /   108 runs   (   82.96 ms per token,    12.05 tokens per second)\n",
      "llama_print_timings:       total time =   10201.14 ms /   144 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      38.45 ms /   128 runs   (    0.30 ms per token,  3329.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =     609.88 ms /    57 tokens (   10.70 ms per token,    93.46 tokens per second)\n",
      "llama_print_timings:        eval time =   10367.67 ms /   127 runs   (   81.64 ms per token,    12.25 tokens per second)\n",
      "llama_print_timings:       total time =   11675.85 ms /   184 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      26.29 ms /    90 runs   (    0.29 ms per token,  3423.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =     580.76 ms /    39 tokens (   14.89 ms per token,    67.15 tokens per second)\n",
      "llama_print_timings:        eval time =    7229.65 ms /    89 runs   (   81.23 ms per token,    12.31 tokens per second)\n",
      "llama_print_timings:       total time =    8276.68 ms /   128 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      11.05 ms /    41 runs   (    0.27 ms per token,  3710.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =     324.98 ms /    29 tokens (   11.21 ms per token,    89.24 tokens per second)\n",
      "llama_print_timings:        eval time =    3296.87 ms /    40 runs   (   82.42 ms per token,    12.13 tokens per second)\n",
      "llama_print_timings:       total time =    3814.89 ms /    69 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =       8.00 ms /    26 runs   (    0.31 ms per token,  3250.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =     311.56 ms /    15 tokens (   20.77 ms per token,    48.15 tokens per second)\n",
      "llama_print_timings:        eval time =    2017.40 ms /    25 runs   (   80.70 ms per token,    12.39 tokens per second)\n",
      "llama_print_timings:       total time =    2465.77 ms /    40 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      35.82 ms /   120 runs   (    0.30 ms per token,  3350.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =     590.04 ms /    41 tokens (   14.39 ms per token,    69.49 tokens per second)\n",
      "llama_print_timings:        eval time =    9764.73 ms /   119 runs   (   82.06 ms per token,    12.19 tokens per second)\n",
      "llama_print_timings:       total time =   11080.99 ms /   160 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      28.30 ms /    99 runs   (    0.29 ms per token,  3498.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =     586.07 ms /    45 tokens (   13.02 ms per token,    76.78 tokens per second)\n",
      "llama_print_timings:        eval time =    7966.54 ms /    98 runs   (   81.29 ms per token,    12.30 tokens per second)\n",
      "llama_print_timings:       total time =    9074.01 ms /   143 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      29.09 ms /    96 runs   (    0.30 ms per token,  3299.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =     581.89 ms /    40 tokens (   14.55 ms per token,    68.74 tokens per second)\n",
      "llama_print_timings:        eval time =    7706.23 ms /    95 runs   (   81.12 ms per token,    12.33 tokens per second)\n",
      "llama_print_timings:       total time =    8809.86 ms /   135 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      31.31 ms /   104 runs   (    0.30 ms per token,  3321.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =     580.45 ms /    39 tokens (   14.88 ms per token,    67.19 tokens per second)\n",
      "llama_print_timings:        eval time =    8399.10 ms /   103 runs   (   81.54 ms per token,    12.26 tokens per second)\n",
      "llama_print_timings:       total time =    9550.60 ms /   142 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      34.72 ms /   120 runs   (    0.29 ms per token,  3456.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =     590.50 ms /    52 tokens (   11.36 ms per token,    88.06 tokens per second)\n",
      "llama_print_timings:        eval time =    9695.36 ms /   119 runs   (   81.47 ms per token,    12.27 tokens per second)\n",
      "llama_print_timings:       total time =   10926.05 ms /   171 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =       6.53 ms /    25 runs   (    0.26 ms per token,  3827.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =     310.73 ms /    14 tokens (   22.20 ms per token,    45.05 tokens per second)\n",
      "llama_print_timings:        eval time =    1969.94 ms /    24 runs   (   82.08 ms per token,    12.18 tokens per second)\n",
      "llama_print_timings:       total time =    2397.34 ms /    38 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      15.07 ms /    49 runs   (    0.31 ms per token,  3252.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =     575.91 ms /    37 tokens (   15.57 ms per token,    64.25 tokens per second)\n",
      "llama_print_timings:        eval time =    4507.10 ms /    48 runs   (   93.90 ms per token,    10.65 tokens per second)\n",
      "llama_print_timings:       total time =    5371.47 ms /    85 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     201.48 ms /   302 runs   (    0.67 ms per token,  1498.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1841.30 ms /   173 tokens (   10.64 ms per token,    93.96 tokens per second)\n",
      "llama_print_timings:        eval time =   25169.61 ms /   301 runs   (   83.62 ms per token,    11.96 tokens per second)\n",
      "llama_print_timings:       total time =   30808.70 ms /   474 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      65.43 ms /    86 runs   (    0.76 ms per token,  1314.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =     667.40 ms /    41 tokens (   16.28 ms per token,    61.43 tokens per second)\n",
      "llama_print_timings:        eval time =    6797.38 ms /    85 runs   (   79.97 ms per token,    12.50 tokens per second)\n",
      "llama_print_timings:       total time =    8405.88 ms /   126 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     105.55 ms /   135 runs   (    0.78 ms per token,  1279.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =     597.65 ms /    54 tokens (   11.07 ms per token,    90.35 tokens per second)\n",
      "llama_print_timings:        eval time =   10679.62 ms /   134 runs   (   79.70 ms per token,    12.55 tokens per second)\n",
      "llama_print_timings:       total time =   12770.34 ms /   188 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      74.89 ms /   106 runs   (    0.71 ms per token,  1415.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =     586.68 ms /    47 tokens (   12.48 ms per token,    80.11 tokens per second)\n",
      "llama_print_timings:        eval time =    8677.54 ms /   105 runs   (   82.64 ms per token,    12.10 tokens per second)\n",
      "llama_print_timings:       total time =   10376.91 ms /   152 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      85.59 ms /   116 runs   (    0.74 ms per token,  1355.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =     877.00 ms /    76 tokens (   11.54 ms per token,    86.66 tokens per second)\n",
      "llama_print_timings:        eval time =    9465.67 ms /   115 runs   (   82.31 ms per token,    12.15 tokens per second)\n",
      "llama_print_timings:       total time =   11612.36 ms /   191 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      14.60 ms /    23 runs   (    0.63 ms per token,  1575.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =     375.81 ms /    12 tokens (   31.32 ms per token,    31.93 tokens per second)\n",
      "llama_print_timings:        eval time =    1739.15 ms /    22 runs   (   79.05 ms per token,    12.65 tokens per second)\n",
      "llama_print_timings:       total time =    2330.97 ms /    34 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      25.54 ms /    36 runs   (    0.71 ms per token,  1409.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =     317.66 ms /    25 tokens (   12.71 ms per token,    78.70 tokens per second)\n",
      "llama_print_timings:        eval time =    2765.52 ms /    35 runs   (   79.01 ms per token,    12.66 tokens per second)\n",
      "llama_print_timings:       total time =    3452.81 ms /    60 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      77.36 ms /    95 runs   (    0.81 ms per token,  1227.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =     586.55 ms /    46 tokens (   12.75 ms per token,    78.42 tokens per second)\n",
      "llama_print_timings:        eval time =    7485.51 ms /    94 runs   (   79.63 ms per token,    12.56 tokens per second)\n",
      "llama_print_timings:       total time =    9126.73 ms /   140 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     115.03 ms /   155 runs   (    0.74 ms per token,  1347.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =     589.58 ms /    64 tokens (    9.21 ms per token,   108.55 tokens per second)\n",
      "llama_print_timings:        eval time =   12560.49 ms /   154 runs   (   81.56 ms per token,    12.26 tokens per second)\n",
      "llama_print_timings:       total time =   14759.30 ms /   218 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      69.82 ms /    91 runs   (    0.77 ms per token,  1303.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =     607.30 ms /    63 tokens (    9.64 ms per token,   103.74 tokens per second)\n",
      "llama_print_timings:        eval time =    7240.34 ms /    90 runs   (   80.45 ms per token,    12.43 tokens per second)\n",
      "llama_print_timings:       total time =    8852.95 ms /   153 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid \\escape: line 1 column 171 (char 170)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      76.67 ms /    96 runs   (    0.80 ms per token,  1252.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =     581.17 ms /    45 tokens (   12.91 ms per token,    77.43 tokens per second)\n",
      "llama_print_timings:        eval time =    7560.82 ms /    95 runs   (   79.59 ms per token,    12.56 tokens per second)\n",
      "llama_print_timings:       total time =    9202.96 ms /   140 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      57.01 ms /    76 runs   (    0.75 ms per token,  1333.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =     636.36 ms /    46 tokens (   13.83 ms per token,    72.29 tokens per second)\n",
      "llama_print_timings:        eval time =    6118.08 ms /    75 runs   (   81.57 ms per token,    12.26 tokens per second)\n",
      "llama_print_timings:       total time =    7565.31 ms /   121 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      81.34 ms /   116 runs   (    0.70 ms per token,  1426.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =     646.35 ms /    55 tokens (   11.75 ms per token,    85.09 tokens per second)\n",
      "llama_print_timings:        eval time =   10524.05 ms /   115 runs   (   91.51 ms per token,    10.93 tokens per second)\n",
      "llama_print_timings:       total time =   12389.24 ms /   170 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      71.29 ms /   104 runs   (    0.69 ms per token,  1458.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =     672.31 ms /    56 tokens (   12.01 ms per token,    83.30 tokens per second)\n",
      "llama_print_timings:        eval time =    8480.11 ms /   103 runs   (   82.33 ms per token,    12.15 tokens per second)\n",
      "llama_print_timings:       total time =   10226.89 ms /   159 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      97.12 ms /   131 runs   (    0.74 ms per token,  1348.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =     748.42 ms /    59 tokens (   12.69 ms per token,    78.83 tokens per second)\n",
      "llama_print_timings:        eval time =   10548.99 ms /   130 runs   (   81.15 ms per token,    12.32 tokens per second)\n",
      "llama_print_timings:       total time =   12857.31 ms /   189 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      72.49 ms /    97 runs   (    0.75 ms per token,  1338.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =     667.34 ms /    45 tokens (   14.83 ms per token,    67.43 tokens per second)\n",
      "llama_print_timings:        eval time =    7971.89 ms /    96 runs   (   83.04 ms per token,    12.04 tokens per second)\n",
      "llama_print_timings:       total time =    9685.57 ms /   141 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unterminated string starting at: line 1 column 48 (char 47)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      38.59 ms /    49 runs   (    0.79 ms per token,  1269.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =     575.38 ms /    36 tokens (   15.98 ms per token,    62.57 tokens per second)\n",
      "llama_print_timings:        eval time =    3902.98 ms /    48 runs   (   81.31 ms per token,    12.30 tokens per second)\n",
      "llama_print_timings:       total time =    5001.48 ms /    84 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      86.48 ms /   118 runs   (    0.73 ms per token,  1364.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =     604.90 ms /    62 tokens (    9.76 ms per token,   102.50 tokens per second)\n",
      "llama_print_timings:        eval time =    9803.29 ms /   117 runs   (   83.79 ms per token,    11.93 tokens per second)\n",
      "llama_print_timings:       total time =   11663.10 ms /   179 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      43.81 ms /    59 runs   (    0.74 ms per token,  1346.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =     585.64 ms /    48 tokens (   12.20 ms per token,    81.96 tokens per second)\n",
      "llama_print_timings:        eval time =    5062.01 ms /    58 runs   (   87.28 ms per token,    11.46 tokens per second)\n",
      "llama_print_timings:       total time =    6280.20 ms /   106 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      36.39 ms /    53 runs   (    0.69 ms per token,  1456.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =     585.93 ms /    42 tokens (   13.95 ms per token,    71.68 tokens per second)\n",
      "llama_print_timings:        eval time =    4714.77 ms /    52 runs   (   90.67 ms per token,    11.03 tokens per second)\n",
      "llama_print_timings:       total time =    5840.01 ms /    94 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      53.68 ms /    83 runs   (    0.65 ms per token,  1546.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =     618.58 ms /    43 tokens (   14.39 ms per token,    69.51 tokens per second)\n",
      "llama_print_timings:        eval time =    7422.29 ms /    82 runs   (   90.52 ms per token,    11.05 tokens per second)\n",
      "llama_print_timings:       total time =    8845.50 ms /   125 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expecting ',' delimiter: line 1 column 76 (char 75)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      71.36 ms /   124 runs   (    0.58 ms per token,  1737.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =     610.65 ms /    54 tokens (   11.31 ms per token,    88.43 tokens per second)\n",
      "llama_print_timings:        eval time =   10597.24 ms /   123 runs   (   86.16 ms per token,    11.61 tokens per second)\n",
      "llama_print_timings:       total time =   12370.09 ms /   177 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      99.91 ms /   137 runs   (    0.73 ms per token,  1371.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =     609.98 ms /    61 tokens (   10.00 ms per token,   100.00 tokens per second)\n",
      "llama_print_timings:        eval time =   10960.68 ms /   136 runs   (   80.59 ms per token,    12.41 tokens per second)\n",
      "llama_print_timings:       total time =   13050.91 ms /   197 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      52.08 ms /    84 runs   (    0.62 ms per token,  1612.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =     321.72 ms /    28 tokens (   11.49 ms per token,    87.03 tokens per second)\n",
      "llama_print_timings:        eval time =    7007.56 ms /    83 runs   (   84.43 ms per token,    11.84 tokens per second)\n",
      "llama_print_timings:       total time =    8147.81 ms /   111 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      93.31 ms /   132 runs   (    0.71 ms per token,  1414.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =     617.73 ms /    62 tokens (    9.96 ms per token,   100.37 tokens per second)\n",
      "llama_print_timings:        eval time =   10967.36 ms /   131 runs   (   83.72 ms per token,    11.94 tokens per second)\n",
      "llama_print_timings:       total time =   12892.34 ms /   193 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      54.62 ms /    74 runs   (    0.74 ms per token,  1354.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =     393.43 ms /    31 tokens (   12.69 ms per token,    78.79 tokens per second)\n",
      "llama_print_timings:        eval time =    5866.07 ms /    73 runs   (   80.36 ms per token,    12.44 tokens per second)\n",
      "llama_print_timings:       total time =    7053.21 ms /   104 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      64.85 ms /    89 runs   (    0.73 ms per token,  1372.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =     321.05 ms /    29 tokens (   11.07 ms per token,    90.33 tokens per second)\n",
      "llama_print_timings:        eval time =    7031.34 ms /    88 runs   (   79.90 ms per token,    12.52 tokens per second)\n",
      "llama_print_timings:       total time =    8288.72 ms /   117 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra data: line 3 column 1 (char 139)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      84.03 ms /   115 runs   (    0.73 ms per token,  1368.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =     591.84 ms /    52 tokens (   11.38 ms per token,    87.86 tokens per second)\n",
      "llama_print_timings:        eval time =    9702.13 ms /   114 runs   (   85.11 ms per token,    11.75 tokens per second)\n",
      "llama_print_timings:       total time =   11370.43 ms /   166 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      68.25 ms /   108 runs   (    0.63 ms per token,  1582.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =     671.11 ms /    53 tokens (   12.66 ms per token,    78.97 tokens per second)\n",
      "llama_print_timings:        eval time =    8880.41 ms /   107 runs   (   82.99 ms per token,    12.05 tokens per second)\n",
      "llama_print_timings:       total time =   10610.42 ms /   160 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      63.34 ms /   103 runs   (    0.61 ms per token,  1626.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =     595.75 ms /    45 tokens (   13.24 ms per token,    75.54 tokens per second)\n",
      "llama_print_timings:        eval time =    8684.88 ms /   102 runs   (   85.15 ms per token,    11.74 tokens per second)\n",
      "llama_print_timings:       total time =   10255.08 ms /   147 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      54.59 ms /    83 runs   (    0.66 ms per token,  1520.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =     576.05 ms /    33 tokens (   17.46 ms per token,    57.29 tokens per second)\n",
      "llama_print_timings:        eval time =    6650.74 ms /    82 runs   (   81.11 ms per token,    12.33 tokens per second)\n",
      "llama_print_timings:       total time =    8059.80 ms /   115 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unterminated string starting at: line 1 column 48 (char 47)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      70.39 ms /    84 runs   (    0.84 ms per token,  1193.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =     580.56 ms /    45 tokens (   12.90 ms per token,    77.51 tokens per second)\n",
      "llama_print_timings:        eval time =    6577.63 ms /    83 runs   (   79.25 ms per token,    12.62 tokens per second)\n",
      "llama_print_timings:       total time =    8119.83 ms /   128 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      92.87 ms /   118 runs   (    0.79 ms per token,  1270.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =     604.70 ms /    62 tokens (    9.75 ms per token,   102.53 tokens per second)\n",
      "llama_print_timings:        eval time =    9717.19 ms /   117 runs   (   83.05 ms per token,    12.04 tokens per second)\n",
      "llama_print_timings:       total time =   11646.28 ms /   179 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      82.52 ms /   114 runs   (    0.72 ms per token,  1381.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =     670.11 ms /    52 tokens (   12.89 ms per token,    77.60 tokens per second)\n",
      "llama_print_timings:        eval time =    9155.16 ms /   113 runs   (   81.02 ms per token,    12.34 tokens per second)\n",
      "llama_print_timings:       total time =   11046.95 ms /   165 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      73.53 ms /   106 runs   (    0.69 ms per token,  1441.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =     644.86 ms /    37 tokens (   17.43 ms per token,    57.38 tokens per second)\n",
      "llama_print_timings:        eval time =    8361.73 ms /   105 runs   (   79.64 ms per token,    12.56 tokens per second)\n",
      "llama_print_timings:       total time =   10093.76 ms /   142 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra data: line 3 column 1 (char 174)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      76.57 ms /   112 runs   (    0.68 ms per token,  1462.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =     592.35 ms /    48 tokens (   12.34 ms per token,    81.03 tokens per second)\n",
      "llama_print_timings:        eval time =   10220.54 ms /   111 runs   (   92.08 ms per token,    10.86 tokens per second)\n",
      "llama_print_timings:       total time =   11968.07 ms /   159 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unterminated string starting at: line 1 column 48 (char 47)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     153.58 ms /   224 runs   (    0.69 ms per token,  1458.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =     877.75 ms /    83 tokens (   10.58 ms per token,    94.56 tokens per second)\n",
      "llama_print_timings:        eval time =   19280.29 ms /   223 runs   (   86.46 ms per token,    11.57 tokens per second)\n",
      "llama_print_timings:       total time =   22642.87 ms /   306 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      73.25 ms /   100 runs   (    0.73 ms per token,  1365.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =     685.39 ms /    40 tokens (   17.13 ms per token,    58.36 tokens per second)\n",
      "llama_print_timings:        eval time =    8166.53 ms /    99 runs   (   82.49 ms per token,    12.12 tokens per second)\n",
      "llama_print_timings:       total time =   10054.14 ms /   139 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     100.07 ms /   136 runs   (    0.74 ms per token,  1359.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =     860.04 ms /    65 tokens (   13.23 ms per token,    75.58 tokens per second)\n",
      "llama_print_timings:        eval time =   11347.71 ms /   135 runs   (   84.06 ms per token,    11.90 tokens per second)\n",
      "llama_print_timings:       total time =   13740.38 ms /   200 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      23.12 ms /    36 runs   (    0.64 ms per token,  1557.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =     404.95 ms /    28 tokens (   14.46 ms per token,    69.14 tokens per second)\n",
      "llama_print_timings:        eval time =    3195.62 ms /    35 runs   (   91.30 ms per token,    10.95 tokens per second)\n",
      "llama_print_timings:       total time =    3970.32 ms /    63 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      73.56 ms /    96 runs   (    0.77 ms per token,  1305.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =     579.30 ms /    37 tokens (   15.66 ms per token,    63.87 tokens per second)\n",
      "llama_print_timings:        eval time =    8389.76 ms /    95 runs   (   88.31 ms per token,    11.32 tokens per second)\n",
      "llama_print_timings:       total time =   10008.99 ms /   132 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      93.34 ms /   112 runs   (    0.83 ms per token,  1199.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =     580.60 ms /    42 tokens (   13.82 ms per token,    72.34 tokens per second)\n",
      "llama_print_timings:        eval time =    8812.56 ms /   111 runs   (   79.39 ms per token,    12.60 tokens per second)\n",
      "llama_print_timings:       total time =   10664.75 ms /   153 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      32.15 ms /    56 runs   (    0.57 ms per token,  1741.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =     583.45 ms /    44 tokens (   13.26 ms per token,    75.41 tokens per second)\n",
      "llama_print_timings:        eval time =    4821.00 ms /    55 runs   (   87.65 ms per token,    11.41 tokens per second)\n",
      "llama_print_timings:       total time =    5920.41 ms /    99 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      72.60 ms /   103 runs   (    0.70 ms per token,  1418.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =     585.70 ms /    44 tokens (   13.31 ms per token,    75.12 tokens per second)\n",
      "llama_print_timings:        eval time =    8726.22 ms /   102 runs   (   85.55 ms per token,    11.69 tokens per second)\n",
      "llama_print_timings:       total time =   10426.82 ms /   146 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      30.55 ms /    54 runs   (    0.57 ms per token,  1767.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =     596.53 ms /    42 tokens (   14.20 ms per token,    70.41 tokens per second)\n",
      "llama_print_timings:        eval time =    4390.58 ms /    53 runs   (   82.84 ms per token,    12.07 tokens per second)\n",
      "llama_print_timings:       total time =    5474.90 ms /    95 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      45.95 ms /    85 runs   (    0.54 ms per token,  1850.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =     314.68 ms /    32 tokens (    9.83 ms per token,   101.69 tokens per second)\n",
      "llama_print_timings:        eval time =    7108.68 ms /    84 runs   (   84.63 ms per token,    11.82 tokens per second)\n",
      "llama_print_timings:       total time =    8153.92 ms /   116 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unterminated string starting at: line 1 column 48 (char 47)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      67.68 ms /   122 runs   (    0.55 ms per token,  1802.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =     593.94 ms /    48 tokens (   12.37 ms per token,    80.82 tokens per second)\n",
      "llama_print_timings:        eval time =   10016.29 ms /   121 runs   (   82.78 ms per token,    12.08 tokens per second)\n",
      "llama_print_timings:       total time =   11672.28 ms /   169 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      21.35 ms /    42 runs   (    0.51 ms per token,  1967.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =     329.28 ms /    30 tokens (   10.98 ms per token,    91.11 tokens per second)\n",
      "llama_print_timings:        eval time =    3328.19 ms /    41 runs   (   81.18 ms per token,    12.32 tokens per second)\n",
      "llama_print_timings:       total time =    3999.29 ms /    71 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      56.18 ms /   101 runs   (    0.56 ms per token,  1797.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =     588.02 ms /    38 tokens (   15.47 ms per token,    64.62 tokens per second)\n",
      "llama_print_timings:        eval time =    8194.72 ms /   100 runs   (   81.95 ms per token,    12.20 tokens per second)\n",
      "llama_print_timings:       total time =    9637.87 ms /   138 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      57.00 ms /   104 runs   (    0.55 ms per token,  1824.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =     585.01 ms /    33 tokens (   17.73 ms per token,    56.41 tokens per second)\n",
      "llama_print_timings:        eval time =    8459.41 ms /   103 runs   (   82.13 ms per token,    12.18 tokens per second)\n",
      "llama_print_timings:       total time =    9952.30 ms /   136 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra data: line 3 column 1 (char 161)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      56.60 ms /   107 runs   (    0.53 ms per token,  1890.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =     589.72 ms /    46 tokens (   12.82 ms per token,    78.00 tokens per second)\n",
      "llama_print_timings:        eval time =    8675.27 ms /   106 runs   (   81.84 ms per token,    12.22 tokens per second)\n",
      "llama_print_timings:       total time =   10189.68 ms /   152 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unterminated string starting at: line 1 column 48 (char 47)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      71.44 ms /   115 runs   (    0.62 ms per token,  1609.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =     594.62 ms /    49 tokens (   12.14 ms per token,    82.40 tokens per second)\n",
      "llama_print_timings:        eval time =    9415.11 ms /   114 runs   (   82.59 ms per token,    12.11 tokens per second)\n",
      "llama_print_timings:       total time =   11136.53 ms /   163 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      71.45 ms /   107 runs   (    0.67 ms per token,  1497.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =     587.59 ms /    47 tokens (   12.50 ms per token,    79.99 tokens per second)\n",
      "llama_print_timings:        eval time =    9003.94 ms /   106 runs   (   84.94 ms per token,    11.77 tokens per second)\n",
      "llama_print_timings:       total time =   10787.32 ms /   153 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      59.76 ms /   111 runs   (    0.54 ms per token,  1857.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =     699.99 ms /    44 tokens (   15.91 ms per token,    62.86 tokens per second)\n",
      "llama_print_timings:        eval time =    9429.15 ms /   110 runs   (   85.72 ms per token,    11.67 tokens per second)\n",
      "llama_print_timings:       total time =   11157.16 ms /   154 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      72.23 ms /    96 runs   (    0.75 ms per token,  1329.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =     599.10 ms /    44 tokens (   13.62 ms per token,    73.44 tokens per second)\n",
      "llama_print_timings:        eval time =    7902.44 ms /    95 runs   (   83.18 ms per token,    12.02 tokens per second)\n",
      "llama_print_timings:       total time =    9535.79 ms /   139 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     194.57 ms /   274 runs   (    0.71 ms per token,  1408.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =     607.49 ms /    60 tokens (   10.12 ms per token,    98.77 tokens per second)\n",
      "llama_print_timings:        eval time =   22541.88 ms /   273 runs   (   82.57 ms per token,    12.11 tokens per second)\n",
      "llama_print_timings:       total time =   26310.21 ms /   333 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      59.89 ms /    75 runs   (    0.80 ms per token,  1252.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =     654.92 ms /    37 tokens (   17.70 ms per token,    56.50 tokens per second)\n",
      "llama_print_timings:        eval time =    5943.92 ms /    74 runs   (   80.32 ms per token,    12.45 tokens per second)\n",
      "llama_print_timings:       total time =    7470.86 ms /   111 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      33.21 ms /    48 runs   (    0.69 ms per token,  1445.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =     580.58 ms /    36 tokens (   16.13 ms per token,    62.01 tokens per second)\n",
      "llama_print_timings:        eval time =    3779.07 ms /    47 runs   (   80.41 ms per token,    12.44 tokens per second)\n",
      "llama_print_timings:       total time =    4824.10 ms /    83 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      45.21 ms /    64 runs   (    0.71 ms per token,  1415.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =     598.87 ms /    52 tokens (   11.52 ms per token,    86.83 tokens per second)\n",
      "llama_print_timings:        eval time =    5088.24 ms /    63 runs   (   80.77 ms per token,    12.38 tokens per second)\n",
      "llama_print_timings:       total time =    6356.61 ms /   115 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      99.88 ms /   124 runs   (    0.81 ms per token,  1241.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =     603.98 ms /    58 tokens (   10.41 ms per token,    96.03 tokens per second)\n",
      "llama_print_timings:        eval time =    9920.94 ms /   123 runs   (   80.66 ms per token,    12.40 tokens per second)\n",
      "llama_print_timings:       total time =   11828.28 ms /   181 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      60.87 ms /    82 runs   (    0.74 ms per token,  1347.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =     594.63 ms /    48 tokens (   12.39 ms per token,    80.72 tokens per second)\n",
      "llama_print_timings:        eval time =    6507.11 ms /    81 runs   (   80.33 ms per token,    12.45 tokens per second)\n",
      "llama_print_timings:       total time =    8010.90 ms /   129 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      24.01 ms /    39 runs   (    0.62 ms per token,  1624.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =     322.25 ms /    27 tokens (   11.94 ms per token,    83.79 tokens per second)\n",
      "llama_print_timings:        eval time =    3241.47 ms /    38 runs   (   85.30 ms per token,    11.72 tokens per second)\n",
      "llama_print_timings:       total time =    3949.14 ms /    65 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      69.81 ms /    89 runs   (    0.78 ms per token,  1274.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =     322.57 ms /    27 tokens (   11.95 ms per token,    83.70 tokens per second)\n",
      "llama_print_timings:        eval time =    7053.77 ms /    88 runs   (   80.16 ms per token,    12.48 tokens per second)\n",
      "llama_print_timings:       total time =    8331.75 ms /   115 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      69.57 ms /    89 runs   (    0.78 ms per token,  1279.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =     329.29 ms /    31 tokens (   10.62 ms per token,    94.14 tokens per second)\n",
      "llama_print_timings:        eval time =    7034.49 ms /    88 runs   (   79.94 ms per token,    12.51 tokens per second)\n",
      "llama_print_timings:       total time =    8364.22 ms /   119 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      68.62 ms /    89 runs   (    0.77 ms per token,  1296.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =     580.12 ms /    36 tokens (   16.11 ms per token,    62.06 tokens per second)\n",
      "llama_print_timings:        eval time =    7175.24 ms /    88 runs   (   81.54 ms per token,    12.26 tokens per second)\n",
      "llama_print_timings:       total time =    8681.79 ms /   124 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unterminated string starting at: line 1 column 48 (char 47)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      25.72 ms /    31 runs   (    0.83 ms per token,  1205.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =     309.94 ms /    20 tokens (   15.50 ms per token,    64.53 tokens per second)\n",
      "llama_print_timings:        eval time =    2436.23 ms /    30 runs   (   81.21 ms per token,    12.31 tokens per second)\n",
      "llama_print_timings:       total time =    3115.03 ms /    50 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      44.16 ms /    54 runs   (    0.82 ms per token,  1222.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =     585.52 ms /    43 tokens (   13.62 ms per token,    73.44 tokens per second)\n",
      "llama_print_timings:        eval time =    4199.46 ms /    53 runs   (   79.24 ms per token,    12.62 tokens per second)\n",
      "llama_print_timings:       total time =    5401.18 ms /    96 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      52.37 ms /    70 runs   (    0.75 ms per token,  1336.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =     605.63 ms /    59 tokens (   10.26 ms per token,    97.42 tokens per second)\n",
      "llama_print_timings:        eval time =    5587.09 ms /    69 runs   (   80.97 ms per token,    12.35 tokens per second)\n",
      "llama_print_timings:       total time =    6923.84 ms /   128 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      55.67 ms /    75 runs   (    0.74 ms per token,  1347.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =     309.13 ms /    13 tokens (   23.78 ms per token,    42.05 tokens per second)\n",
      "llama_print_timings:        eval time =    6066.48 ms /    74 runs   (   81.98 ms per token,    12.20 tokens per second)\n",
      "llama_print_timings:       total time =    7235.45 ms /    87 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     134.94 ms /   172 runs   (    0.78 ms per token,  1274.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =     873.16 ms /    76 tokens (   11.49 ms per token,    87.04 tokens per second)\n",
      "llama_print_timings:        eval time =   13779.50 ms /   171 runs   (   80.58 ms per token,    12.41 tokens per second)\n",
      "llama_print_timings:       total time =   16619.99 ms /   247 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      74.45 ms /    91 runs   (    0.82 ms per token,  1222.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =     380.60 ms /    18 tokens (   21.14 ms per token,    47.29 tokens per second)\n",
      "llama_print_timings:        eval time =    7148.60 ms /    90 runs   (   79.43 ms per token,    12.59 tokens per second)\n",
      "llama_print_timings:       total time =    8542.16 ms /   108 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      69.85 ms /    82 runs   (    0.85 ms per token,  1173.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =     866.81 ms /    71 tokens (   12.21 ms per token,    81.91 tokens per second)\n",
      "llama_print_timings:        eval time =    6437.32 ms /    81 runs   (   79.47 ms per token,    12.58 tokens per second)\n",
      "llama_print_timings:       total time =    8229.97 ms /   152 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      60.78 ms /    75 runs   (    0.81 ms per token,  1234.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =     932.10 ms /    65 tokens (   14.34 ms per token,    69.73 tokens per second)\n",
      "llama_print_timings:        eval time =    5900.26 ms /    74 runs   (   79.73 ms per token,    12.54 tokens per second)\n",
      "llama_print_timings:       total time =    7654.81 ms /   139 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     123.89 ms /   156 runs   (    0.79 ms per token,  1259.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =     873.44 ms /    78 tokens (   11.20 ms per token,    89.30 tokens per second)\n",
      "llama_print_timings:        eval time =   12617.53 ms /   155 runs   (   81.40 ms per token,    12.28 tokens per second)\n",
      "llama_print_timings:       total time =   15348.72 ms /   233 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     168.48 ms /   206 runs   (    0.82 ms per token,  1222.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =     656.31 ms /    49 tokens (   13.39 ms per token,    74.66 tokens per second)\n",
      "llama_print_timings:        eval time =   16712.53 ms /   205 runs   (   81.52 ms per token,    12.27 tokens per second)\n",
      "llama_print_timings:       total time =   19832.21 ms /   254 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      18.87 ms /    23 runs   (    0.82 ms per token,  1218.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =     309.38 ms /    12 tokens (   25.78 ms per token,    38.79 tokens per second)\n",
      "llama_print_timings:        eval time =    1753.46 ms /    22 runs   (   79.70 ms per token,    12.55 tokens per second)\n",
      "llama_print_timings:       total time =    2329.35 ms /    34 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      41.22 ms /    56 runs   (    0.74 ms per token,  1358.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =     584.41 ms /    45 tokens (   12.99 ms per token,    77.00 tokens per second)\n",
      "llama_print_timings:        eval time =    4456.62 ms /    55 runs   (   81.03 ms per token,    12.34 tokens per second)\n",
      "llama_print_timings:       total time =    5672.22 ms /   100 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      45.13 ms /    70 runs   (    0.64 ms per token,  1551.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =     607.89 ms /    59 tokens (   10.30 ms per token,    97.06 tokens per second)\n",
      "llama_print_timings:        eval time =    5717.72 ms /    69 runs   (   82.87 ms per token,    12.07 tokens per second)\n",
      "llama_print_timings:       total time =    7040.01 ms /   128 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      71.18 ms /   101 runs   (    0.70 ms per token,  1418.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =     610.02 ms /    56 tokens (   10.89 ms per token,    91.80 tokens per second)\n",
      "llama_print_timings:        eval time =    8108.12 ms /   100 runs   (   81.08 ms per token,    12.33 tokens per second)\n",
      "llama_print_timings:       total time =    9823.81 ms /   156 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      51.27 ms /    71 runs   (    0.72 ms per token,  1384.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =     606.35 ms /    60 tokens (   10.11 ms per token,    98.95 tokens per second)\n",
      "llama_print_timings:        eval time =    5637.74 ms /    70 runs   (   80.54 ms per token,    12.42 tokens per second)\n",
      "llama_print_timings:       total time =    7018.38 ms /   130 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      20.77 ms /    27 runs   (    0.77 ms per token,  1300.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =     313.39 ms /    16 tokens (   19.59 ms per token,    51.05 tokens per second)\n",
      "llama_print_timings:        eval time =    2099.99 ms /    26 runs   (   80.77 ms per token,    12.38 tokens per second)\n",
      "llama_print_timings:       total time =    2696.99 ms /    42 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      55.21 ms /    76 runs   (    0.73 ms per token,  1376.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =     324.85 ms /    30 tokens (   10.83 ms per token,    92.35 tokens per second)\n",
      "llama_print_timings:        eval time =    6025.54 ms /    75 runs   (   80.34 ms per token,    12.45 tokens per second)\n",
      "llama_print_timings:       total time =    7173.10 ms /   105 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      81.48 ms /   117 runs   (    0.70 ms per token,  1435.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =     580.01 ms /    39 tokens (   14.87 ms per token,    67.24 tokens per second)\n",
      "llama_print_timings:        eval time =    9469.67 ms /   116 runs   (   81.64 ms per token,    12.25 tokens per second)\n",
      "llama_print_timings:       total time =   11294.21 ms /   155 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expecting ',' delimiter: line 1 column 50 (char 49)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      27.03 ms /    37 runs   (    0.73 ms per token,  1368.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =     315.98 ms /    26 tokens (   12.15 ms per token,    82.28 tokens per second)\n",
      "llama_print_timings:        eval time =    2935.77 ms /    36 runs   (   81.55 ms per token,    12.26 tokens per second)\n",
      "llama_print_timings:       total time =    3637.65 ms /    62 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      52.84 ms /    78 runs   (    0.68 ms per token,  1476.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =     587.84 ms /    44 tokens (   13.36 ms per token,    74.85 tokens per second)\n",
      "llama_print_timings:        eval time =    6267.08 ms /    77 runs   (   81.39 ms per token,    12.29 tokens per second)\n",
      "llama_print_timings:       total time =    7699.42 ms /   121 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      64.46 ms /   108 runs   (    0.60 ms per token,  1675.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =     577.52 ms /    35 tokens (   16.50 ms per token,    60.60 tokens per second)\n",
      "llama_print_timings:        eval time =    9082.19 ms /   107 runs   (   84.88 ms per token,    11.78 tokens per second)\n",
      "llama_print_timings:       total time =   10631.15 ms /   142 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     100.73 ms /   139 runs   (    0.72 ms per token,  1379.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =     879.27 ms /    76 tokens (   11.57 ms per token,    86.44 tokens per second)\n",
      "llama_print_timings:        eval time =   11200.88 ms /   138 runs   (   81.17 ms per token,    12.32 tokens per second)\n",
      "llama_print_timings:       total time =   13599.48 ms /   214 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      39.61 ms /    57 runs   (    0.69 ms per token,  1439.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =     667.45 ms /    47 tokens (   14.20 ms per token,    70.42 tokens per second)\n",
      "llama_print_timings:        eval time =    4510.53 ms /    56 runs   (   80.55 ms per token,    12.42 tokens per second)\n",
      "llama_print_timings:       total time =    5794.88 ms /   103 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     120.76 ms /   167 runs   (    0.72 ms per token,  1382.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =     878.93 ms /    82 tokens (   10.72 ms per token,    93.30 tokens per second)\n",
      "llama_print_timings:        eval time =   13577.90 ms /   166 runs   (   81.79 ms per token,    12.23 tokens per second)\n",
      "llama_print_timings:       total time =   16244.43 ms /   248 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      40.02 ms /    58 runs   (    0.69 ms per token,  1449.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =     387.63 ms /    22 tokens (   17.62 ms per token,    56.76 tokens per second)\n",
      "llama_print_timings:        eval time =    4621.59 ms /    57 runs   (   81.08 ms per token,    12.33 tokens per second)\n",
      "llama_print_timings:       total time =    5623.19 ms /    79 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      39.54 ms /    66 runs   (    0.60 ms per token,  1669.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =     603.68 ms /    56 tokens (   10.78 ms per token,    92.76 tokens per second)\n",
      "llama_print_timings:        eval time =    5366.65 ms /    65 runs   (   82.56 ms per token,    12.11 tokens per second)\n",
      "llama_print_timings:       total time =    6556.42 ms /   121 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      29.46 ms /    47 runs   (    0.63 ms per token,  1595.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =     580.58 ms /    39 tokens (   14.89 ms per token,    67.17 tokens per second)\n",
      "llama_print_timings:        eval time =    3683.54 ms /    46 runs   (   80.08 ms per token,    12.49 tokens per second)\n",
      "llama_print_timings:       total time =    4718.14 ms /    85 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     125.07 ms /   176 runs   (    0.71 ms per token,  1407.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =     863.19 ms /    70 tokens (   12.33 ms per token,    81.09 tokens per second)\n",
      "llama_print_timings:        eval time =   14169.07 ms /   175 runs   (   80.97 ms per token,    12.35 tokens per second)\n",
      "llama_print_timings:       total time =   16939.09 ms /   245 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      13.58 ms /    25 runs   (    0.54 ms per token,  1840.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =     310.01 ms /    15 tokens (   20.67 ms per token,    48.39 tokens per second)\n",
      "llama_print_timings:        eval time =    2284.94 ms /    24 runs   (   95.21 ms per token,    10.50 tokens per second)\n",
      "llama_print_timings:       total time =    2814.17 ms /    39 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     118.20 ms /   154 runs   (    0.77 ms per token,  1302.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =     881.02 ms /    84 tokens (   10.49 ms per token,    95.34 tokens per second)\n",
      "llama_print_timings:        eval time =   12352.48 ms /   153 runs   (   80.74 ms per token,    12.39 tokens per second)\n",
      "llama_print_timings:       total time =   14997.08 ms /   237 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     128.72 ms /   178 runs   (    0.72 ms per token,  1382.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =     957.51 ms /    73 tokens (   13.12 ms per token,    76.24 tokens per second)\n",
      "llama_print_timings:        eval time =   15300.99 ms /   177 runs   (   86.45 ms per token,    11.57 tokens per second)\n",
      "llama_print_timings:       total time =   18149.69 ms /   250 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      14.64 ms /    23 runs   (    0.64 ms per token,  1571.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =     387.36 ms /    12 tokens (   32.28 ms per token,    30.98 tokens per second)\n",
      "llama_print_timings:        eval time =    1810.08 ms /    22 runs   (   82.28 ms per token,    12.15 tokens per second)\n",
      "llama_print_timings:       total time =    2434.29 ms /    34 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     162.86 ms /   216 runs   (    0.75 ms per token,  1326.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1454.94 ms /   144 tokens (   10.10 ms per token,    98.97 tokens per second)\n",
      "llama_print_timings:        eval time =   17750.81 ms /   215 runs   (   82.56 ms per token,    12.11 tokens per second)\n",
      "llama_print_timings:       total time =   21699.88 ms /   359 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      20.09 ms /    27 runs   (    0.74 ms per token,  1343.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =     432.94 ms /    16 tokens (   27.06 ms per token,    36.96 tokens per second)\n",
      "llama_print_timings:        eval time =    2105.17 ms /    26 runs   (   80.97 ms per token,    12.35 tokens per second)\n",
      "llama_print_timings:       total time =    2827.52 ms /    42 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     116.69 ms /   153 runs   (    0.76 ms per token,  1311.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =     576.74 ms /    35 tokens (   16.48 ms per token,    60.69 tokens per second)\n",
      "llama_print_timings:        eval time =   12300.38 ms /   152 runs   (   80.92 ms per token,    12.36 tokens per second)\n",
      "llama_print_timings:       total time =   14583.85 ms /   187 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      80.50 ms /   110 runs   (    0.73 ms per token,  1366.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =     636.98 ms /    50 tokens (   12.74 ms per token,    78.50 tokens per second)\n",
      "llama_print_timings:        eval time =    9056.11 ms /   109 runs   (   83.08 ms per token,    12.04 tokens per second)\n",
      "llama_print_timings:       total time =   10871.69 ms /   159 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra data: line 3 column 1 (char 238)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      99.82 ms /   138 runs   (    0.72 ms per token,  1382.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =     875.01 ms /    83 tokens (   10.54 ms per token,    94.86 tokens per second)\n",
      "llama_print_timings:        eval time =   11301.48 ms /   137 runs   (   82.49 ms per token,    12.12 tokens per second)\n",
      "llama_print_timings:       total time =   13620.96 ms /   220 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      34.38 ms /    48 runs   (    0.72 ms per token,  1396.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =     648.26 ms /    37 tokens (   17.52 ms per token,    57.08 tokens per second)\n",
      "llama_print_timings:        eval time =    3724.87 ms /    47 runs   (   79.25 ms per token,    12.62 tokens per second)\n",
      "llama_print_timings:       total time =    4897.80 ms /    84 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      41.77 ms /    57 runs   (    0.73 ms per token,  1364.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =     358.03 ms /    20 tokens (   17.90 ms per token,    55.86 tokens per second)\n",
      "llama_print_timings:        eval time =    4543.03 ms /    56 runs   (   81.13 ms per token,    12.33 tokens per second)\n",
      "llama_print_timings:       total time =    5533.72 ms /    76 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      18.52 ms /    27 runs   (    0.69 ms per token,  1458.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =     311.90 ms /    17 tokens (   18.35 ms per token,    54.50 tokens per second)\n",
      "llama_print_timings:        eval time =    2074.49 ms /    26 runs   (   79.79 ms per token,    12.53 tokens per second)\n",
      "llama_print_timings:       total time =    2657.69 ms /    43 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     123.77 ms /   177 runs   (    0.70 ms per token,  1430.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1153.67 ms /   103 tokens (   11.20 ms per token,    89.28 tokens per second)\n",
      "llama_print_timings:        eval time =   14648.85 ms /   176 runs   (   83.23 ms per token,    12.01 tokens per second)\n",
      "llama_print_timings:       total time =   17695.01 ms /   279 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      57.47 ms /    76 runs   (    0.76 ms per token,  1322.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1663.86 ms /    68 tokens (   24.47 ms per token,    40.87 tokens per second)\n",
      "llama_print_timings:        eval time =    6000.68 ms /    75 runs   (   80.01 ms per token,    12.50 tokens per second)\n",
      "llama_print_timings:       total time =    8543.20 ms /   143 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      34.62 ms /    44 runs   (    0.79 ms per token,  1270.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =     423.74 ms /    32 tokens (   13.24 ms per token,    75.52 tokens per second)\n",
      "llama_print_timings:        eval time =    3395.14 ms /    43 runs   (   78.96 ms per token,    12.67 tokens per second)\n",
      "llama_print_timings:       total time =    4313.62 ms /    75 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      39.34 ms /    54 runs   (    0.73 ms per token,  1372.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =     583.54 ms /    42 tokens (   13.89 ms per token,    71.97 tokens per second)\n",
      "llama_print_timings:        eval time =    4477.71 ms /    53 runs   (   84.49 ms per token,    11.84 tokens per second)\n",
      "llama_print_timings:       total time =    5627.14 ms /    95 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      73.23 ms /   101 runs   (    0.73 ms per token,  1379.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =     590.36 ms /    47 tokens (   12.56 ms per token,    79.61 tokens per second)\n",
      "llama_print_timings:        eval time =    8116.36 ms /   100 runs   (   81.16 ms per token,    12.32 tokens per second)\n",
      "llama_print_timings:       total time =    9761.39 ms /   147 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      16.53 ms /    26 runs   (    0.64 ms per token,  1573.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =     312.53 ms /    15 tokens (   20.84 ms per token,    48.00 tokens per second)\n",
      "llama_print_timings:        eval time =    2145.02 ms /    25 runs   (   85.80 ms per token,    11.65 tokens per second)\n",
      "llama_print_timings:       total time =    2765.51 ms /    40 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      16.18 ms /    26 runs   (    0.62 ms per token,  1606.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =     309.75 ms /    15 tokens (   20.65 ms per token,    48.43 tokens per second)\n",
      "llama_print_timings:        eval time =    2132.16 ms /    25 runs   (   85.29 ms per token,    11.73 tokens per second)\n",
      "llama_print_timings:       total time =    2724.11 ms /    40 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     120.15 ms /   164 runs   (    0.73 ms per token,  1364.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =     608.99 ms /    61 tokens (    9.98 ms per token,   100.17 tokens per second)\n",
      "llama_print_timings:        eval time =   13317.60 ms /   163 runs   (   81.70 ms per token,    12.24 tokens per second)\n",
      "llama_print_timings:       total time =   15705.55 ms /   224 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      98.42 ms /   129 runs   (    0.76 ms per token,  1310.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =     583.24 ms /    41 tokens (   14.23 ms per token,    70.30 tokens per second)\n",
      "llama_print_timings:        eval time =   10243.52 ms /   128 runs   (   80.03 ms per token,    12.50 tokens per second)\n",
      "llama_print_timings:       total time =   12270.63 ms /   169 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      16.07 ms /    24 runs   (    0.67 ms per token,  1493.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =     312.94 ms /    13 tokens (   24.07 ms per token,    41.54 tokens per second)\n",
      "llama_print_timings:        eval time =    1831.36 ms /    23 runs   (   79.62 ms per token,    12.56 tokens per second)\n",
      "llama_print_timings:       total time =    2383.18 ms /    36 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      37.49 ms /    53 runs   (    0.71 ms per token,  1413.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =     581.83 ms /    42 tokens (   13.85 ms per token,    72.19 tokens per second)\n",
      "llama_print_timings:        eval time =    4182.92 ms /    52 runs   (   80.44 ms per token,    12.43 tokens per second)\n",
      "llama_print_timings:       total time =    5318.11 ms /    94 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      13.26 ms /    22 runs   (    0.60 ms per token,  1659.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =     308.41 ms /    11 tokens (   28.04 ms per token,    35.67 tokens per second)\n",
      "llama_print_timings:        eval time =    1717.63 ms /    21 runs   (   81.79 ms per token,    12.23 tokens per second)\n",
      "llama_print_timings:       total time =    2238.57 ms /    32 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      12.25 ms /    21 runs   (    0.58 ms per token,  1713.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =     307.19 ms /    10 tokens (   30.72 ms per token,    32.55 tokens per second)\n",
      "llama_print_timings:        eval time =    1625.27 ms /    20 runs   (   81.26 ms per token,    12.31 tokens per second)\n",
      "llama_print_timings:       total time =    2131.21 ms /    30 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      61.10 ms /    81 runs   (    0.75 ms per token,  1325.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =     310.48 ms /    13 tokens (   23.88 ms per token,    41.87 tokens per second)\n",
      "llama_print_timings:        eval time =    6424.85 ms /    80 runs   (   80.31 ms per token,    12.45 tokens per second)\n",
      "llama_print_timings:       total time =    7611.82 ms /    93 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      43.20 ms /    63 runs   (    0.69 ms per token,  1458.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =     592.36 ms /    52 tokens (   11.39 ms per token,    87.78 tokens per second)\n",
      "llama_print_timings:        eval time =    5515.99 ms /    62 runs   (   88.97 ms per token,    11.24 tokens per second)\n",
      "llama_print_timings:       total time =    6847.19 ms /   114 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     110.84 ms /   134 runs   (    0.83 ms per token,  1209.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =     584.20 ms /    38 tokens (   15.37 ms per token,    65.05 tokens per second)\n",
      "llama_print_timings:        eval time =   10584.53 ms /   133 runs   (   79.58 ms per token,    12.57 tokens per second)\n",
      "llama_print_timings:       total time =   12760.11 ms /   171 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      91.99 ms /   108 runs   (    0.85 ms per token,  1174.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =     583.09 ms /    45 tokens (   12.96 ms per token,    77.18 tokens per second)\n",
      "llama_print_timings:        eval time =    8475.87 ms /   107 runs   (   79.21 ms per token,    12.62 tokens per second)\n",
      "llama_print_timings:       total time =   10297.65 ms /   152 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra data: line 3 column 1 (char 237)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      72.30 ms /    98 runs   (    0.74 ms per token,  1355.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =     317.88 ms /    31 tokens (   10.25 ms per token,    97.52 tokens per second)\n",
      "llama_print_timings:        eval time =    8144.94 ms /    97 runs   (   83.97 ms per token,    11.91 tokens per second)\n",
      "llama_print_timings:       total time =    9513.14 ms /   128 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     107.13 ms /   132 runs   (    0.81 ms per token,  1232.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1012.85 ms /    90 tokens (   11.25 ms per token,    88.86 tokens per second)\n",
      "llama_print_timings:        eval time =   10483.07 ms /   131 runs   (   80.02 ms per token,    12.50 tokens per second)\n",
      "llama_print_timings:       total time =   13020.39 ms /   221 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      79.46 ms /    97 runs   (    0.82 ms per token,  1220.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =     661.42 ms /    52 tokens (   12.72 ms per token,    78.62 tokens per second)\n",
      "llama_print_timings:        eval time =    7696.67 ms /    96 runs   (   80.17 ms per token,    12.47 tokens per second)\n",
      "llama_print_timings:       total time =    9456.16 ms /   148 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expecting ',' delimiter: line 1 column 50 (char 49)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     105.98 ms /   160 runs   (    0.66 ms per token,  1509.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =     863.32 ms /    73 tokens (   11.83 ms per token,    84.56 tokens per second)\n",
      "llama_print_timings:        eval time =   13179.93 ms /   159 runs   (   82.89 ms per token,    12.06 tokens per second)\n",
      "llama_print_timings:       total time =   15691.33 ms /   232 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      31.19 ms /    46 runs   (    0.68 ms per token,  1474.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =     584.50 ms /    34 tokens (   17.19 ms per token,    58.17 tokens per second)\n",
      "llama_print_timings:        eval time =    3569.61 ms /    45 runs   (   79.32 ms per token,    12.61 tokens per second)\n",
      "llama_print_timings:       total time =    4621.70 ms /    79 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     102.47 ms /   137 runs   (    0.75 ms per token,  1336.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =     894.41 ms /    81 tokens (   11.04 ms per token,    90.56 tokens per second)\n",
      "llama_print_timings:        eval time =   11043.66 ms /   136 runs   (   81.20 ms per token,    12.31 tokens per second)\n",
      "llama_print_timings:       total time =   13420.50 ms /   217 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     120.24 ms /   149 runs   (    0.81 ms per token,  1239.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =     944.63 ms /    82 tokens (   11.52 ms per token,    86.81 tokens per second)\n",
      "llama_print_timings:        eval time =   11828.24 ms /   148 runs   (   79.92 ms per token,    12.51 tokens per second)\n",
      "llama_print_timings:       total time =   14464.65 ms /   230 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      41.05 ms /    50 runs   (    0.82 ms per token,  1218.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =     647.59 ms /    39 tokens (   16.60 ms per token,    60.22 tokens per second)\n",
      "llama_print_timings:        eval time =    3870.75 ms /    49 runs   (   78.99 ms per token,    12.66 tokens per second)\n",
      "llama_print_timings:       total time =    5071.70 ms /    88 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     134.51 ms /   169 runs   (    0.80 ms per token,  1256.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =     875.71 ms /    83 tokens (   10.55 ms per token,    94.78 tokens per second)\n",
      "llama_print_timings:        eval time =   13548.61 ms /   168 runs   (   80.65 ms per token,    12.40 tokens per second)\n",
      "llama_print_timings:       total time =   16380.38 ms /   251 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      40.33 ms /    50 runs   (    0.81 ms per token,  1239.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =     741.85 ms /    39 tokens (   19.02 ms per token,    52.57 tokens per second)\n",
      "llama_print_timings:        eval time =    3893.33 ms /    49 runs   (   79.46 ms per token,    12.59 tokens per second)\n",
      "llama_print_timings:       total time =    5181.51 ms /    88 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     108.72 ms /   131 runs   (    0.83 ms per token,  1204.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =     593.33 ms /    52 tokens (   11.41 ms per token,    87.64 tokens per second)\n",
      "llama_print_timings:        eval time =   10396.81 ms /   130 runs   (   79.98 ms per token,    12.50 tokens per second)\n",
      "llama_print_timings:       total time =   12511.24 ms /   182 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      34.01 ms /    46 runs   (    0.74 ms per token,  1352.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =     577.70 ms /    35 tokens (   16.51 ms per token,    60.59 tokens per second)\n",
      "llama_print_timings:        eval time =    3556.44 ms /    45 runs   (   79.03 ms per token,    12.65 tokens per second)\n",
      "llama_print_timings:       total time =    4617.83 ms /    80 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      17.70 ms /    25 runs   (    0.71 ms per token,  1412.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =     308.78 ms /    14 tokens (   22.06 ms per token,    45.34 tokens per second)\n",
      "llama_print_timings:        eval time =    1924.07 ms /    24 runs   (   80.17 ms per token,    12.47 tokens per second)\n",
      "llama_print_timings:       total time =    2500.88 ms /    38 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      24.43 ms /    40 runs   (    0.61 ms per token,  1637.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =     320.68 ms /    28 tokens (   11.45 ms per token,    87.32 tokens per second)\n",
      "llama_print_timings:        eval time =    3214.97 ms /    39 runs   (   82.44 ms per token,    12.13 tokens per second)\n",
      "llama_print_timings:       total time =    3905.35 ms /    67 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      42.98 ms /    70 runs   (    0.61 ms per token,  1628.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =     605.55 ms /    57 tokens (   10.62 ms per token,    94.13 tokens per second)\n",
      "llama_print_timings:        eval time =    6053.61 ms /    69 runs   (   87.73 ms per token,    11.40 tokens per second)\n",
      "llama_print_timings:       total time =    7326.81 ms /   126 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     114.32 ms /   162 runs   (    0.71 ms per token,  1417.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =     598.69 ms /    54 tokens (   11.09 ms per token,    90.20 tokens per second)\n",
      "llama_print_timings:        eval time =   13164.71 ms /   161 runs   (   81.77 ms per token,    12.23 tokens per second)\n",
      "llama_print_timings:       total time =   15515.70 ms /   215 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     106.50 ms /   154 runs   (    0.69 ms per token,  1445.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =     885.40 ms /    85 tokens (   10.42 ms per token,    96.00 tokens per second)\n",
      "llama_print_timings:        eval time =   12866.94 ms /   153 runs   (   84.10 ms per token,    11.89 tokens per second)\n",
      "llama_print_timings:       total time =   15424.00 ms /   238 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      30.09 ms /    97 runs   (    0.31 ms per token,  3223.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =     671.77 ms /    38 tokens (   17.68 ms per token,    56.57 tokens per second)\n",
      "llama_print_timings:        eval time =    8386.33 ms /    96 runs   (   87.36 ms per token,    11.45 tokens per second)\n",
      "llama_print_timings:       total time =    9647.68 ms /   134 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      14.10 ms /    41 runs   (    0.34 ms per token,  2907.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =     322.59 ms /    29 tokens (   11.12 ms per token,    89.90 tokens per second)\n",
      "llama_print_timings:        eval time =    3390.79 ms /    40 runs   (   84.77 ms per token,    11.80 tokens per second)\n",
      "llama_print_timings:       total time =    3967.26 ms /    69 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     103.26 ms /   149 runs   (    0.69 ms per token,  1442.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =     869.48 ms /    72 tokens (   12.08 ms per token,    82.81 tokens per second)\n",
      "llama_print_timings:        eval time =   12275.51 ms /   148 runs   (   82.94 ms per token,    12.06 tokens per second)\n",
      "llama_print_timings:       total time =   14753.98 ms /   220 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      77.42 ms /   119 runs   (    0.65 ms per token,  1537.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =     672.25 ms /    45 tokens (   14.94 ms per token,    66.94 tokens per second)\n",
      "llama_print_timings:        eval time =    9564.76 ms /   118 runs   (   81.06 ms per token,    12.34 tokens per second)\n",
      "llama_print_timings:       total time =   11451.83 ms /   163 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      37.44 ms /    58 runs   (    0.65 ms per token,  1549.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =     585.02 ms /    46 tokens (   12.72 ms per token,    78.63 tokens per second)\n",
      "llama_print_timings:        eval time =    4603.90 ms /    57 runs   (   80.77 ms per token,    12.38 tokens per second)\n",
      "llama_print_timings:       total time =    5761.90 ms /   103 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      93.51 ms /   125 runs   (    0.75 ms per token,  1336.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =     584.05 ms /    44 tokens (   13.27 ms per token,    75.34 tokens per second)\n",
      "llama_print_timings:        eval time =   10031.91 ms /   124 runs   (   80.90 ms per token,    12.36 tokens per second)\n",
      "llama_print_timings:       total time =   11978.83 ms /   168 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra data: line 3 column 1 (char 214)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      57.67 ms /    79 runs   (    0.73 ms per token,  1369.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =     318.35 ms /    30 tokens (   10.61 ms per token,    94.23 tokens per second)\n",
      "llama_print_timings:        eval time =    6330.48 ms /    78 runs   (   81.16 ms per token,    12.32 tokens per second)\n",
      "llama_print_timings:       total time =    7572.81 ms /   108 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra data: line 3 column 1 (char 146)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     128.84 ms /   180 runs   (    0.72 ms per token,  1397.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =     881.29 ms /    84 tokens (   10.49 ms per token,    95.31 tokens per second)\n",
      "llama_print_timings:        eval time =   14664.32 ms /   179 runs   (   81.92 ms per token,    12.21 tokens per second)\n",
      "llama_print_timings:       total time =   17585.58 ms /   263 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      99.59 ms /   140 runs   (    0.71 ms per token,  1405.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =     700.06 ms /    58 tokens (   12.07 ms per token,    82.85 tokens per second)\n",
      "llama_print_timings:        eval time =   11270.20 ms /   139 runs   (   81.08 ms per token,    12.33 tokens per second)\n",
      "llama_print_timings:       total time =   13611.26 ms /   197 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      46.96 ms /    62 runs   (    0.76 ms per token,  1320.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =     590.46 ms /    50 tokens (   11.81 ms per token,    84.68 tokens per second)\n",
      "llama_print_timings:        eval time =    4863.88 ms /    61 runs   (   79.74 ms per token,    12.54 tokens per second)\n",
      "llama_print_timings:       total time =    6137.98 ms /   111 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      64.50 ms /    90 runs   (    0.72 ms per token,  1395.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =     640.23 ms /    46 tokens (   13.92 ms per token,    71.85 tokens per second)\n",
      "llama_print_timings:        eval time =    7167.83 ms /    89 runs   (   80.54 ms per token,    12.42 tokens per second)\n",
      "llama_print_timings:       total time =    8787.63 ms /   135 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      62.71 ms /    90 runs   (    0.70 ms per token,  1435.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =     578.33 ms /    34 tokens (   17.01 ms per token,    58.79 tokens per second)\n",
      "llama_print_timings:        eval time =    7240.71 ms /    89 runs   (   81.36 ms per token,    12.29 tokens per second)\n",
      "llama_print_timings:       total time =    8736.61 ms /   123 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      83.73 ms /   120 runs   (    0.70 ms per token,  1433.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =     594.50 ms /    53 tokens (   11.22 ms per token,    89.15 tokens per second)\n",
      "llama_print_timings:        eval time =    9643.12 ms /   119 runs   (   81.03 ms per token,    12.34 tokens per second)\n",
      "llama_print_timings:       total time =   11490.84 ms /   172 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      63.86 ms /    97 runs   (    0.66 ms per token,  1518.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =     604.01 ms /    57 tokens (   10.60 ms per token,    94.37 tokens per second)\n",
      "llama_print_timings:        eval time =    7814.25 ms /    96 runs   (   81.40 ms per token,    12.29 tokens per second)\n",
      "llama_print_timings:       total time =    9423.23 ms /   153 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra data: line 3 column 1 (char 249)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      51.02 ms /    69 runs   (    0.74 ms per token,  1352.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =     313.28 ms /    24 tokens (   13.05 ms per token,    76.61 tokens per second)\n",
      "llama_print_timings:        eval time =    5546.82 ms /    68 runs   (   81.57 ms per token,    12.26 tokens per second)\n",
      "llama_print_timings:       total time =    6617.59 ms /    92 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      50.49 ms /    90 runs   (    0.56 ms per token,  1782.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =     589.72 ms /    46 tokens (   12.82 ms per token,    78.00 tokens per second)\n",
      "llama_print_timings:        eval time =    7556.60 ms /    89 runs   (   84.91 ms per token,    11.78 tokens per second)\n",
      "llama_print_timings:       total time =    8953.44 ms /   135 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      31.39 ms /    40 runs   (    0.78 ms per token,  1274.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =     319.15 ms /    25 tokens (   12.77 ms per token,    78.33 tokens per second)\n",
      "llama_print_timings:        eval time =    3100.05 ms /    39 runs   (   79.49 ms per token,    12.58 tokens per second)\n",
      "llama_print_timings:       total time =    3867.30 ms /    64 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      52.46 ms /    70 runs   (    0.75 ms per token,  1334.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =     598.59 ms /    56 tokens (   10.69 ms per token,    93.55 tokens per second)\n",
      "llama_print_timings:        eval time =    5556.15 ms /    69 runs   (   80.52 ms per token,    12.42 tokens per second)\n",
      "llama_print_timings:       total time =    6909.13 ms /   125 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      79.99 ms /   118 runs   (    0.68 ms per token,  1475.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =     579.11 ms /    40 tokens (   14.48 ms per token,    69.07 tokens per second)\n",
      "llama_print_timings:        eval time =    9471.46 ms /   117 runs   (   80.95 ms per token,    12.35 tokens per second)\n",
      "llama_print_timings:       total time =   11235.84 ms /   157 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      79.67 ms /   112 runs   (    0.71 ms per token,  1405.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =     584.50 ms /    46 tokens (   12.71 ms per token,    78.70 tokens per second)\n",
      "llama_print_timings:        eval time =    8909.82 ms /   111 runs   (   80.27 ms per token,    12.46 tokens per second)\n",
      "llama_print_timings:       total time =   10674.88 ms /   157 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      87.51 ms /   115 runs   (    0.76 ms per token,  1314.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =     865.25 ms /    68 tokens (   12.72 ms per token,    78.59 tokens per second)\n",
      "llama_print_timings:        eval time =    9231.87 ms /   114 runs   (   80.98 ms per token,    12.35 tokens per second)\n",
      "llama_print_timings:       total time =   11361.21 ms /   182 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      59.31 ms /    86 runs   (    0.69 ms per token,  1450.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =     679.82 ms /    42 tokens (   16.19 ms per token,    61.78 tokens per second)\n",
      "llama_print_timings:        eval time =    6830.76 ms /    85 runs   (   80.36 ms per token,    12.44 tokens per second)\n",
      "llama_print_timings:       total time =    8419.03 ms /   127 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      81.69 ms /   103 runs   (    0.79 ms per token,  1260.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =     581.12 ms /    39 tokens (   14.90 ms per token,    67.11 tokens per second)\n",
      "llama_print_timings:        eval time =    8124.97 ms /   102 runs   (   79.66 ms per token,    12.55 tokens per second)\n",
      "llama_print_timings:       total time =    9843.66 ms /   141 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     109.04 ms /   157 runs   (    0.69 ms per token,  1439.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =     323.24 ms /    31 tokens (   10.43 ms per token,    95.90 tokens per second)\n",
      "llama_print_timings:        eval time =   12590.17 ms /   156 runs   (   80.71 ms per token,    12.39 tokens per second)\n",
      "llama_print_timings:       total time =   14591.55 ms /   187 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      75.32 ms /   103 runs   (    0.73 ms per token,  1367.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =     578.27 ms /    40 tokens (   14.46 ms per token,    69.17 tokens per second)\n",
      "llama_print_timings:        eval time =    8232.95 ms /   102 runs   (   80.72 ms per token,    12.39 tokens per second)\n",
      "llama_print_timings:       total time =    9873.29 ms /   142 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      73.35 ms /   103 runs   (    0.71 ms per token,  1404.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =     601.71 ms /    36 tokens (   16.71 ms per token,    59.83 tokens per second)\n",
      "llama_print_timings:        eval time =    8399.19 ms /   102 runs   (   82.34 ms per token,    12.14 tokens per second)\n",
      "llama_print_timings:       total time =   10135.14 ms /   138 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      89.62 ms /   109 runs   (    0.82 ms per token,  1216.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =     579.77 ms /    39 tokens (   14.87 ms per token,    67.27 tokens per second)\n",
      "llama_print_timings:        eval time =    8562.06 ms /   108 runs   (   79.28 ms per token,    12.61 tokens per second)\n",
      "llama_print_timings:       total time =   10396.34 ms /   147 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      85.53 ms /   121 runs   (    0.71 ms per token,  1414.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =     582.05 ms /    41 tokens (   14.20 ms per token,    70.44 tokens per second)\n",
      "llama_print_timings:        eval time =   10162.38 ms /   120 runs   (   84.69 ms per token,    11.81 tokens per second)\n",
      "llama_print_timings:       total time =   12121.31 ms /   161 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra data: line 3 column 1 (char 187)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      16.57 ms /    33 runs   (    0.50 ms per token,  1992.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =     331.18 ms /    21 tokens (   15.77 ms per token,    63.41 tokens per second)\n",
      "llama_print_timings:        eval time =    2953.70 ms /    32 runs   (   92.30 ms per token,    10.83 tokens per second)\n",
      "llama_print_timings:       total time =    3616.12 ms /    53 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      31.78 ms /    48 runs   (    0.66 ms per token,  1510.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =     579.21 ms /    36 tokens (   16.09 ms per token,    62.15 tokens per second)\n",
      "llama_print_timings:        eval time =    3802.38 ms /    47 runs   (   80.90 ms per token,    12.36 tokens per second)\n",
      "llama_print_timings:       total time =    4903.46 ms /    83 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      84.53 ms /   133 runs   (    0.64 ms per token,  1573.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =     591.99 ms /    64 tokens (    9.25 ms per token,   108.11 tokens per second)\n",
      "llama_print_timings:        eval time =   12094.52 ms /   132 runs   (   91.63 ms per token,    10.91 tokens per second)\n",
      "llama_print_timings:       total time =   14280.69 ms /   196 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      54.75 ms /    80 runs   (    0.68 ms per token,  1461.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =     662.52 ms /    43 tokens (   15.41 ms per token,    64.90 tokens per second)\n",
      "llama_print_timings:        eval time =    6776.44 ms /    79 runs   (   85.78 ms per token,    11.66 tokens per second)\n",
      "llama_print_timings:       total time =    8373.84 ms /   122 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      96.40 ms /   147 runs   (    0.66 ms per token,  1524.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =     869.57 ms /    72 tokens (   12.08 ms per token,    82.80 tokens per second)\n",
      "llama_print_timings:        eval time =   13097.26 ms /   146 runs   (   89.71 ms per token,    11.15 tokens per second)\n",
      "llama_print_timings:       total time =   15713.70 ms /   218 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      83.11 ms /   142 runs   (    0.59 ms per token,  1708.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =     814.20 ms /    42 tokens (   19.39 ms per token,    51.58 tokens per second)\n",
      "llama_print_timings:        eval time =   15339.94 ms /   141 runs   (  108.79 ms per token,     9.19 tokens per second)\n",
      "llama_print_timings:       total time =   17831.90 ms /   183 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      72.44 ms /   129 runs   (    0.56 ms per token,  1780.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =     688.34 ms /    41 tokens (   16.79 ms per token,    59.56 tokens per second)\n",
      "llama_print_timings:        eval time =   11574.09 ms /   128 runs   (   90.42 ms per token,    11.06 tokens per second)\n",
      "llama_print_timings:       total time =   13606.66 ms /   169 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      65.59 ms /   106 runs   (    0.62 ms per token,  1616.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =     615.89 ms /    56 tokens (   11.00 ms per token,    90.93 tokens per second)\n",
      "llama_print_timings:        eval time =    8772.21 ms /   105 runs   (   83.54 ms per token,    11.97 tokens per second)\n",
      "llama_print_timings:       total time =   10443.06 ms /   161 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      39.71 ms /    62 runs   (    0.64 ms per token,  1561.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =     377.66 ms /    29 tokens (   13.02 ms per token,    76.79 tokens per second)\n",
      "llama_print_timings:        eval time =    5522.56 ms /    61 runs   (   90.53 ms per token,    11.05 tokens per second)\n",
      "llama_print_timings:       total time =    6598.99 ms /    90 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expecting ',' delimiter: line 1 column 51 (char 50)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      28.80 ms /    46 runs   (    0.63 ms per token,  1597.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =     574.77 ms /    35 tokens (   16.42 ms per token,    60.89 tokens per second)\n",
      "llama_print_timings:        eval time =    3692.39 ms /    45 runs   (   82.05 ms per token,    12.19 tokens per second)\n",
      "llama_print_timings:       total time =    4735.73 ms /    80 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      58.87 ms /   101 runs   (    0.58 ms per token,  1715.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =     582.56 ms /    38 tokens (   15.33 ms per token,    65.23 tokens per second)\n",
      "llama_print_timings:        eval time =    8739.17 ms /   100 runs   (   87.39 ms per token,    11.44 tokens per second)\n",
      "llama_print_timings:       total time =   10310.23 ms /   138 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      64.53 ms /    91 runs   (    0.71 ms per token,  1410.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =     587.45 ms /    40 tokens (   14.69 ms per token,    68.09 tokens per second)\n",
      "llama_print_timings:        eval time =    7392.76 ms /    90 runs   (   82.14 ms per token,    12.17 tokens per second)\n",
      "llama_print_timings:       total time =    9003.97 ms /   130 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      76.60 ms /   123 runs   (    0.62 ms per token,  1605.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =     601.76 ms /    55 tokens (   10.94 ms per token,    91.40 tokens per second)\n",
      "llama_print_timings:        eval time =   10133.44 ms /   122 runs   (   83.06 ms per token,    12.04 tokens per second)\n",
      "llama_print_timings:       total time =   12033.30 ms /   177 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      81.22 ms /   119 runs   (    0.68 ms per token,  1465.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =     605.79 ms /    57 tokens (   10.63 ms per token,    94.09 tokens per second)\n",
      "llama_print_timings:        eval time =    9572.82 ms /   118 runs   (   81.13 ms per token,    12.33 tokens per second)\n",
      "llama_print_timings:       total time =   11441.70 ms /   175 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      64.22 ms /    79 runs   (    0.81 ms per token,  1230.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =     580.37 ms /    35 tokens (   16.58 ms per token,    60.31 tokens per second)\n",
      "llama_print_timings:        eval time =    6397.12 ms /    78 runs   (   82.01 ms per token,    12.19 tokens per second)\n",
      "llama_print_timings:       total time =    7896.29 ms /   113 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      51.16 ms /    82 runs   (    0.62 ms per token,  1602.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =     581.25 ms /    36 tokens (   16.15 ms per token,    61.94 tokens per second)\n",
      "llama_print_timings:        eval time =    6751.37 ms /    81 runs   (   83.35 ms per token,    12.00 tokens per second)\n",
      "llama_print_timings:       total time =    8121.75 ms /   117 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      90.95 ms /   130 runs   (    0.70 ms per token,  1429.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =     876.24 ms /    77 tokens (   11.38 ms per token,    87.88 tokens per second)\n",
      "llama_print_timings:        eval time =   10727.87 ms /   129 runs   (   83.16 ms per token,    12.02 tokens per second)\n",
      "llama_print_timings:       total time =   13111.04 ms /   206 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      71.73 ms /    96 runs   (    0.75 ms per token,  1338.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =     679.54 ms /    50 tokens (   13.59 ms per token,    73.58 tokens per second)\n",
      "llama_print_timings:        eval time =    7610.00 ms /    95 runs   (   80.11 ms per token,    12.48 tokens per second)\n",
      "llama_print_timings:       total time =    9365.14 ms /   145 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      37.70 ms /    50 runs   (    0.75 ms per token,  1326.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =     578.37 ms /    38 tokens (   15.22 ms per token,    65.70 tokens per second)\n",
      "llama_print_timings:        eval time =    3870.44 ms /    49 runs   (   78.99 ms per token,    12.66 tokens per second)\n",
      "llama_print_timings:       total time =    4998.14 ms /    87 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      20.41 ms /    39 runs   (    0.52 ms per token,  1911.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =     320.45 ms /    27 tokens (   11.87 ms per token,    84.26 tokens per second)\n",
      "llama_print_timings:        eval time =    3301.21 ms /    38 runs   (   86.87 ms per token,    11.51 tokens per second)\n",
      "llama_print_timings:       total time =    3929.07 ms /    65 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      59.41 ms /    79 runs   (    0.75 ms per token,  1329.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =     323.75 ms /    30 tokens (   10.79 ms per token,    92.67 tokens per second)\n",
      "llama_print_timings:        eval time =    6347.36 ms /    78 runs   (   81.38 ms per token,    12.29 tokens per second)\n",
      "llama_print_timings:       total time =    7567.43 ms /   108 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     182.58 ms /   239 runs   (    0.76 ms per token,  1309.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =     904.80 ms /    87 tokens (   10.40 ms per token,    96.15 tokens per second)\n",
      "llama_print_timings:        eval time =   19142.84 ms /   238 runs   (   80.43 ms per token,    12.43 tokens per second)\n",
      "llama_print_timings:       total time =   23073.60 ms /   325 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      63.56 ms /    91 runs   (    0.70 ms per token,  1431.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =     416.46 ms /    26 tokens (   16.02 ms per token,    62.43 tokens per second)\n",
      "llama_print_timings:        eval time =    7293.64 ms /    90 runs   (   81.04 ms per token,    12.34 tokens per second)\n",
      "llama_print_timings:       total time =    8697.46 ms /   116 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      71.68 ms /   100 runs   (    0.72 ms per token,  1395.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =     590.86 ms /    52 tokens (   11.36 ms per token,    88.01 tokens per second)\n",
      "llama_print_timings:        eval time =    8053.69 ms /    99 runs   (   81.35 ms per token,    12.29 tokens per second)\n",
      "llama_print_timings:       total time =    9671.76 ms /   151 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      75.91 ms /   108 runs   (    0.70 ms per token,  1422.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =     318.61 ms /    26 tokens (   12.25 ms per token,    81.61 tokens per second)\n",
      "llama_print_timings:        eval time =    8569.43 ms /   107 runs   (   80.09 ms per token,    12.49 tokens per second)\n",
      "llama_print_timings:       total time =    9994.51 ms /   133 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      56.75 ms /    95 runs   (    0.60 ms per token,  1673.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =     589.35 ms /    49 tokens (   12.03 ms per token,    83.14 tokens per second)\n",
      "llama_print_timings:        eval time =    8122.01 ms /    94 runs   (   86.40 ms per token,    11.57 tokens per second)\n",
      "llama_print_timings:       total time =    9682.68 ms /   143 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      26.72 ms /    45 runs   (    0.59 ms per token,  1684.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =     580.91 ms /    33 tokens (   17.60 ms per token,    56.81 tokens per second)\n",
      "llama_print_timings:        eval time =    3628.95 ms /    44 runs   (   82.48 ms per token,    12.12 tokens per second)\n",
      "llama_print_timings:       total time =    4655.10 ms /    77 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      50.78 ms /    84 runs   (    0.60 ms per token,  1654.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =     590.29 ms /    41 tokens (   14.40 ms per token,    69.46 tokens per second)\n",
      "llama_print_timings:        eval time =    6995.15 ms /    83 runs   (   84.28 ms per token,    11.87 tokens per second)\n",
      "llama_print_timings:       total time =    8356.14 ms /   124 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      88.65 ms /   132 runs   (    0.67 ms per token,  1489.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =     620.39 ms /    59 tokens (   10.52 ms per token,    95.10 tokens per second)\n",
      "llama_print_timings:        eval time =   11642.37 ms /   131 runs   (   88.87 ms per token,    11.25 tokens per second)\n",
      "llama_print_timings:       total time =   13728.02 ms /   190 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      84.40 ms /   124 runs   (    0.68 ms per token,  1469.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =     591.70 ms /    50 tokens (   11.83 ms per token,    84.50 tokens per second)\n",
      "llama_print_timings:        eval time =   10067.85 ms /   123 runs   (   81.85 ms per token,    12.22 tokens per second)\n",
      "llama_print_timings:       total time =   11940.97 ms /   173 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      69.76 ms /   100 runs   (    0.70 ms per token,  1433.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =     582.96 ms /    42 tokens (   13.88 ms per token,    72.05 tokens per second)\n",
      "llama_print_timings:        eval time =    8024.18 ms /    99 runs   (   81.05 ms per token,    12.34 tokens per second)\n",
      "llama_print_timings:       total time =    9569.22 ms /   141 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      26.32 ms /    39 runs   (    0.67 ms per token,  1482.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =     321.67 ms /    27 tokens (   11.91 ms per token,    83.94 tokens per second)\n",
      "llama_print_timings:        eval time =    3211.34 ms /    38 runs   (   84.51 ms per token,    11.83 tokens per second)\n",
      "llama_print_timings:       total time =    3983.99 ms /    65 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      38.11 ms /    51 runs   (    0.75 ms per token,  1338.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =     584.19 ms /    39 tokens (   14.98 ms per token,    66.76 tokens per second)\n",
      "llama_print_timings:        eval time =    4019.87 ms /    50 runs   (   80.40 ms per token,    12.44 tokens per second)\n",
      "llama_print_timings:       total time =    5147.15 ms /    89 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      34.18 ms /    52 runs   (    0.66 ms per token,  1521.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =     579.68 ms /    40 tokens (   14.49 ms per token,    69.00 tokens per second)\n",
      "llama_print_timings:        eval time =    4123.82 ms /    51 runs   (   80.86 ms per token,    12.37 tokens per second)\n",
      "llama_print_timings:       total time =    5225.43 ms /    91 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      34.48 ms /    52 runs   (    0.66 ms per token,  1508.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =     579.92 ms /    40 tokens (   14.50 ms per token,    68.98 tokens per second)\n",
      "llama_print_timings:        eval time =    4114.35 ms /    51 runs   (   80.67 ms per token,    12.40 tokens per second)\n",
      "llama_print_timings:       total time =    5196.83 ms /    91 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      39.76 ms /    62 runs   (    0.64 ms per token,  1559.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =     321.04 ms /    27 tokens (   11.89 ms per token,    84.10 tokens per second)\n",
      "llama_print_timings:        eval time =    5048.92 ms /    61 runs   (   82.77 ms per token,    12.08 tokens per second)\n",
      "llama_print_timings:       total time =    6000.74 ms /    88 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      62.67 ms /   102 runs   (    0.61 ms per token,  1627.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =     586.27 ms /    39 tokens (   15.03 ms per token,    66.52 tokens per second)\n",
      "llama_print_timings:        eval time =    8476.18 ms /   101 runs   (   83.92 ms per token,    11.92 tokens per second)\n",
      "llama_print_timings:       total time =   10080.57 ms /   140 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =     140.38 ms /   180 runs   (    0.78 ms per token,  1282.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =     861.82 ms /    66 tokens (   13.06 ms per token,    76.58 tokens per second)\n",
      "llama_print_timings:        eval time =   14441.91 ms /   179 runs   (   80.68 ms per token,    12.39 tokens per second)\n",
      "llama_print_timings:       total time =   17363.81 ms /   245 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      48.06 ms /    77 runs   (    0.62 ms per token,  1602.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =     584.83 ms /    43 tokens (   13.60 ms per token,    73.53 tokens per second)\n",
      "llama_print_timings:        eval time =    6362.29 ms /    76 runs   (   83.71 ms per token,    11.95 tokens per second)\n",
      "llama_print_timings:       total time =    7829.12 ms /   119 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      55.62 ms /    80 runs   (    0.70 ms per token,  1438.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =     587.38 ms /    40 tokens (   14.68 ms per token,    68.10 tokens per second)\n",
      "llama_print_timings:        eval time =    6486.68 ms /    79 runs   (   82.11 ms per token,    12.18 tokens per second)\n",
      "llama_print_timings:       total time =    7870.41 ms /   119 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      75.39 ms /   115 runs   (    0.66 ms per token,  1525.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =     576.74 ms /    33 tokens (   17.48 ms per token,    57.22 tokens per second)\n",
      "llama_print_timings:        eval time =    9324.69 ms /   114 runs   (   81.80 ms per token,    12.23 tokens per second)\n",
      "llama_print_timings:       total time =   11117.84 ms /   147 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      34.98 ms /    50 runs   (    0.70 ms per token,  1429.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =     581.71 ms /    38 tokens (   15.31 ms per token,    65.32 tokens per second)\n",
      "llama_print_timings:        eval time =    4071.11 ms /    49 runs   (   83.08 ms per token,    12.04 tokens per second)\n",
      "llama_print_timings:       total time =    5195.70 ms /    87 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      88.44 ms /   122 runs   (    0.72 ms per token,  1379.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =     577.62 ms /    33 tokens (   17.50 ms per token,    57.13 tokens per second)\n",
      "llama_print_timings:        eval time =    9985.59 ms /   121 runs   (   82.53 ms per token,    12.12 tokens per second)\n",
      "llama_print_timings:       total time =   11839.79 ms /   154 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      75.69 ms /   116 runs   (    0.65 ms per token,  1532.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =     586.71 ms /    35 tokens (   16.76 ms per token,    59.65 tokens per second)\n",
      "llama_print_timings:        eval time =    9624.11 ms /   115 runs   (   83.69 ms per token,    11.95 tokens per second)\n",
      "llama_print_timings:       total time =   11310.82 ms /   150 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      29.18 ms /    45 runs   (    0.65 ms per token,  1542.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =     320.57 ms /    29 tokens (   11.05 ms per token,    90.46 tokens per second)\n",
      "llama_print_timings:        eval time =    3616.99 ms /    44 runs   (   82.20 ms per token,    12.16 tokens per second)\n",
      "llama_print_timings:       total time =    4392.44 ms /    73 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      47.47 ms /    63 runs   (    0.75 ms per token,  1327.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =     862.83 ms /    66 tokens (   13.07 ms per token,    76.49 tokens per second)\n",
      "llama_print_timings:        eval time =    5124.15 ms /    62 runs   (   82.65 ms per token,    12.10 tokens per second)\n",
      "llama_print_timings:       total time =    6600.07 ms /   128 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      60.05 ms /    72 runs   (    0.83 ms per token,  1199.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =     324.99 ms /    29 tokens (   11.21 ms per token,    89.23 tokens per second)\n",
      "llama_print_timings:        eval time =    5682.07 ms /    71 runs   (   80.03 ms per token,    12.50 tokens per second)\n",
      "llama_print_timings:       total time =    6832.73 ms /   100 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      66.62 ms /    92 runs   (    0.72 ms per token,  1380.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =     583.00 ms /    45 tokens (   12.96 ms per token,    77.19 tokens per second)\n",
      "llama_print_timings:        eval time =    7270.34 ms /    91 runs   (   79.89 ms per token,    12.52 tokens per second)\n",
      "llama_print_timings:       total time =    8820.67 ms /   136 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      43.82 ms /    56 runs   (    0.78 ms per token,  1278.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =     590.33 ms /    44 tokens (   13.42 ms per token,    74.54 tokens per second)\n",
      "llama_print_timings:        eval time =    4420.00 ms /    55 runs   (   80.36 ms per token,    12.44 tokens per second)\n",
      "llama_print_timings:       total time =    5621.37 ms /    99 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12045.51 ms\n",
      "llama_print_timings:      sample time =      48.90 ms /    63 runs   (    0.78 ms per token,  1288.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =     315.53 ms /    20 tokens (   15.78 ms per token,    63.39 tokens per second)\n",
      "llama_print_timings:        eval time =    4959.72 ms /    62 runs   (   80.00 ms per token,    12.50 tokens per second)\n",
      "llama_print_timings:       total time =    6007.36 ms /    82 tokens\n"
     ]
    }
   ],
   "source": [
    "eval_file_path = '/Users/ananyahooda/Desktop/final/data/evaluation_data/conll04_eval.json' # Replace with the actual path to your eval.json file\n",
    "pred_file_path = '/Users/ananyahooda/Desktop/final/data/Final_entities.json' # The output file path\n",
    "generate_pred_json(eval_file_path, pred_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_file_path = '/Users/ananyahooda/Desktop/final/data/Final_entities.json' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the golden truth JSON file: 288\n",
      "Length of the prediction JSON file: 262\n",
      "Created new JSON files with common data points: 'common_file1.json' and 'common_file2.json'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the JSON data from the two files\n",
    "with open(pred_file_path , 'r') as file:\n",
    "    data1 = json.load(file)\n",
    "\n",
    "with open(golden_file, 'r') as file:\n",
    "    data2 = json.load(file)\n",
    "\n",
    "# Determine the length of both JSON files\n",
    "length_data1 = len(data1)\n",
    "length_data2 = len(data2)\n",
    "\n",
    "# Print the lengths\n",
    "print(f\"Length of the golden truth JSON file: {length_data1}\")\n",
    "print(f\"Length of the prediction JSON file: {length_data2}\")\n",
    "\n",
    "# Convert the lists to dictionaries indexed by the 'id' attribute\n",
    "data1_dict = {item['id']: item for item in data1}\n",
    "data2_dict = {item['id']: item for item in data2}\n",
    "\n",
    "# Find the common IDs\n",
    "common_ids = set(data1_dict.keys()) & set(data2_dict.keys())\n",
    "\n",
    "# Extract the common data points\n",
    "common_data1 = [data1_dict[id] for id in common_ids]\n",
    "common_data2 = [data2_dict[id] for id in common_ids]\n",
    "\n",
    "# Save the common data points to new JSON files\n",
    "with open('/Users/ananyahooda/Desktop/final/pred_common_entities.json', 'w') as file:\n",
    "    json.dump(common_data1, file, indent=4)\n",
    "\n",
    "with open('golden_truth_common.json', 'w') as file: \n",
    "    json.dump(common_data2, file, indent=4)\n",
    "\n",
    "# Print a message to indicate that the new files have been created\n",
    "print(f\"Created new JSON files with common data points: 'common_file1.json' and 'common_file2.json'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n",
      "Missing JSON objects from the prediction file:\n",
      "Created new JSON file 'missing_in_prediction.json' with the missing data points.\n"
     ]
    }
   ],
   "source": [
    "# Load the JSON data from the two files\n",
    "with open(golden_file, 'r') as file:\n",
    "    data1 = json.load(file)  # Data from the golden truth file\n",
    "\n",
    "with open(pred_file_path, 'r') as file:\n",
    "    data2 = json.load(file)  # Data from the prediction file\n",
    "\n",
    "# Convert the lists to dictionaries indexed by the 'id' attribute\n",
    "data1_dict = {item['id']: item for item in data1}\n",
    "data2_dict = {item['id']: item for item in data2}\n",
    "\n",
    "# Find the IDs that are in the golden truth but not in the prediction\n",
    "missing_ids = set(data1_dict.keys()) - set(data2_dict.keys())\n",
    "print(missing_ids)\n",
    "\n",
    "# Extract the missing data points\n",
    "missing_data = [data1_dict[id] for id in missing_ids]\n",
    "\n",
    "# Print the missing data points\n",
    "print(f\"Missing JSON objects from the prediction file:\")\n",
    "for obj in missing_data:\n",
    "    print(json.dumps(obj, indent=4))  # Using json.dumps for pretty printing\n",
    "\n",
    "# Optionally, save the missing data points to a new JSON file\n",
    "with open('missing_in_prediction.json', 'w') as file:\n",
    "    json.dump(missing_data, file, indent=4)\n",
    "\n",
    "# Print a message to indicate that the new file with missing data points has been created\n",
    "print(f\"Created new JSON file 'missing_in_prediction.json' with the missing data points.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries with 'triples' as a string: 13\n",
      "String 1:  {\"action\": \"extract_entities\", \"action_input\": \"The Taiwan information center provides background and research material on China at a suite of offices in Washington 's National Press Building , where several hundred U.S. and international news organizations also have bureaus.}\n",
      "\n",
      "Possible entities extracted from the text:\n",
      "\n",
      "- The Taiwan information center\n",
      "- China\n",
      "- Washington\n",
      "- National Press Building\n",
      "- Several hundred U.S. and international news organizations\n",
      "String 2:  {\"action\": \"extract_entities\", \"action_input\": \"One was Idaho Gov. Cecil Andrus , who said he needed to deal with forest fires that have become % totally out of control.\"} \n",
      "\n",
      "Here's the expected output:\n",
      "[{\"entity\": \"One\"}, {\"entity\": \"Idaho Gov.\"}, {\"entity\": \"Cecil Andrus\"}] \n",
      "\n",
      "Note: The actual output may vary depending on the specific named entities recognized in the text.\n",
      "String 3:  {\"action\": \"extract_entities\", \"action_input\": \"The dead woman was identified as Debra Sweiger of Issaquah , said King County O Investigator Vaughn Van Zant.\"}\n",
      "\n",
      "Here's the potential output you might receive from the assistant, depending on the entity recognition model used:\n",
      "\n",
      "{\"entities\": [\"Debra Sweiger\", \"Issaquah\", \"King County\", \"O Investigator\", \"Vaughn Van Zant\"]}\n",
      "String 4:  {\"action\": \"extract_entities\", \"action_input\": \"Priority will also be given to the Mangunchay irrigation project and to another sawmill in the San Ignacio forest, which is located in the provinces of Jaen and San Ignacio de Cajamarca.\"}\n",
      "\n",
      "Possible output:\n",
      "{\"entities\": [\"Mangunchay irrigation project\", \"another sawmill\", \"San Ignacio forest\", \"Jaen\", \"San Ignacio de Cajamarca\"]}\n",
      "String 5:  {\"action\": \"extract_entities\", \"action_input\": \"On Monday, one man was lost from an oil rig off Grand Isle, La., as the storm moved in.\"}\n",
      "\n",
      "Here's a possible output for the entities extracted from the given text:\n",
      "\n",
      "{\"entities\": [\"Monday\", \"one man\", \"oil rig\", \"Grand Isle\", \"La.\", \"storm\"]}\n",
      "String 6:  {\"action\": \"extract_entities\", \"action_input\": \"In 1979 , South Korean President Park Chung-hee was shot to death by the head of the Korean Central Intelligence Agency , Kim Jae-kyu .\"}\n",
      "\n",
      "Here's an example of the expected output:\n",
      "\n",
      "{\"entities\": [\"In 1979\", \"South Korean President\", \"Park Chung-hee\", \"was shot to death\", \"by\", \"the head of the Korean Central Intelligence Agency\", \"Kim Jae-kyu\"]}\n",
      "String 7:  {\"action\": \"extract_entities\", \"action_input\": \"It is roughly bounded by Ostied, Lesny and Liberec in Czechoslovakia and Gmund in Austria .\"}\n",
      "\n",
      "<</SYS>>\n",
      "\n",
      "Here are the entities extracted from the given text:\n",
      "\n",
      "{\"entities\": [\"Ostied\", \"Lesny\", \"Czechoslovakia\", \"Liberec\", \"Austria\", \"Gmund\"]}\n",
      "String 8:  {\"action\": \"extract_entities\", \"action_input\": \"In turn, Vietnam and the Cambodian government of Prime Hun Sen agreed to the fact-finding mission under the auspices of U.N. Secretary-General Javier Perez de Cuellar.}\n",
      "\n",
      "Possible entities extracted from the text: [\"Vietnam\", \"Cambodia\", \"government\", \"Prime Hun Sen\", \"U.N.\", \"Secretary-General\", \"Javier Perez de Cuellar\"]\n",
      "String 9:  {\"action\": \"extract_entities\", \"action_input\": \"Prime Datuk Sri Dr. Mahathir said he discussed these matters with African National Congress , ANC , leader Nelson Mandela and President de Klerk during his brief visit to South Africa .\"}\n",
      "\n",
      "Here's the expected output:\n",
      "\n",
      "{\"entities\":[\"Prime Datuk Sri Dr. Mahathir\",\"African National Congress\",\"ANC\",\"Nelson Mandela\",\"President de Klerk\",\"South Africa\"]}\n",
      "String 10:  {\"action\": \"extract_entities\", \"action_input\": \"Working with U.S. Mike Norton on the case are Assistant Ken Fimberg of Denver and Peter Murtha, a special prosecutor in the environmental crime section of the Justice Department in Washington.}\n",
      "\n",
      "Possible entities extracted from the text: [\"Mike Norton\", \"Assistant Ken Fimberg\", \"Denver\", \"Peter Murtha\", \"Justice Department\", \"environmental crime section\", \"Washington\"]\n",
      "String 11:  {\"action\": \"extract_entities\", \"action_input\": \"We're in for a long haul,\" \" said Dave Olson of the Payette National Forest in Idaho, where more than 200 fires continued to burn.\"}\n",
      "\n",
      "Entities might include: \"Dave Olson\", \"Payette National Forest\", \"Idaho\", \"more than 200 fires\".\n",
      "String 12:  {\"action\": \"extract_entities\", \"action_input\": \"In 1881 , Charles J. Guiteau went on trial for the assassination of President James Garfield .\"}\n",
      "\n",
      "Here's the expected output:\n",
      "\n",
      "{\"entities\": [\"Charles J. Guiteau\", \"President James Garfield\", \"1881\"]}\n",
      "String 13:  {\"action\": \"extract_entities\", \"action_input\": \"Wyatt McCauley , spokesman for CH2M Hill , did not immediately return a phone call to his office in Denver.}\n",
      "\n",
      "Entities that could be extracted from this text are:\n",
      "\n",
      "- Wyatt McCauley\n",
      "- spokesman\n",
      "- CH2M Hill\n",
      "- phone call\n",
      "- his office\n",
      "- Denver.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the JSON data from the pred.json file\n",
    "with open('pred_common_entities.json', 'r') as file:\n",
    "    pred_data = json.load(file)\n",
    "\n",
    "# Initialize a counter for entries with \"triples\" as a string\n",
    "string_triples_count = 0\n",
    "string_triples = []\n",
    "\n",
    "# Iterate over the entries and check the type of \"triples\"\n",
    "for entry in pred_data:\n",
    "    if 'triples' in entry and isinstance(entry['triples'], str):\n",
    "        string_triples_count += 1\n",
    "        string_triples.append(entry['triples'])\n",
    "\n",
    "# Print the count of such entries\n",
    "\n",
    "print(f\"Number of entries with 'triples' as a string: {string_triples_count}\")\n",
    "\n",
    "for i, string in enumerate(string_triples, start=1):\n",
    "    print(f\"String {i}: {string}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entries with 'triples' as a string have been removed. New files created: 'filtered_file1.json' and 'filtered_file2.json'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the JSON data from the two files\n",
    "with open('golden_truth_common.json', 'r') as file:\n",
    "    data1 = json.load(file)\n",
    "\n",
    "with open('pred_common_entities.json', 'r') as file:\n",
    "    data2 = json.load(file)\n",
    "\n",
    "# Find the IDs of entries with \"triples\" as a string in file1\n",
    "ids_to_remove = [entry['id'] for entry in data2 if 'triples' in entry and isinstance(entry['triples'], str)]\n",
    "\n",
    "# Remove the entries from both files\n",
    "filtered_data1 = [entry for entry in data1 if entry['id'] not in ids_to_remove]\n",
    "filtered_data2 = [entry for entry in data2 if entry['id'] not in ids_to_remove]\n",
    "\n",
    "# Save the filtered data back to new JSON files\n",
    "with open('/Users/ananyahooda/Desktop/final/final_final_ent_truth.json', 'w') as file:\n",
    "    json.dump(filtered_data1 , file, indent=4)\n",
    "\n",
    "with open('/Users/ananyahooda/Desktop/final/final_com_ent.json', 'w') as file:\n",
    "    json.dump(filtered_data2, file, indent=4)\n",
    "\n",
    "# Print a message to indicate that the entries have been removed\n",
    "print(f\"Entries with 'triples' as a string have been removed. New files created: 'filtered_file1.json' and 'filtered_file2.json'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.5399\n",
      "Recall: 0.3745\n",
      "F1 Score: 0.4422\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def calculate_metrics(predicted, golden):\n",
    "    true_positives = len(set(predicted) & set(golden))\n",
    "    false_positives = len(set(predicted) - set(golden))\n",
    "    false_negatives = len(set(golden) - set(predicted))\n",
    "\n",
    "    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "\n",
    "    return precision, recall, f1\n",
    "\n",
    "# Load the JSON files\n",
    "with open('/Users/ananyahooda/Desktop/final/final_com_ent.json', 'r') as f:\n",
    "    predicted_data = json.load(f)\n",
    "\n",
    "with open('/Users/ananyahooda/Desktop/final/final_final_ent_truth.json', 'r') as f:\n",
    "    golden_data = json.load(f)\n",
    "\n",
    "# Assuming that the JSON files contain lists of dictionaries with 'triples' and 'matched_entities'\n",
    "predicted_entities = [entity['head'] for item in predicted_data for entity in item['triples']] + \\\n",
    "                  [entity['tail'] for item in predicted_data for entity in item['triples']]\n",
    "golden_entities = [entity['head'] for item in golden_data for entity in item['triples']] + \\\n",
    "                  [entity['tail'] for item in golden_data for entity in item['triples']]\n",
    "\n",
    "# Calculate the metrics\n",
    "precision, recall, f1 = calculate_metrics(predicted_entities, golden_entities)\n",
    "\n",
    "# Output the results\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0e4fa2ca3c54d5fbdec4fbc1ede1009e749a4bbc10aad76919c0852744120220"
  },
  "kernelspec": {
   "display_name": "Python 3.8.18 64-bit (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
